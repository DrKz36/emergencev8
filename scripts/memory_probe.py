#!/usr/bin/env python3
"""
Benchmark de rÃ©tention mÃ©moire - Agent Ã‰MERGENCE
=================================================

Test la capacitÃ© de rÃ©tention de contexte des agents Neo, Anima, Nexus
Ã  trois jalons temporels : T+1h, T+24h, T+7j

Usage:
    # Pour Neo
    AGENT_NAME=Neo python scripts/memory_probe.py

    # Pour Anima
    AGENT_NAME=Anima python scripts/memory_probe.py

    # Pour Nexus
    AGENT_NAME=Nexus python scripts/memory_probe.py

Variables d'environnement:
    AGENT_NAME      : Neo | Anima | Nexus (dÃ©faut: Neo)
    SESSION_ID      : ID de session (dÃ©faut: session-{agent})
    BACKEND_URL     : URL backend (dÃ©faut: http://localhost:8000)
    JWT_TOKEN       : Token JWT pour auth (dÃ©faut: demo-token)

Sortie:
    Fichier CSV : memory_results_{agent}.csv
"""

import os
import time
import csv
import datetime as dt
import yaml
import httpx
from pathlib import Path
from typing import List, Dict

# Configuration
AGENT_NAME = os.getenv("AGENT_NAME", "Neo")
SESSION_ID = os.getenv(
    "SESSION_ID",
    f"session-{AGENT_NAME.lower()}-{dt.datetime.utcnow().strftime('%Y%m%d%H%M%S')}",
)
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")
JWT_TOKEN = os.getenv("JWT_TOKEN", "demo-token")

# Chemins
ROOT_DIR = Path(__file__).parent.parent
RESULTS_CSV = ROOT_DIR / f"memory_results_{AGENT_NAME.lower()}.csv"
GROUND_TRUTH_PATH = ROOT_DIR / "prompts" / "ground_truth.yml"

# DÃ©lais de test (nom, secondes)
# Pour tests rapides, utiliser des dÃ©lais courts (ex: 60s, 120s, 180s)
# Pour production, utiliser les vrais dÃ©lais (3600, 86400, 604800)
DELTAS = [("T+1h", 3600), ("T+1d", 86400), ("T+1w", 604800)]

# Mode debug (dÃ©lais raccourcis)
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"
if DEBUG_MODE:
    print("âš ï¸  MODE DEBUG: DÃ©lais raccourcis pour tests rapides")
    DELTAS = [("T+1min", 60), ("T+2min", 120), ("T+3min", 180)]


def call_agent(message: str) -> str:
    """Appelle l'API /api/chat pour envoyer un message Ã  l'agent."""
    url = f"{BACKEND_URL}/api/chat"
    payload = {
        "agent": AGENT_NAME.lower(),
        "message": message,
        "session_id": SESSION_ID,
    }
    headers = {
        "Authorization": f"Bearer {JWT_TOKEN}",
        "Content-Type": "application/json",
    }

    try:
        with httpx.Client(timeout=30.0) as client:
            r = client.post(url, json=payload, headers=headers)
            r.raise_for_status()
            return r.json().get("response", "")
    except httpx.HTTPStatusError as e:
        print(f"âŒ Erreur HTTP {e.response.status_code}: {e.response.text}")
        return ""
    except httpx.RequestError as e:
        print(f"âŒ Erreur de connexion: {e}")
        return ""
    except Exception as e:
        print(f"âŒ Erreur inattendue: {e}")
        return ""


def normalize(text: str) -> str:
    """Normalise une chaÃ®ne pour comparaison (minuscules, espaces simplifiÃ©s)."""
    return " ".join(text.lower().split())


def score(ground_truth: str, prediction: str) -> float:
    """
    Calcule un score de similaritÃ© entre vÃ©ritÃ© terrain et prÃ©diction.

    Returns:
        1.0 si correspondance exacte
        0.5 si la vÃ©ritÃ© est contenue dans la prÃ©diction
        0.0 sinon
    """
    gt = normalize(ground_truth)
    pred = normalize(prediction)

    if gt == pred:
        return 1.0
    if gt and gt in pred:
        return 0.5
    return 0.0


def ensure_csv():
    """CrÃ©e le fichier CSV avec headers si nÃ©cessaire."""
    if not RESULTS_CSV.exists():
        RESULTS_CSV.write_text(
            "timestamp_utc,agent,session,tick,fact_id,score,truth,prediction\n",
            encoding="utf-8",
        )


def log_result(tick: str, fact_id: str, score_val: float, truth: str, prediction: str):
    """Enregistre un rÃ©sultat dans le CSV."""
    ensure_csv()
    with open(RESULTS_CSV, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(
            [
                dt.datetime.utcnow().isoformat(),
                AGENT_NAME,
                SESSION_ID,
                tick,
                fact_id,
                f"{score_val:.2f}",
                truth,
                prediction,
            ]
        )


def inject_context(facts: List[Dict]):
    """Injecte le contexte initial dans la mÃ©moire de l'agent."""
    ctx_lines = [f"- {fact['prompt']}" for fact in facts]
    ctx = "\n".join(ctx_lines)

    message = f"""MÃ©morise ces informations pour la suite de notre conversation :

{ctx}

Ces informations sont importantes et je vais te les redemander plus tard.
Confirme-moi que tu les as bien mÃ©morisÃ©es."""

    response = call_agent(message)
    print(f"âœ… Contexte injectÃ©. RÃ©ponse agent: {response[:100]}...")


def ask_recall(tick: str, facts: List[Dict]):
    """Teste le rappel des faits Ã  un instant donnÃ©."""
    print(f"\nğŸ“ Test de rappel Ã  {tick}")
    for fact in facts:
        question = f"Peux-tu rappeler la valeur associÃ©e Ã  : {fact['prompt']} (rÃ©ponds uniquement avec la valeur, rien d'autre)"
        pred = call_agent(question)

        score_val = score(fact["answer"], pred)
        log_result(tick, fact["id"], score_val, fact["answer"], pred)

        # Affichage rÃ©sultat
        emoji = "âœ…" if score_val >= 0.5 else "âŒ"
        print(
            f"  {emoji} {fact['id']}: score={score_val:.2f} | attendu='{fact['answer']}' | obtenu='{pred[:50]}'"
        )


def run():
    """ExÃ©cute le benchmark complet."""
    print("=" * 80)
    print(f"ğŸ§  BENCHMARK DE RÃ‰TENTION MÃ‰MOIRE - Agent {AGENT_NAME}")
    print("=" * 80)
    print(f"Session ID : {SESSION_ID}")
    print(f"Backend URL: {BACKEND_URL}")
    print(f"RÃ©sultats  : {RESULTS_CSV}")
    print()

    # Chargement des faits
    if not GROUND_TRUTH_PATH.exists():
        print(f"âŒ Fichier ground truth introuvable: {GROUND_TRUTH_PATH}")
        return

    with open(GROUND_TRUTH_PATH, "r", encoding="utf-8") as f:
        facts = yaml.safe_load(f)["facts"]

    print(f"ğŸ“š {len(facts)} faits chargÃ©s depuis ground_truth.yml")

    # Injection du contexte
    print(f"\nğŸ”„ Injection du contexte initial dans {AGENT_NAME}...")
    inject_context(facts)

    # Planification des tests
    t0 = time.time()
    schedule = [(label, t0 + seconds) for label, seconds in DELTAS]

    print("\nâ° Tests planifiÃ©s:")
    for label, when in schedule:
        dt_when = dt.datetime.fromtimestamp(when)
        print(f"  - {label}: {dt_when.strftime('%Y-%m-%d %H:%M:%S')}")

    # Boucle de tests
    while schedule:
        label, when = schedule[0]
        now = time.time()

        if now >= when:
            print(
                f"\nğŸš€ ExÃ©cution test {label} (planifiÃ© Ã  {dt.datetime.fromtimestamp(when).strftime('%H:%M:%S')})"
            )
            ask_recall(label, facts)
            schedule.pop(0)
        else:
            # Attente
            remaining = int(when - now)
            if remaining > 60:
                print(
                    f"â³ Attente {remaining // 60}min {remaining % 60}s avant prochain test ({label})...",
                    end="\r",
                )
            time.sleep(min(30, remaining))

    print("\n" + "=" * 80)
    print(f"âœ… Benchmark terminÃ© pour {AGENT_NAME}")
    print(f"ğŸ“Š RÃ©sultats sauvegardÃ©s dans : {RESULTS_CSV}")
    print("=" * 80)


if __name__ == "__main__":
    try:
        run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Benchmark interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        import traceback

        traceback.print_exc()
