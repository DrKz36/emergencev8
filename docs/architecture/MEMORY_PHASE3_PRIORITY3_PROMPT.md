# Prompt Instance Suivante - M√©moire Phase 3 : Priorit√© 3 (Groupement Th√©matique)

**Date:** 2025-10-15
**Contexte:** Suite de l'impl√©mentation Phase 3 - Priorit√©s 1 & 2 compl√©t√©es
**Objectif:** Impl√©menter le groupement th√©matique intelligent des concepts consolid√©s

---

## üéØ Contexte pour la Prochaine Instance

Bonjour ! Tu reprends le d√©veloppement apr√®s la **Phase 3 - Priorit√©s 1 & 2 compl√©t√©es et valid√©es**.

### √âtat Actuel (Fin Priorit√©s 1 & 2 - 2025-10-15)

**‚úÖ Fonctionnalit√©s Op√©rationnelles:**

1. **Cache de Recherche Consolid√©e (Priorit√© 1)** ‚úÖ
   - Cache intelligent pour m√©moire consolid√©e
   - R√©duction latence: 1.95s ‚Üí 22ms (87% am√©lioration)
   - Cache hit rate: ~33% (cible atteinte)
   - 7 tests unitaires (100% PASS)

2. **Redis + M√©triques Prometheus (Priorit√© 2)** ‚úÖ
   - Redis op√©rationnel (Docker container)
   - 5 nouvelles m√©triques Prometheus
   - Observabilit√© compl√®te (/metrics endpoint)
   - Tests automatis√©s (3/3 PASS)

3. **D√©tection & Enrichissement Temporel (Phase 2)** ‚úÖ
   - D√©tection questions temporelles: ‚úÖ
   - Enrichissement avec m√©moire consolid√©e: ‚úÖ
   - Format: `**[15 oct √† 3h08] Toi :** ...`

**üìä M√©triques Actuelles:**
- Cache hit: 22ms (vs 175ms miss)
- Concepts consolid√©s trouv√©s: 4/4
- Redis backend: Op√©rationnel
- M√©triques Prometheus: 5 m√©triques actives

**üîß Architecture Actuelle:**
```
User Query (temporelle)
    ‚Üì
_is_temporal_query() ‚Üí D√©tection
    ‚Üì
_get_cached_consolidated_memory() ‚Üí Cache/ChromaDB
    ‚Üì
4 concepts consolid√©s (exemple)
    ‚Üì
_build_temporal_history_context() ‚Üí Formatage lin√©aire
    ‚Üì
Contexte enrichi (20 messages + 4 concepts)
```

---

## üöÄ Phase 3 - Priorit√© 3 : Groupement Th√©matique Intelligent

### Probl√®me Identifi√©

**Format actuel (lin√©aire):**
```
### Historique r√©cent de cette conversation

**[2 oct √† 16h45] M√©moire (concept) :** L'utilisateur demande des citations du po√®me fondateur...
**[2 oct √† 16h45] M√©moire (concept) :** L'utilisateur pr√©f√®re Python pour automation...
**[8 oct √† 14h32] M√©moire (concept) :** L'utilisateur demande configuration Docker...
**[8 oct √† 14h35] M√©moire (concept) :** L'utilisateur demande optimisation Kubernetes...
**[10 oct √† 9h15] Toi :** Peux-tu m'expliquer Docker ?
**[10 oct √† 9h16] Anima :** Docker est une plateforme...
```

**Probl√®mes:**
- ‚ùå Concepts similaires dispers√©s (Docker + Kubernetes s√©par√©s)
- ‚ùå Pas de vue d'ensemble th√©matique
- ‚ùå Difficult√© √† identifier les sujets r√©currents
- ‚ùå Contexte verbeux pour Anima

**Format souhait√© (group√©):**
```
### Historique r√©cent de cette conversation

**[Po√®me Fondateur]** Discussion (1 √©change)
  - 2 oct √† 16h45: Citations demand√©es...

**[Infrastructure & D√©ploiement]** Discussion r√©currente (2 √©changes)
  - 8 oct √† 14h32: Configuration Docker...
  - 8 oct √† 14h35: Optimisation Kubernetes...

**[Pr√©f√©rences Techniques]** Note
  - 2 oct √† 16h45: Python pour automation

**Messages r√©cents:**
**[10 oct √† 9h15] Toi :** Peux-tu m'expliquer Docker ?
**[10 oct √† 9h16] Anima :** Docker est une plateforme...
```

**Avantages:**
- ‚úÖ Th√®mes clairs et regroup√©s
- ‚úÖ Contexte plus concis (r√©duction ~30%)
- ‚úÖ Meilleure compr√©hension pour Anima
- ‚úÖ Identification rapide des sujets r√©currents

---

## üìã Objectifs Priorit√© 3

### 1. Clustering des Concepts Similaires

**M√©thode:** Embeddings + Cosine Similarity

**Impl√©mentation:**
- Utiliser `SentenceTransformer` d√©j√† disponible (all-MiniLM-L6-v2)
- Calculer embeddings pour chaque concept consolid√©
- Regrouper concepts similaires (seuil cosine > 0.7)

**Fichier:** `src/backend/features/chat/service.py`

### 2. Extraction de Titres Intelligents

**M√©thode:** TF-IDF + Extraction Keywords

**Impl√©mentation:**
- Analyser le texte combin√© du groupe
- Extraire 2-3 mots-cl√©s les plus pertinents
- Formater en titre lisible (ex: "Infrastructure & D√©ploiement")

### 3. Formatage Group√©

**Structure:**
```
**[Titre du Groupe]** Discussion r√©currente (N √©changes)
  - Date 1: R√©sum√© court...
  - Date 2: R√©sum√© court...

**Messages r√©cents:** (garder les 10 plus r√©cents non group√©s)
```

### 4. Tests & Validation

**Tests unitaires:**
- Clustering: v√©rifier regroupement concepts similaires
- Extraction titres: v√©rifier pertinence
- Format final: v√©rifier lisibilit√©

**Tests manuels:**
- Questions temporelles avec 5+ concepts
- V√©rifier contexte plus concis
- V√©rifier qualit√© r√©ponses Anima

---

## üîß Impl√©mentation D√©taill√©e

### √âtape 1: M√©thode de Clustering

**Fichier:** `src/backend/features/chat/service.py`

**Nouvelle m√©thode (√† ajouter apr√®s `_get_cached_consolidated_memory`):**

```python
async def _group_concepts_by_theme(
    self,
    consolidated_entries: List[Dict[str, Any]]
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Groupe les concepts consolid√©s par similarit√© s√©mantique.

    Args:
        consolidated_entries: Liste de concepts avec timestamp, content, type

    Returns:
        Dict[group_id, List[concepts]] - Concepts regroup√©s par th√®me

    Algorithme:
    1. Si < 3 concepts ‚Üí pas de groupement (retour simple)
    2. G√©n√©rer embeddings pour chaque concept
    3. Calculer matrice de similarit√© cosine
    4. Regrouper concepts avec similarit√© > 0.7
    5. Assigner concepts orphelins au groupe le plus proche (si > 0.5)
    """

    # Pas de groupement si peu de concepts
    if len(consolidated_entries) < 3:
        return {"ungrouped": consolidated_entries}

    try:
        # Extraire les contenus pour embedding
        contents = [entry["content"] for entry in consolidated_entries]

        # G√©n√©rer embeddings avec le mod√®le d√©j√† charg√©
        # self.vector_service.model est le SentenceTransformer
        embeddings = self.vector_service.model.encode(contents)

        # Calculer similarit√© cosine
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(embeddings)

        # Clustering simple avec seuil
        groups = {}
        assigned = set()
        group_id = 0

        for i in range(len(consolidated_entries)):
            if i in assigned:
                continue

            # Cr√©er nouveau groupe
            group_key = f"theme_{group_id}"
            groups[group_key] = [consolidated_entries[i]]
            assigned.add(i)

            # Ajouter concepts similaires (cosine > 0.7)
            for j in range(i + 1, len(consolidated_entries)):
                if j not in assigned and similarity_matrix[i][j] > 0.7:
                    groups[group_key].append(consolidated_entries[j])
                    assigned.add(j)

            group_id += 1

        logger.info(f"[ThematicGrouping] {len(consolidated_entries)} concepts ‚Üí {len(groups)} groupes")

        return groups

    except Exception as e:
        logger.warning(f"[ThematicGrouping] Erreur clustering: {e}")
        # Fallback: retour sans groupement
        return {"ungrouped": consolidated_entries}
```

**D√©pendances:**
- `sklearn.metrics.pairwise.cosine_similarity` (d√©j√† disponible via scikit-learn)
- `self.vector_service.model` (SentenceTransformer d√©j√† charg√©)

### √âtape 2: Extraction de Titres

**Nouvelle m√©thode (√† ajouter apr√®s `_group_concepts_by_theme`):**

```python
def _extract_group_title(self, concepts: List[Dict[str, Any]]) -> str:
    """
    Extrait un titre repr√©sentatif pour un groupe de concepts.

    M√©thode:
    1. Concat√©ner tous les contenus du groupe
    2. Tokenizer et nettoyer (stop words, ponctuation)
    3. Calculer fr√©quence des mots (TF simple)
    4. Prendre les 2-3 mots les plus fr√©quents et significatifs
    5. Formater en titre lisible

    Args:
        concepts: Liste de concepts du groupe

    Returns:
        Titre format√© (ex: "Infrastructure & D√©ploiement")
    """

    # Concat√©ner tous les contenus
    combined_text = " ".join([c.get("content", "") for c in concepts])

    # Nettoyer et tokenizer
    import re
    words = re.findall(r'\b[a-zA-Z√Ä-√ø]{4,}\b', combined_text.lower())

    # Stop words simples (√† enrichir si besoin)
    stop_words = {
        '√™tre', 'avoir', 'faire', 'dire', 'aller', 'voir', 'savoir',
        'pouvoir', 'vouloir', 'venir', 'devoir', 'prendre', 'donner',
        'utilisateur', 'demande', 'question', 'discussion', 'parler',
        'the', 'and', 'for', 'that', 'with', 'this', 'from', 'they',
        'have', 'will', 'what', 'been', 'more', 'when', 'there'
    }

    # Calculer fr√©quence
    word_freq = {}
    for word in words:
        if word not in stop_words and len(word) > 3:
            word_freq[word] = word_freq.get(word, 0) + 1

    # Prendre les 2-3 mots les plus fr√©quents
    if not word_freq:
        return "Discussion"

    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]

    # Formater en titre (capitaliser)
    title_words = [w[0].capitalize() for w in top_words[:2]]  # Max 2 mots

    # Joindre avec &
    if len(title_words) == 2:
        return f"{title_words[0]} & {title_words[1]}"
    elif len(title_words) == 1:
        return title_words[0]
    else:
        return "Discussion"
```

### √âtape 3: Formatage Group√©

**Modifier `_build_temporal_history_context` (lignes ~1277-1395):**

**Avant (code actuel):**
```python
# ‚úÖ Phase 3: Enrichir avec concepts consolid√©s (avec cache)
n_results = min(5, max(3, len(messages) // 4)) if messages else 5

consolidated_entries = []
if last_user_message and user_id:
    consolidated_entries = await self._get_cached_consolidated_memory(
        user_id=user_id,
        query_text=last_user_message,
        n_results=n_results
    )

# ... formatage lin√©aire actuel ...
```

**Apr√®s (avec groupement):**
```python
# ‚úÖ Phase 3: Enrichir avec concepts consolid√©s (avec cache)
n_results = min(5, max(3, len(messages) // 4)) if messages else 5

consolidated_entries = []
if last_user_message and user_id:
    consolidated_entries = await self._get_cached_consolidated_memory(
        user_id=user_id,
        query_text=last_user_message,
        n_results=n_results
    )

# üÜï PHASE 3 - PRIORIT√â 3: Groupement th√©matique
grouped_concepts = {}
if len(consolidated_entries) >= 3:
    # Activer groupement si 3+ concepts
    grouped_concepts = await self._group_concepts_by_theme(consolidated_entries)
    logger.info(f"[ThematicGrouping] {len(grouped_concepts)} groupes cr√©√©s")
else:
    # Pas de groupement si peu de concepts
    if consolidated_entries:
        grouped_concepts = {"ungrouped": consolidated_entries}

# ... suite du formatage (voir section suivante) ...
```

**Nouveau formatage des groupes:**

```python
# Formater les groupes th√©matiques
if grouped_concepts:
    lines.append("**Th√®mes abord√©s:**")
    lines.append("")

    for group_id, concepts in grouped_concepts.items():
        if group_id == "ungrouped":
            # Pas de titre de groupe pour concepts non group√©s
            for concept in concepts:
                # Formatage standard (comme avant)
                dt = datetime.fromisoformat(concept["timestamp"].replace("Z", "+00:00"))
                date_str = f"{dt.day} {months[dt.month]} √† {dt.hour}h{dt.minute:02d}"
                preview = concept["content"]
                lines.append(f"**[{date_str}] M√©moire ({concept['type']}) :** {preview}")
        else:
            # Groupe th√©matique
            title = self._extract_group_title(concepts)
            count = len(concepts)
            label = "√©change" if count == 1 else "√©changes"

            lines.append(f"**[{title}]** Discussion r√©currente ({count} {label})")

            for concept in concepts:
                dt = datetime.fromisoformat(concept["timestamp"].replace("Z", "+00:00"))
                date_str = f"{dt.day} {months[dt.month]} √† {dt.hour}h{dt.minute:02d}"
                # Preview raccourci pour groupes
                preview = concept["content"][:60] + "..." if len(concept["content"]) > 60 else concept["content"]
                lines.append(f"  - {date_str}: {preview}")

    lines.append("")

# Formater les messages r√©cents (garder les 10 plus r√©cents)
lines.append("**Messages r√©cents:**")
recent_messages = all_events[-10:] if len(all_events) > 10 else all_events

for event in recent_messages:
    # ... formatage standard des messages ...
```

---

## üß™ Tests √† Impl√©menter

### Test 1: Clustering de Base

**Fichier:** `tests/backend/features/chat/test_thematic_grouping.py` (nouveau)

```python
import pytest
from unittest.mock import Mock
from backend.features.chat.service import ChatService

class TestThematicGrouping:
    """Tests pour le groupement th√©matique des concepts."""

    @pytest.fixture
    def mock_vector_service(self):
        """Mock VectorService avec SentenceTransformer."""
        vs = Mock()

        # Mock du mod√®le pour embeddings
        model_mock = Mock()
        model_mock.encode = Mock(return_value=[
            [0.1, 0.2, 0.3, 0.4],  # Concept 1: Docker
            [0.1, 0.2, 0.35, 0.4], # Concept 2: Kubernetes (similaire √† 1)
            [0.8, 0.1, 0.1, 0.1],  # Concept 3: Po√®me (diff√©rent)
        ])
        vs.model = model_mock

        return vs

    @pytest.mark.asyncio
    async def test_clustering_similar_concepts(self, mock_vector_service):
        """Test que concepts similaires sont group√©s ensemble."""

        concepts = [
            {"content": "Configuration Docker", "timestamp": "2025-10-08T14:32:00Z", "type": "concept"},
            {"content": "Optimisation Kubernetes", "timestamp": "2025-10-08T14:35:00Z", "type": "concept"},
            {"content": "Citations po√®me fondateur", "timestamp": "2025-10-02T16:45:00Z", "type": "concept"},
        ]

        # Cr√©er service (simplifi√© pour test)
        service = Mock()
        service.vector_service = mock_vector_service

        # Appeler m√©thode de groupement (√† impl√©menter)
        # groups = await service._group_concepts_by_theme(concepts)

        # Assertions attendues:
        # - 2 groupes (Docker+Kubernetes / Po√®me)
        # - Docker et Kubernetes dans le m√™me groupe
        # - Po√®me dans un groupe s√©par√©

        # assert len(groups) == 2
        # assert any(len(g) == 2 for g in groups.values())  # Un groupe de 2
        # assert any(len(g) == 1 for g in groups.values())  # Un groupe de 1
```

### Test 2: Extraction de Titres

```python
def test_extract_group_title(self):
    """Test extraction de titre pertinent."""

    concepts = [
        {"content": "L'utilisateur demande configuration Docker pour production"},
        {"content": "L'utilisateur demande optimisation Docker et Kubernetes"},
    ]

    service = Mock()
    # title = service._extract_group_title(concepts)

    # Assertions:
    # - Titre contient "Docker" (mot cl√© principal)
    # - Pas de stop words ("utilisateur", "demande")
    # - Format lisible

    # assert "docker" in title.lower()
    # assert "utilisateur" not in title.lower()
```

### Test 3: Format Group√©

```python
@pytest.mark.asyncio
async def test_grouped_context_format(self):
    """Test que le contexte group√© est plus concis."""

    # Simuler 5 concepts similaires
    concepts = [
        {"content": f"Concept Docker #{i}", "timestamp": f"2025-10-{8+i}T14:00:00Z", "type": "concept"}
        for i in range(5)
    ]

    # Comparer taille contexte avec/sans groupement
    # context_ungrouped = ... (format lin√©aire)
    # context_grouped = ... (format group√©)

    # assert len(context_grouped) < len(context_ungrouped)
    # assert "Discussion r√©currente" in context_grouped
    # assert "5 √©changes" in context_grouped
```

---

## üìä Crit√®res de Succ√®s

| Crit√®re | Cible | Validation |
|---------|-------|------------|
| **Clustering fonctionnel** | Concepts similaires group√©s | Seuil cosine > 0.7 |
| **Extraction titres** | Titres pertinents | Mots-cl√©s significatifs (pas stop words) |
| **Format group√©** | Plus concis | R√©duction 20-30% taille contexte |
| **Tests unitaires** | 100% pass | 3+ tests PASS |
| **Qualit√© r√©ponses** | Anima comprend bien | Test manuel en prod |
| **Performance** | Overhead < 100ms | Mesure avec m√©triques existantes |

---

## üéØ Plan d'Action Recommand√©

### Session 1: Impl√©mentation (2-3h)

**Ordre d'impl√©mentation:**

1. ‚úÖ **M√©thode `_group_concepts_by_theme()`**
   - Embeddings avec `vector_service.model.encode()`
   - Matrice similarit√© cosine
   - Clustering seuil 0.7
   - Logger nombre de groupes

2. ‚úÖ **M√©thode `_extract_group_title()`**
   - Concat√©nation textes
   - Tokenisation et nettoyage
   - Fr√©quence mots (TF simple)
   - Formatage titre (2 mots max)

3. ‚úÖ **Modification `_build_temporal_history_context()`**
   - Appel groupement si 3+ concepts
   - Nouveau formatage group√©
   - Garder 10 messages r√©cents s√©par√©s

4. ‚úÖ **Tests unitaires**
   - Clustering: concepts similaires
   - Extraction: titres pertinents
   - Format: r√©duction taille

5. ‚úÖ **Validation automatique**
   - Compiler code
   - Lancer tests
   - V√©rifier pas de r√©gression

### Session 2: Tests Manuels en Production (30min - Utilisateur)

**Tests √† effectuer:**

1. ‚úÖ **Question temporelle avec 5+ concepts:**
   - Poser: "Quand avons-nous parl√© de mes projets techniques ?"
   - V√©rifier groupement concepts similaires
   - V√©rifier titres pertinents

2. ‚úÖ **V√©rifier qualit√© r√©ponses Anima:**
   - Anima comprend le contexte group√© ?
   - R√©ponses pr√©cises avec dates ?
   - Pas de perte d'information ?

3. ‚úÖ **Mesurer performance:**
   - Observer logs temps de traitement
   - V√©rifier overhead clustering < 100ms
   - Consulter `/metrics` pour nouvelles m√©triques

4. ‚úÖ **Documenter r√©sultats:**
   - Screenshot contexte group√©
   - Comparer taille avant/apr√®s
   - Noter qualit√© r√©ponses

---

## üìù Fichiers √† Modifier/Cr√©er

### Code Source

1. **`src/backend/features/chat/service.py`**
   - Ajouter `_group_concepts_by_theme()` apr√®s ligne ~1246
   - Ajouter `_extract_group_title()` apr√®s nouvelle m√©thode
   - Modifier `_build_temporal_history_context()` lignes ~1277-1395
   - **Estimation:** +150 lignes

### Tests

2. **`tests/backend/features/chat/test_thematic_grouping.py`** (nouveau)
   - Tests clustering
   - Tests extraction titres
   - Tests format group√©
   - **Estimation:** 200 lignes

### Documentation

3. **`docs/architecture/MEMORY_PHASE3_GROUPING_IMPLEMENTATION.md`** (nouveau)
   - Architecture groupement
   - Algorithme clustering
   - Exemples avant/apr√®s
   - **Estimation:** 300 lignes

4. **`reports/memory_phase3_grouping_session_2025-10-15.md`** (nouveau)
   - Rapport de session
   - R√©sultats tests
   - Screenshots
   - **Estimation:** 250 lignes

---

## üîç Points d'Attention

### Performance

**‚ö†Ô∏è Embeddings peuvent √™tre co√ªteux**
- 5 concepts √ó 50ms = 250ms
- Clustering: ~10-20ms
- **Total overhead attendu: 200-300ms**

**Solution si trop lent:**
- Limiter groupement √† max 10 concepts
- Cacher embeddings si m√™me requ√™te r√©p√©t√©e
- Logger dur√©e pour monitoring

### Qualit√©

**‚ö†Ô∏è Extraction titres peut √™tre impr√©cise**
- D√©pend de la richesse du vocabulaire
- Stop words peuvent varier selon contexte

**Solution:**
- Enrichir liste stop words si n√©cessaire
- Fallback vers "Discussion" si extraction √©choue
- Tests manuels pour valider pertinence

### Edge Cases

**‚ö†Ô∏è Cas limites √† g√©rer:**
- 0 concepts ‚Üí pas de groupement
- 1-2 concepts ‚Üí pas de groupement
- Tous concepts identiques ‚Üí 1 seul groupe
- Aucun concept similaire ‚Üí N groupes de 1

**Solution:**
- Seuil minimum 3 concepts pour activer groupement
- Fallback gracieux vers format lin√©aire
- Logger warnings si comportement inattendu

---

## üìö R√©f√©rences

### Documentation Existante

1. **Phase 3 - Priorit√©s 1 & 2:**
   - [MEMORY_PHASE3_CACHE_IMPLEMENTATION.md](MEMORY_PHASE3_CACHE_IMPLEMENTATION.md)
   - [MEMORY_PHASE3_REDIS_METRICS.md](MEMORY_PHASE3_REDIS_METRICS.md)

2. **Phase 2 - Contexte Temporel:**
   - [MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md](MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md)

3. **Rapports de Session:**
   - [memory_phase3_cache_session_2025-10-15.md](../../reports/memory_phase3_cache_session_2025-10-15.md)
   - [memory_phase3_redis_metrics_session_2025-10-15.md](../../reports/memory_phase3_redis_metrics_session_2025-10-15.md)

### Code Source Cl√©

- [service.py:1130-1395](../../src/backend/features/chat/service.py#L1130-L1395) - Contexte temporel actuel
- [vector_service.py](../../src/backend/features/memory/vector_service.py) - SentenceTransformer

### D√©pendances

- `scikit-learn` (d√©j√† install√©) - Pour `cosine_similarity`
- `SentenceTransformer` (d√©j√† charg√©) - Pour embeddings
- Pas de nouvelle d√©pendance √† installer ‚úÖ

---

## üéä Checklist Avant de Commencer

- [ ] Lire cette documentation compl√®te
- [ ] V√©rifier Phase 3 P1 & P2 fonctionnelles
- [ ] Backend d√©marre sans erreur
- [ ] Redis op√©rationnel (`docker ps`)
- [ ] Avoir un thread test avec 5+ concepts consolid√©s
- [ ] Comprendre architecture actuelle (service.py lignes 1130-1395)

---

## üöÄ R√©sum√© Ex√©cutif

**Objectif:** Regrouper concepts consolid√©s similaires pour contexte plus concis et intelligent.

**M√©thode:**
1. Embeddings (SentenceTransformer d√©j√† disponible)
2. Clustering (cosine similarity > 0.7)
3. Extraction titres (TF-IDF simple)
4. Formatage group√©

**R√©sultat attendu:**
- R√©duction taille contexte: 20-30%
- Meilleure lisibilit√© pour Anima
- Identification th√®mes r√©currents
- Overhead performance: < 300ms

**Tests:**
- Automatiques: 3+ tests unitaires
- Manuels (utilisateur): Questions temporelles avec 5+ concepts

**Dur√©e estim√©e:** 2-3h impl√©mentation + 30min tests utilisateur

---

**Bon courage pour la Priorit√© 3 ! üöÄ**

L'objectif est de rendre le contexte temporel encore plus **intelligent et concis** en regroupant automatiquement les concepts similaires.

**Prochaine √©tape:** Impl√©menter les 3 m√©thodes puis tester avec des questions temporelles r√©elles.

---

**Cr√©√© le:** 2025-10-15
**Par:** Session Phase 3 - Pr√©paration Priorit√© 3
**Statut:** ‚úÖ Pr√™t pour impl√©mentation
**D√©pendances:** Phase 3 P1 & P2 compl√©t√©es ‚úÖ
