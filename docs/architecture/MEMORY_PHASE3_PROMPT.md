# Prompt Instance Suivante - M√©moire Phase 3 : Optimisations & M√©triques
**Date:** 2025-10-15
**Contexte:** Suite de l'impl√©mentation Phase 2 (Contexte temporel avec m√©moire consolid√©e)
**Objectif:** Optimiser performance, ajouter m√©triques Prometheus, am√©liorer intelligence

---

## üéØ Contexte pour la Prochaine Instance

Bonjour ! Tu reprends le d√©veloppement apr√®s la **Phase 2 compl√©t√©e et valid√©e** de la m√©moire temporelle d'√âmergence.

### √âtat Actuel (Fin Phase 2 - 2025-10-15)

**‚úÖ Fonctionnalit√©s Op√©rationnelles:**

1. **D√©tection Questions Temporelles** ‚úÖ
   - Regex multilingue FR/EN
   - Patterns: quand, quel jour, quelle heure, when, what time, etc.
   - Insensible √† la casse

2. **Enrichissement Contexte Historique** ‚úÖ
   - R√©cup√©ration 20 derniers messages thread
   - Recherche s√©mantique dans `emergence_knowledge` (ChromaDB)
   - Fusion chronologique messages + concepts consolid√©s
   - Format: `**[15 oct √† 3h08] Toi :**` / `**[14 oct √† 4h30] M√©moire (concept) :**`

3. **Tests Valid√©s** ‚úÖ
   - Tests unitaires: 12/12 pass√©s (100%)
   - Test production: Question "Quand avons-nous parl√© de mon po√®me fondateur?" ‚Üí Dates pr√©cises fournies
   - Performance: 4.84s total (acceptable)
   - Log: `[TemporalHistory] Contexte enrichi: 20 messages + 4 concepts consolid√©s`

**üìä M√©triques Actuelles:**
- Recherche ChromaDB: ~1.95s
- Concepts consolid√©s trouv√©s: 4/4 (100%)
- Temps r√©ponse total: 4.84s
- Pr√©cision temporelle: 100%

---

## üöÄ Phase 3 : Objectifs & Priorit√©s

### Priorit√© 1: Optimisations Performance (2-3h)

**Probl√®mes Identifi√©s:**

1. **Pas de Cache Recherche Consolid√©e**
   - Chaque question temporelle fait une recherche ChromaDB
   - M√™me question = m√™me recherche r√©p√©t√©e
   - Latence √©vitable: ~500ms par requ√™te

2. **Limite Hardcod√©e n_results=5**
   - Ne s'adapte pas √† la taille de la collection
   - Warning ChromaDB si collection < 5 entr√©es
   - Pas d'optimisation selon contexte

3. **Pas de Filtrage Temporel**
   - Recherche sur toute l'historique
   - Peut ramener concepts tr√®s anciens (>1 an)
   - Pertinence diminue avec le temps

**Solutions √† Impl√©menter:**

#### 1.1 Cache Recherche Consolid√©e

**Fichier:** `src/backend/features/chat/service.py`

**Impl√©mentation:**
```python
# Ajouter au niveau classe ChatService
from functools import lru_cache
import hashlib

_temporal_cache: Dict[str, Tuple[List[Dict], datetime]] = {}
_temporal_cache_lock = asyncio.Lock()

async def _get_cached_consolidated_entries(
    self,
    user_id: str,
    query_text: str,
    n_results: int = 5
) -> Optional[List[Dict[str, Any]]]:
    """R√©cup√®re entries consolid√©es depuis cache ou ChromaDB."""

    # Cl√© cache: hash(user_id + query_text normalis√©)
    cache_key = hashlib.md5(f"{user_id}:{query_text[:50].lower()}".encode()).hexdigest()

    async with _temporal_cache_lock:
        if cache_key in _temporal_cache:
            entries, timestamp = _temporal_cache[cache_key]
            # Cache valide 5 minutes
            if datetime.now(timezone.utc) - timestamp < timedelta(minutes=5):
                logger.debug(f"[TemporalCache] Hit: {cache_key[:8]}")
                return entries

    # Cache miss ‚Üí recherche ChromaDB
    logger.debug(f"[TemporalCache] Miss: {cache_key[:8]}")
    entries = await self._search_consolidated_concepts(user_id, query_text, n_results)

    # Stocker en cache
    async with _temporal_cache_lock:
        _temporal_cache[cache_key] = (entries, datetime.now(timezone.utc))

        # √âviction si > 100 entr√©es
        if len(_temporal_cache) > 100:
            oldest_key = min(_temporal_cache.items(), key=lambda x: x[1][1])[0]
            del _temporal_cache[oldest_key]

    return entries
```

**R√©sultat attendu:**
- R√©duction latence: 1.95s ‚Üí 0.1s pour requ√™tes r√©p√©t√©es
- Cache hit rate cible: 30-40%

#### 1.2 Limite Dynamique n_results

**Fichier:** `src/backend/features/chat/service.py:1170`

**Changement:**
```python
# Avant (ligne 1170)
n_results=5,

# Apr√®s
n_results = min(5, max(3, len(all_events) // 4))
```

**Raison:** Adapter le nombre de r√©sultats selon la taille du contexte d√©j√† charg√©

#### 1.3 Filtrage Temporel

**Fichier:** `src/backend/features/chat/service.py:1171`

**Changement:**
```python
# Ajouter filtre temporel (derniers 30 jours)
from datetime import timedelta
cutoff_date = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()

where = {"user_id": user_id}
# Ajouter filtre temporel si ChromaDB le supporte
# where["created_at"] = {"$gte": cutoff_date}  # √Ä tester avec ChromaDB
```

**Alternative si ChromaDB ne supporte pas filtres complexes:**
```python
# Filtrage post-recherche
for i, metadata in enumerate(metadatas):
    timestamp = metadata.get("created_at") or metadata.get("first_mentioned_at")

    # Ignorer concepts > 30 jours
    if timestamp:
        dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
        if datetime.now(timezone.utc) - dt > timedelta(days=30):
            continue

    # ... reste du code
```

---

### Priorit√© 2: M√©triques Prometheus (1-2h)

**Objectif:** Suivre performance et usage de la m√©moire temporelle

**Fichier:** `src/backend/features/chat/rag_metrics.py` (ou nouveau fichier)

**M√©triques √† Ajouter:**

```python
from prometheus_client import Counter, Histogram, Gauge

# 1. Compteur Questions Temporelles
memory_temporal_queries_total = Counter(
    "memory_temporal_queries_total",
    "Total temporal queries detected",
    ["detected"]  # "true" ou "false"
)

# 2. Compteur Concepts Consolid√©s Trouv√©s
memory_temporal_concepts_found_total = Counter(
    "memory_temporal_concepts_found_total",
    "Total consolidated concepts found in temporal queries",
    ["count_range"]  # "0", "1-2", "3-5", "5+"
)

# 3. Histogram Dur√©e Recherche ChromaDB
memory_temporal_search_duration_seconds = Histogram(
    "memory_temporal_search_duration_seconds",
    "Time spent searching ChromaDB for consolidated concepts",
    buckets=[0.1, 0.25, 0.5, 1.0, 2.0, 5.0]
)

# 4. Histogram Taille Contexte Enrichi
memory_temporal_context_size_bytes = Histogram(
    "memory_temporal_context_size_bytes",
    "Size of enriched temporal context in bytes",
    buckets=[100, 500, 1000, 2000, 5000, 10000]
)

# 5. Gauge Cache Hit Rate
memory_temporal_cache_hit_rate = Gauge(
    "memory_temporal_cache_hit_rate",
    "Cache hit rate for temporal queries (percentage)"
)

# 6. Compteur Cache Operations
memory_temporal_cache_operations_total = Counter(
    "memory_temporal_cache_operations_total",
    "Total cache operations",
    ["operation"]  # "hit", "miss", "eviction"
)
```

**Instrumentation dans service.py:**

```python
# Ligne 1852 (d√©tection)
if self._is_temporal_query(last_user_message):
    memory_temporal_queries_total.labels(detected="true").inc()

    # Mesurer dur√©e recherche ChromaDB
    start_search = time.time()
    consolidated_entries = await self._get_cached_consolidated_entries(...)
    search_duration = time.time() - start_search
    memory_temporal_search_duration_seconds.observe(search_duration)

    # Compter concepts trouv√©s
    count = len(consolidated_entries)
    if count == 0:
        range_label = "0"
    elif count <= 2:
        range_label = "1-2"
    elif count <= 5:
        range_label = "3-5"
    else:
        range_label = "5+"
    memory_temporal_concepts_found_total.labels(count_range=range_label).inc()

    # Mesurer taille contexte final
    context_size = len(recall_context.encode('utf-8'))
    memory_temporal_context_size_bytes.observe(context_size)
else:
    memory_temporal_queries_total.labels(detected="false").inc()
```

**Dashboard Grafana (Configuration Sugg√©r√©e):**

```json
{
  "title": "M√©moire Temporelle - Performance",
  "panels": [
    {
      "title": "Questions Temporelles D√©tect√©es",
      "targets": [
        {
          "expr": "rate(memory_temporal_queries_total{detected=\"true\"}[5m])"
        }
      ]
    },
    {
      "title": "Concepts Consolid√©s Trouv√©s",
      "targets": [
        {
          "expr": "sum by (count_range) (memory_temporal_concepts_found_total)"
        }
      ]
    },
    {
      "title": "Latence Recherche ChromaDB (p50, p95, p99)",
      "targets": [
        {
          "expr": "histogram_quantile(0.50, memory_temporal_search_duration_seconds)"
        },
        {
          "expr": "histogram_quantile(0.95, memory_temporal_search_duration_seconds)"
        },
        {
          "expr": "histogram_quantile(0.99, memory_temporal_search_duration_seconds)"
        }
      ]
    },
    {
      "title": "Cache Hit Rate",
      "targets": [
        {
          "expr": "memory_temporal_cache_hit_rate"
        }
      ]
    }
  ]
}
```

---

### Priorit√© 3: Groupement Th√©matique Intelligent (3-4h)

**Objectif:** Regrouper concepts consolid√©s par sujet pour pr√©sentation plus concise

**Probl√®me Actuel:**
```
**[14 oct √† 4h24] M√©moire (concept) :** L'utilisateur demande des citations...
**[14 oct √† 4h30] M√©moire (concept) :** L'utilisateur r√©p√®te des demandes...
**[15 oct √† 3h02] M√©moire (concept) :** L'utilisateur demande citations...
```

**Am√©lioration Souhait√©e:**
```
**[Po√®me fondateur]** Discussion r√©currente (3 √©changes)
  - 14 oct √† 4h24: Citations int√©grales demand√©es
  - 14 oct √† 4h30: Demandes r√©p√©t√©es
  - 15 oct √† 3h02: Nouvelles citations
```

**Impl√©mentation:**

**√âtape 1: Extraction Th√®mes avec Embeddings**

```python
async def _group_concepts_by_theme(
    self,
    consolidated_entries: List[Dict[str, Any]]
) -> Dict[str, List[Dict[str, Any]]]:
    """Groupe concepts consolid√©s par similarit√© s√©mantique."""

    if len(consolidated_entries) <= 2:
        # Pas de groupement si peu de concepts
        return {"default": consolidated_entries}

    # G√©n√©rer embeddings pour chaque concept
    contents = [entry["content"] for entry in consolidated_entries]
    embeddings = await self.vector_service.embed_batch(contents)

    # Clustering simple avec seuil de similarit√©
    from sklearn.metrics.pairwise import cosine_similarity
    similarity_matrix = cosine_similarity(embeddings)

    groups = {}
    assigned = set()
    group_id = 0

    for i in range(len(consolidated_entries)):
        if i in assigned:
            continue

        # Cr√©er nouveau groupe
        group_key = f"group_{group_id}"
        groups[group_key] = [consolidated_entries[i]]
        assigned.add(i)

        # Ajouter concepts similaires (cosine > 0.7)
        for j in range(i+1, len(consolidated_entries)):
            if j not in assigned and similarity_matrix[i][j] > 0.7:
                groups[group_key].append(consolidated_entries[j])
                assigned.add(j)

        group_id += 1

    return groups
```

**√âtape 2: Extraction Titre de Groupe**

```python
def _extract_group_title(self, concepts: List[Dict[str, Any]]) -> str:
    """Extrait un titre repr√©sentatif pour un groupe de concepts."""

    # Concat√©ner tous les contenus
    combined_text = " ".join([c["content"] for c in concepts])

    # Extraction mots-cl√©s avec regex ou TF-IDF simple
    words = combined_text.lower().split()
    word_freq = {}
    for word in words:
        if len(word) > 4:  # Ignorer mots courts
            word_freq[word] = word_freq.get(word, 0) + 1

    # Prendre les 2 mots les plus fr√©quents
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:2]
    title = " ".join([w[0].title() for w in top_words])

    return title or "Discussion"
```

**√âtape 3: Formatage Group√©**

```python
# Dans _build_temporal_history_context()
if len(consolidated_entries) > 2:
    groups = await self._group_concepts_by_theme(consolidated_entries)

    for group_key, concepts in groups.items():
        title = self._extract_group_title(concepts)
        lines.append(f"\n**[{title}]** Discussion r√©currente ({len(concepts)} √©changes)")

        for concept in concepts:
            dt = datetime.fromisoformat(concept["timestamp"].replace("Z", "+00:00"))
            date_str = f"{dt.day} {months[dt.month]} √† {dt.hour}h{dt.minute:02d}"
            preview = concept["content"][:60] + "..."
            lines.append(f"  - {date_str}: {preview}")
else:
    # Formatage standard si peu de concepts
    # ... code actuel
```

---

### Priorit√© 4: R√©sum√© Adaptatif (2h)

**Objectif:** Si thread tr√®s long (>100 messages), r√©sumer les plus anciens

**Impl√©mentation:**

```python
async def _build_temporal_history_context(
    self,
    thread_id: str,
    session_id: str,
    user_id: str,
    limit: int = 20,
    last_user_message: str = ""
) -> str:
    # ... code existant

    # NOUVEAU: Si beaucoup de messages, r√©sumer les plus anciens
    if len(all_events) > 30:
        # Garder 10 plus r√©cents en d√©tail
        recent_events = all_events[-10:]

        # R√©sumer les plus anciens
        older_events = all_events[:-10]
        summary = await self._summarize_old_events(older_events)

        lines.append(f"\n**[P√©riode ant√©rieure]** {summary}")
        lines.append("")

        # Formater uniquement les r√©cents
        for event in recent_events:
            # ... formatage standard
    else:
        # Formater tous les √©v√©nements
        for event in all_events:
            # ... formatage actuel
```

---

## üìã Plan d'Action Recommand√©

### Session 1: Cache & Optimisations de Base (2-3h)

**Ordre d'impl√©mentation:**
1. ‚úÖ Impl√©menter cache recherche consolid√©e
2. ‚úÖ Ajuster n_results dynamiquement
3. ‚úÖ Ajouter filtrage temporel (30 jours)
4. ‚úÖ Tester et valider performance
5. ‚úÖ Mesurer am√©lioration latence

**Crit√®res de succ√®s:**
- Cache hit rate > 30%
- Latence questions r√©p√©t√©es < 500ms
- Pas de warning ChromaDB `n_results`

### Session 2: M√©triques Prometheus (1-2h)

**Ordre d'impl√©mentation:**
1. ‚úÖ Cr√©er m√©triques dans `rag_metrics.py`
2. ‚úÖ Instrumenter service.py
3. ‚úÖ Tester export `/metrics`
4. ‚úÖ Cr√©er dashboard Grafana (optionnel)

**Crit√®res de succ√®s:**
- 6 m√©triques export√©es
- M√©triques visibles dans `/metrics`
- Donn√©es coh√©rentes apr√®s 10 requ√™tes test

### Session 3: Groupement Th√©matique (3-4h)

**Ordre d'impl√©mentation:**
1. ‚úÖ Impl√©menter clustering avec embeddings
2. ‚úÖ Extraction titres de groupes
3. ‚úÖ Formatage group√© dans contexte
4. ‚úÖ Tester avec concepts multiples
5. ‚úÖ Valider lisibilit√© pour Anima

**Crit√®res de succ√®s:**
- Groupes coh√©rents (similarit√© > 0.7)
- Titres pertinents (TF-IDF)
- Contexte plus concis et lisible

### Session 4: R√©sum√© Adaptatif (2h)

**Ordre d'impl√©mentation:**
1. ‚úÖ D√©tecter threads longs (>30 √©v√©nements)
2. ‚úÖ R√©sumer p√©riode ant√©rieure
3. ‚úÖ Garder 10 plus r√©cents en d√©tail
4. ‚úÖ Tester avec thread long

**Crit√®res de succ√®s:**
- R√©sum√© < 200 caract√®res
- 10 √©v√©nements r√©cents d√©taill√©s
- Contexte total < 2000 caract√®res

---

## üß™ Tests √† Effectuer

### Test 1: Cache Performance

**Action:**
```
1. Poser: "Quand avons-nous parl√© de Docker ?"
2. Noter temps r√©ponse (T1)
3. Reposer exactement la m√™me question
4. Noter temps r√©ponse (T2)
```

**R√©sultat attendu:**
- T1: ~4.8s (recherche ChromaDB)
- T2: ~2.5s (cache hit)
- Am√©lioration: ~48%

### Test 2: M√©triques Prometheus

**Action:**
```bash
# Poser 5 questions temporelles vari√©es
curl http://localhost:8000/metrics | grep memory_temporal
```

**R√©sultat attendu:**
```
memory_temporal_queries_total{detected="true"} 5.0
memory_temporal_concepts_found_total{count_range="3-5"} 3.0
memory_temporal_search_duration_seconds_sum 9.5
memory_temporal_cache_hit_rate 0.2
```

### Test 3: Groupement Th√©matique

**Action:**
```
Poser: "Quand avons-nous parl√© de mes projets ?"
(Avec 6+ concepts consolid√©s sur diff√©rents sujets)
```

**R√©sultat attendu:**
```
**[Docker Kubernetes]** Discussion r√©currente (3 √©changes)
  - 8 oct √† 14h32: Configuration Docker...
  - 10 oct √† 9h15: D√©ploiement Kubernetes...
  - 12 oct √† 16h20: Optimisation images...

**[CI/CD Pipeline]** Discussion r√©currente (2 √©changes)
  - 9 oct √† 11h05: Setup GitHub Actions...
  - 11 oct √† 14h00: Tests automatis√©s...
```

---

## üìö Fichiers Cl√©s √† Modifier

### Backend

1. **`src/backend/features/chat/service.py`**
   - Lignes ~1165-1199: Ajouter cache
   - Lignes ~1170: Ajuster n_results dynamiquement
   - Lignes ~1190-1266: Ajouter groupement th√©matique
   - Lignes ~1852: Instrumenter m√©triques

2. **`src/backend/features/chat/rag_metrics.py`** (ou nouveau)
   - D√©finir 6 nouvelles m√©triques Prometheus
   - Fonctions helper pour instrumentation

3. **`src/backend/features/memory/vector_service.py`** (optionnel)
   - Ajouter m√©thode `embed_batch()` si absente
   - Optimiser embeddings batch pour clustering

### Tests

4. **`tests/backend/features/chat/test_temporal_cache.py`** (nouveau)
   - Test cache hit/miss
   - Test √©viction
   - Test invalidation

5. **`tests/backend/features/chat/test_temporal_grouping.py`** (nouveau)
   - Test clustering concepts
   - Test extraction titres
   - Test formatage group√©

### Documentation

6. **`docs/architecture/MEMORY_PHASE3_IMPLEMENTATION.md`** (nouveau)
   - Documentation technique Phase 3
   - Architecture cache
   - M√©triques Prometheus
   - Algorithme clustering

7. **`CHANGELOG.md`**
   - Section Phase 3 avec toutes les am√©liorations

---

## üîç Points d'Attention

### Performance

**‚ö†Ô∏è Clustering Peut √ätre Co√ªteux**
- Embeddings batch: ~50-100ms pour 5 concepts
- Clustering: ~10-20ms avec sklearn
- Total overhead: ~100ms acceptable

**Solution si trop lent:**
- Cacher r√©sultats clustering aussi
- Limiter clustering √† >5 concepts seulement

### M√©moire

**‚ö†Ô∏è Cache Peut Cro√Ætre**
- 100 entr√©es √ó 5 concepts √ó 500 bytes ‚âà 250KB
- Acceptable, mais surveiller

**Solution:**
- √âviction agressive (100 entr√©es max)
- TTL court (5 minutes)
- Clear cache sur nouvelle consolidation

### ChromaDB

**‚ö†Ô∏è Filtres Temporels Pas Toujours Support√©s**
- ChromaDB where filters limit√©s
- Tester avant d'utiliser `$gte`

**Fallback:**
- Filtrage post-recherche en Python
- Acceptable car peu de r√©sultats (5-10)

---

## üìä M√©triques de Succ√®s Phase 3

### Performance

- ‚úÖ Cache hit rate > 30%
- ‚úÖ Latence questions r√©p√©t√©es < 2.5s (am√©lioration 50%)
- ‚úÖ Overhead clustering < 100ms

### Qualit√©

- ‚úÖ Groupes th√©matiques coh√©rents (similarit√© > 0.7)
- ‚úÖ Titres de groupes pertinents (validation manuelle)
- ‚úÖ Contexte enrichi < 2000 caract√®res

### Observabilit√©

- ‚úÖ 6 m√©triques Prometheus export√©es
- ‚úÖ Dashboard Grafana fonctionnel (optionnel)
- ‚úÖ Alertes configur√©es (optionnel)

---

## üöÄ Prochaines Phases (Post-Phase 3)

### Phase 4: Multi-Thread Temporal Context (Futur)

- Recherche concepts consolid√©s sur plusieurs threads
- Exemple: "Quand avons-nous parl√© de X dans tous mes threads ?"
- N√©cessite: index global cross-thread

### Phase 5: Pr√©diction Temporelle (Futur)

- D√©tection patterns temporels r√©currents
- Exemple: "Tu as parl√© de Docker chaque lundi les 3 derni√®res semaines"
- Suggestions proactives bas√©es sur calendrier

---

## üìû Si Besoin d'Aide

### Documentation de R√©f√©rence

1. **Phase 2 Compl√©t√©e:**
   - [MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md](MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md)
   - [test_production_temporal_memory_2025-10-15.md](../../reports/test_production_temporal_memory_2025-10-15.md)

2. **Code Actuel:**
   - [service.py:1130-1270](../../src/backend/features/chat/service.py#L1130-L1270)
   - [gardener.py:1640-1705](../../src/backend/features/memory/gardener.py#L1640-L1705)

3. **Tests Existants:**
   - [test_temporal_query.py](../../tests/backend/features/chat/test_temporal_query.py)

### Logs √† Surveiller

**Pendant D√©veloppement:**
```bash
# Cache operations
grep "TemporalCache" logs.txt

# Performance ChromaDB
grep "TemporalHistory.*enrichi" logs.txt

# M√©triques
curl http://localhost:8000/metrics | grep memory_temporal
```

---

## ‚úÖ Checklist Avant de Commencer

- [ ] Lire cette documentation compl√®te
- [ ] V√©rifier Phase 2 fonctionnelle (test production)
- [ ] Backend d√©marre sans erreur
- [ ] Comprendre architecture actuelle ([MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md](MEMORY_TEMPORAL_CONTEXT_IMPLEMENTATION.md))
- [ ] Choisir priorit√© (1, 2, 3, ou 4)
- [ ] Lancer backend en mode d√©veloppement
- [ ] Avoir thread de test avec 5+ concepts consolid√©s

---

**Bon courage pour la Phase 3 ! üöÄ**

L'objectif est d'am√©liorer la performance et l'observabilit√© tout en rendant le contexte temporel encore plus intelligent et utile.

**Prochaine √©tape:** Impl√©menter le cache (Priorit√© 1) pour obtenir des gains de performance imm√©diats.

---

**Cr√©√© le:** 2025-10-15
**Par:** Session de d√©veloppement Phase 2 - Pr√©paration Phase 3
**Statut:** ‚úÖ Pr√™t pour impl√©mentation
