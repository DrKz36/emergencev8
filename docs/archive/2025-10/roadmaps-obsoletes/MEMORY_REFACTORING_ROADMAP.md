# üß† ROADMAP - REFONTE ARCHITECTURE M√âMOIRE AGENTS

**Date cr√©ation**: 2025-10-18
**Objectif**: Refonte compl√®te du syst√®me de m√©moire pour garantir fiabilit√©, proactivit√© et coh√©rence
**Statut**: PLANIFI√â - Pr√™t pour ex√©cution

---

## üìã TABLE DES MATI√àRES

1. [Contexte & Diagnostic](#contexte--diagnostic)
2. [Architecture Actuelle](#architecture-actuelle)
3. [Probl√®mes Identifi√©s](#probl√®mes-identifi√©s)
4. [Plan d'Ex√©cution](#plan-dex√©cution)
   - [Sprint 1: Clarification Session vs Conversation](#sprint-1-clarification-session-vs-conversation)
   - [Sprint 2: Consolidation Auto Threads Archiv√©s](#sprint-2-consolidation-auto-threads-archiv√©s)
   - [Sprint 3: Rappel Proactif Unifi√©](#sprint-3-rappel-proactif-unifi√©)
   - [Sprint 4: Isolation Agent Stricte](#sprint-4-isolation-agent-stricte)
   - [Sprint 5: Interface Utilisateur](#sprint-5-interface-utilisateur)
5. [Validation & Tests](#validation--tests)
6. [R√©f√©rences Fichiers](#r√©f√©rences-fichiers)

---

## üîç CONTEXTE & DIAGNOSTIC

### Situation Actuelle
- ‚úÖ STM (Short-Term Memory) fonctionne pour sessions actives
- ‚úÖ LTM (Long-Term Memory) vectorielle ChromaDB op√©rationnelle
- ‚ùå **Threads archiv√©s NON consolid√©s automatiquement**
- ‚ùå **Agent ne "se souvient" pas spontan√©ment de conversations pass√©es**
- ‚ùå **Confusion conceptuelle entre Session (WS) et Conversation (persistante)**

### Demande Utilisateur
> "Je veux qu'ils puissent √† la demande se souvenir des conversations pass√©es, y.c celles archiv√©es"

### Probl√®me Principal
Architecture fragment√©e o√π:
- **Session** = Connexion WebSocket √©ph√©m√®re (dur√©e session)
- **Thread** = Conversation persistante (permanent)
- **Confusion**: `threads.session_id` lie conversation √† session WS √©ph√©m√®re

---

## üèóÔ∏è ARCHITECTURE ACTUELLE

### Composants Cl√©s

#### 1. STM - M√©moire Court Terme
```
SessionManager (RAM)
  ‚îî‚îÄ active_sessions: Dict[str, Session]
       ‚îî‚îÄ Session:
            ‚îú‚îÄ history: List[Dict]  (messages)
            ‚îú‚îÄ metadata: {summary, concepts, entities}
            ‚îî‚îÄ finalize_session() ‚Üí DB sessions table
```

**Fichiers**:
- [`src/backend/core/session_manager.py`](src/backend/core/session_manager.py) (lignes 51-943)
- [`src/backend/shared/models.py`](src/backend/shared/models.py) (lignes 29-43)

#### 2. LTM - M√©moire Long Terme
```
ChromaDB: "emergence_knowledge"
  ‚îú‚îÄ type: "concept"      (consolid√©s via MemoryGardener)
  ‚îú‚îÄ type: "preference"   (confidence >= 0.6)
  ‚îú‚îÄ type: "intent"
  ‚îú‚îÄ type: "constraint"
  ‚îî‚îÄ type: "fact"
```

**Fichiers**:
- [`src/backend/features/memory/gardener.py`](src/backend/features/memory/gardener.py) (lignes 1-300+)
- [`src/backend/features/memory/analyzer.py`](src/backend/features/memory/analyzer.py) (lignes 1-300+)
- [`src/backend/features/memory/vector_service.py`](src/backend/features/memory/vector_service.py)

#### 3. Conversations Persistantes
```sql
-- Table threads (conversations)
threads (
    id TEXT PRIMARY KEY,
    session_id TEXT,        -- ‚ö†Ô∏è Lien vers WS √©ph√©m√®re!
    user_id TEXT,
    type TEXT,              -- 'chat' | 'debate'
    archived INTEGER,       -- 0 | 1
    archived_at TEXT,
    last_message_at TEXT,
    message_count INTEGER
)

-- Table messages
messages (
    id TEXT PRIMARY KEY,
    thread_id TEXT,
    role TEXT,
    content TEXT,
    created_at TEXT,
    user_id TEXT
)
```

**Fichiers**:
- [`src/backend/core/database/schema.py`](src/backend/core/database/schema.py) (lignes 85-149)
- [`src/backend/core/database/queries.py`](src/backend/core/database/queries.py) (lignes 814-859)

#### 4. Rappel M√©moire
```python
# STM: Hydratation session depuis thread
SessionManager._hydrate_session_from_thread()  # ligne 470

# LTM: Construction contexte RAG
MemoryContextBuilder.build_memory_context()    # ligne 91

# Archives: Recherche temporelle (PAS UTILIS√â PROACTIVEMENT)
MemoryQueryTool.get_conversation_timeline()    # memory_query_tool.py
```

**Fichiers**:
- [`src/backend/features/chat/memory_ctx.py`](src/backend/features/chat/memory_ctx.py) (lignes 40-675)
- [`src/backend/features/memory/memory_query_tool.py`](src/backend/features/memory/memory_query_tool.py)

---

## üö® PROBL√àMES IDENTIFI√âS

### P1 - CRITIQUE: Confusion Session vs Conversation
**Sympt√¥me**: `threads.session_id` pointe vers session WS √©ph√©m√®re
**Impact**: Impossible de retrouver facilement conversations d'un utilisateur
**Fichiers**:
- `src/backend/core/database/schema.py:88` (d√©finition threads)
- `src/backend/core/database/queries.py:798` (create_thread)

### P2 - HAUTE: Threads Archiv√©s Non Consolid√©s
**Sympt√¥me**: Archivage thread ne d√©clenche pas consolidation LTM
**Impact**: Souvenirs perdus apr√®s archivage
**Fichiers**:
- `src/backend/core/database/queries.py:900-925` (update_thread)
- `src/backend/features/memory/router.py:2026-2122` (consolidate_archived endpoint existe mais pas utilis√©)

### P3 - HAUTE: Pas de Rappel Proactif Archives
**Sympt√¥me**: Agent ne "se souvient" pas spontan√©ment
**Impact**: Contexte appauvri, utilisateur doit demander explicitement
**Fichiers**:
- `src/backend/features/chat/memory_ctx.py:91-194` (build_memory_context)
- `src/backend/features/memory/memory_query_tool.py` (outil existe mais pas int√©gr√©)

### P4 - MOYENNE: Isolation Agent Incoh√©rente
**Sympt√¥me**: Concepts legacy sans `agent_id` visibles par tous agents
**Impact**: Risque confusion/hallucinations
**Fichiers**:
- `src/backend/features/chat/memory_ctx.py:642-674` (_result_matches_agent - filtrage permissif)
- `src/backend/core/memory/memory_sync.py:289-315` (filter_memories_by_agent)

### P5 - BASSE: Architecture Fragment√©e
**Sympt√¥me**: Pas de couche unifi√©e STM+LTM+Archives
**Impact**: Code complexe, maintenance difficile
**Solution**: Cr√©er `UnifiedMemoryRetriever`

---

## üéØ PLAN D'EX√âCUTION

---

## SPRINT 1: Clarification Session vs Conversation
**Dur√©e**: 2-3 jours
**Priorit√©**: üî¥ CRITIQUE
**Objectif**: S√©parer clairement Session (WS) et Conversation (persistante)

### √âtape 1.1: Ajouter `conversation_id` Canonique

**Fichiers √† modifier**:
1. `src/backend/core/database/schema.py`
2. `migrations/` (nouvelle migration)

**Actions**:

```python
# 1. Cr√©er migration: migrations/20251018_add_conversation_id.sql
"""
-- Ajouter colonne conversation_id
ALTER TABLE threads ADD COLUMN conversation_id TEXT;

-- Initialiser avec id existant (r√©trocompatibilit√©)
UPDATE threads SET conversation_id = id WHERE conversation_id IS NULL;

-- Index pour performance
CREATE INDEX IF NOT EXISTS idx_threads_user_conversation
ON threads(user_id, conversation_id);

-- Index composite pour requ√™tes fr√©quentes
CREATE INDEX IF NOT EXISTS idx_threads_user_type_conversation
ON threads(user_id, type, conversation_id);
"""

# 2. Mettre √† jour schema.py (ligne ~85)
# Ajouter conversation_id dans TABLE_DEFINITIONS pour threads:
"""
CREATE TABLE IF NOT EXISTS threads (
    id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,  -- ‚úÖ NOUVEAU: identifiant canonique
    session_id TEXT NOT NULL,       -- Gard√© pour r√©trocompat (lien WS source)
    user_id TEXT NOT NULL,
    type TEXT NOT NULL CHECK(type IN ('chat','debate')),
    ...
)
"""
```

**Validation**:
```bash
# Test migration
pytest tests/test_migrations.py::test_conversation_id_migration

# V√©rifier donn√©es
python -c "
from src.backend.core.database.manager import DatabaseManager
db = DatabaseManager('emergence.db')
await db.connect()
result = await db.fetch_all('SELECT id, conversation_id FROM threads LIMIT 5')
print(result)
"
```

### √âtape 1.2: Mettre √† Jour Code Utilisant threads

**Fichiers √† modifier**:
1. `src/backend/core/database/queries.py`
2. `src/backend/core/session_manager.py`
3. `src/backend/features/threads/router.py`

**Actions**:

```python
# 1. queries.py - Modifier create_thread (ligne ~798)
async def create_thread(
    db: DatabaseManager,
    thread_id: str,
    session_id: str,
    user_id: str,
    type_: str,
    *,
    title: Optional[str] = None,
    agent_id: Optional[str] = None,
    meta: Optional[Dict] = None,
    conversation_id: Optional[str] = None,  # ‚úÖ NOUVEAU
) -> str:
    # G√©n√©rer conversation_id si pas fourni
    if not conversation_id:
        conversation_id = thread_id  # Par d√©faut = thread_id

    await db.execute(
        """
        INSERT INTO threads (
            id, conversation_id, session_id, user_id, type,
            title, agent_id, meta, archived, created_at, updated_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0, ?, ?)
        """,
        (
            thread_id,
            conversation_id,  # ‚úÖ NOUVEAU
            session_id,
            user_id,
            type_,
            title,
            agent_id,
            json.dumps(meta or {}),
            now_iso,
            now_iso
        ),
        commit=True
    )
    return conversation_id

# 2. Ajouter helper pour r√©cup√©rer threads par conversation_id
async def get_threads_by_conversation(
    db: DatabaseManager,
    conversation_id: str,
    user_id: str,
    *,
    include_archived: bool = False
) -> List[Dict[str, Any]]:
    """R√©cup√®re tous threads d'une conversation (m√™me conversation, sessions diff√©rentes)"""
    clauses = ["conversation_id = ?", "user_id = ?"]
    params = [conversation_id, user_id]

    if not include_archived:
        clauses.append("archived = 0")

    query = f"""
        SELECT * FROM threads
        WHERE {' AND '.join(clauses)}
        ORDER BY created_at DESC
    """
    rows = await db.fetch_all(query, tuple(params))
    return [dict(r) for r in rows]
```

**Validation**:
```bash
# Test unitaire
pytest tests/core/database/test_queries.py::test_conversation_id_creation
pytest tests/core/database/test_queries.py::test_get_threads_by_conversation

# Test int√©gration
pytest tests/integration/test_conversation_continuity.py
```

### √âtape 1.3: Renommer Concepts dans Code (Optionnel mais Recommand√©)

**Fichiers √† modifier**:
1. `src/backend/shared/models.py` (renommer Session ‚Üí WebSocketSession)
2. Mise √† jour imports dans tous fichiers

**Actions**:
```python
# models.py (ligne 29)
class WebSocketSession(BaseModel):  # ‚úÖ Renomm√© depuis Session
    """
    Repr√©sente une session WebSocket active (√©ph√©m√®re).
    Distinction claire avec Conversation (persistante).
    """
    id: str
    user_id: str
    start_time: datetime
    end_time: Optional[datetime]
    last_activity: datetime
    history: List[Dict[str, Any]]
    metadata: Dict[str, Any]

# Alias r√©trocompatibilit√©
Session = WebSocketSession  # Pour migration en douceur
```

**Note**: Cette √©tape peut √™tre report√©e au Sprint 4 si besoin de livrer rapidement.

### ‚úÖ Crit√®res de Succ√®s Sprint 1
- [ ] Migration `conversation_id` appliqu√©e sans erreur
- [ ] Toutes conversations existantes ont `conversation_id = id`
- [ ] Nouveaux threads cr√©√©s avec `conversation_id`
- [ ] Requ√™tes `get_threads_by_conversation()` fonctionnelles
- [ ] Tests unitaires passent (100% coverage nouvelles fonctions)
- [ ] R√©trocompatibilit√© pr√©serv√©e (`session_id` toujours utilisable)

---

## SPRINT 2: Consolidation Auto Threads Archiv√©s
**Dur√©e**: 3-4 jours
**Priorit√©**: üü† HAUTE
**Objectif**: Garantir que TOUTE conversation archiv√©e soit consolid√©e en LTM

### √âtape 2.1: Hook Automatique lors Archivage

**Fichiers √† modifier**:
1. `src/backend/core/database/queries.py` (update_thread)
2. `src/backend/features/memory/gardener.py`

**Actions**:

```python
# 1. queries.py - Modifier update_thread (ligne ~900)
async def update_thread(
    db: DatabaseManager,
    thread_id: str,
    session_id: Optional[str],
    *,
    user_id: Optional[str] = None,
    title: Optional[str] = None,
    agent_id: Optional[str] = None,
    archived: Optional[bool] = None,
    meta: Optional[Dict[str, Any]] = None,
    gardener = None,  # ‚úÖ NOUVEAU: injection MemoryGardener
) -> None:
    fields: list[str] = []
    params: list[Any] = []

    # ... code existant ...

    if archived is not None:
        fields.append("archived = ?")
        params.append(1 if archived else 0)

        # ‚úÖ NOUVEAU: Si archivage, ajouter timestamp + raison
        if archived:
            fields.append("archived_at = ?")
            params.append(datetime.now(timezone.utc).isoformat())

            # Raison par d√©faut si pas dans meta
            archival_reason = (meta or {}).get('archival_reason', 'manual_archive')
            fields.append("archival_reason = ?")
            params.append(archival_reason)

    # ... code existant de mise √† jour ...

    # ‚úÖ NOUVEAU: D√©clencher consolidation si archivage
    if archived and gardener:
        try:
            logger.info(f"Thread {thread_id} archiv√©, d√©clenchement consolidation LTM...")
            await gardener._tend_single_thread(
                thread_id=thread_id,
                session_id=session_id,
                user_id=user_id or _get_user_from_thread(db, thread_id)
            )

            # Marquer comme consolid√©
            await db.execute(
                "UPDATE threads SET consolidated_at = ? WHERE id = ?",
                (datetime.now(timezone.utc).isoformat(), thread_id),
                commit=True
            )
            logger.info(f"Thread {thread_id} consolid√© en LTM avec succ√®s")
        except Exception as e:
            logger.error(f"√âchec consolidation thread {thread_id}: {e}", exc_info=True)
            # Ne pas bloquer l'archivage si consolidation √©choue
```

**Injection MemoryGardener**:
```python
# 2. Dans router threads ou container
from backend.features.memory.gardener import MemoryGardener

@router.patch("/threads/{thread_id}/archive")
async def archive_thread(
    thread_id: str,
    request: Request,
    reason: Optional[str] = None
):
    container = request.app.state.service_container
    gardener = MemoryGardener(
        db_manager=container.db_manager(),
        vector_service=container.vector_service(),
        memory_analyzer=container.memory_analyzer()
    )

    await queries.update_thread(
        db=container.db_manager(),
        thread_id=thread_id,
        session_id=None,
        archived=True,
        meta={"archival_reason": reason or "manual"},
        gardener=gardener  # ‚úÖ Injection
    )
```

**Validation**:
```python
# Test unitaire
async def test_archive_thread_triggers_consolidation():
    """V√©rifier que archivage d√©clenche consolidation LTM"""
    # Setup
    db = DatabaseManager(':memory:')
    gardener = MemoryGardener(db, vector_service, analyzer)

    # Cr√©er thread avec messages
    thread_id = await create_thread(db, ...)
    await add_message(db, thread_id, "Message test", ...)

    # Archiver (doit d√©clencher consolidation)
    await update_thread(db, thread_id, archived=True, gardener=gardener)

    # V√©rifier consolidation
    thread = await get_thread(db, thread_id)
    assert thread['consolidated_at'] is not None

    # V√©rifier concepts en ChromaDB
    concepts = vector_service.get(where={"thread_id": thread_id})
    assert len(concepts['ids']) > 0
```

### √âtape 2.2: Job Batch Rattrapage Archives Existants

**Fichier √† cr√©er**:
`src/backend/cli/consolidate_all_archives.py`

**Actions**:

```python
# consolidate_all_archives.py
#!/usr/bin/env python3
"""
Script de migration ponctuel: Consolide tous threads archiv√©s non trait√©s.

Usage:
    python src/backend/cli/consolidate_all_archives.py --user-id <user_id>
    python src/backend/cli/consolidate_all_archives.py --all  # Admin only
"""
import asyncio
import argparse
import logging
from datetime import datetime, timezone

from backend.core.database.manager import DatabaseManager
from backend.features.memory.gardener import MemoryGardener
from backend.features.memory.vector_service import VectorService
from backend.features.memory.analyzer import MemoryAnalyzer
from backend.core.database import queries

logger = logging.getLogger(__name__)

async def is_already_consolidated(vector_service, thread_id: str) -> bool:
    """V√©rifie si thread d√©j√† consolid√© en cherchant concepts dans ChromaDB"""
    try:
        collection = vector_service.get_or_create_collection("emergence_knowledge")
        result = collection.get(
            where={"thread_id": thread_id},
            limit=1
        )
        ids = result.get("ids") or []
        if isinstance(ids, list) and len(ids) > 0:
            if isinstance(ids[0], list):
                return len(ids[0]) > 0
            return True
        return False
    except Exception as e:
        logger.warning(f"Check consolidation failed for {thread_id}: {e}")
        return False

async def consolidate_all_archives(
    db: DatabaseManager,
    gardener: MemoryGardener,
    vector_service: VectorService,
    *,
    user_id: Optional[str] = None,
    limit: int = 1000,
    force: bool = False
):
    """Consolide tous threads archiv√©s non trait√©s"""

    # R√©cup√©rer threads archiv√©s
    logger.info(f"R√©cup√©ration threads archiv√©s (user_id={user_id}, limit={limit})...")
    threads = await queries.get_threads(
        db,
        session_id=None,
        user_id=user_id,
        archived_only=True,
        limit=limit
    )

    logger.info(f"Trouv√© {len(threads)} thread(s) archiv√©(s)")

    consolidated = 0
    skipped = 0
    errors = []

    for i, thread in enumerate(threads, 1):
        thread_id = thread.get('id')
        if not thread_id:
            continue

        logger.info(f"[{i}/{len(threads)}] Processing thread {thread_id[:8]}...")

        try:
            # V√©rifier si d√©j√† consolid√©
            if not force and await is_already_consolidated(vector_service, thread_id):
                logger.info(f"  ‚Üí D√©j√† consolid√©, skip")
                skipped += 1
                continue

            # Consolider
            result = await gardener._tend_single_thread(
                thread_id=thread_id,
                session_id=thread.get('session_id'),
                user_id=thread.get('user_id')
            )

            new_concepts = result.get('new_concepts', 0)
            if new_concepts > 0:
                logger.info(f"  ‚Üí Consolid√©: {new_concepts} concepts")
                consolidated += 1

                # Marquer comme consolid√©
                await db.execute(
                    "UPDATE threads SET consolidated_at = ? WHERE id = ?",
                    (datetime.now(timezone.utc).isoformat(), thread_id),
                    commit=True
                )
            else:
                logger.info(f"  ‚Üí Aucun concept extrait")
                skipped += 1

        except Exception as e:
            logger.error(f"  ‚Üí ERREUR: {e}", exc_info=True)
            errors.append({
                'thread_id': thread_id,
                'error': str(e)
            })

    # Rapport final
    logger.info(f"""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë  CONSOLIDATION BATCH TERMIN√âE         ‚ïë
    ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
    ‚ïë  Total threads: {len(threads):4d}               ‚ïë
    ‚ïë  Consolid√©s:    {consolidated:4d}               ‚ïë
    ‚ïë  Skipped:       {skipped:4d}               ‚ïë
    ‚ïë  Erreurs:       {len(errors):4d}               ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    if errors:
        logger.error(f"Erreurs d√©taill√©es:\n{errors}")

    return {
        'total': len(threads),
        'consolidated': consolidated,
        'skipped': skipped,
        'errors': errors
    }

async def main():
    parser = argparse.ArgumentParser(description="Consolide threads archiv√©s")
    parser.add_argument('--user-id', help="User ID √† traiter")
    parser.add_argument('--all', action='store_true', help="Tous utilisateurs (admin)")
    parser.add_argument('--limit', type=int, default=1000, help="Limite threads")
    parser.add_argument('--force', action='store_true', help="Forcer reconsolidation")
    parser.add_argument('--db', default='emergence.db', help="Chemin DB")
    args = parser.parse_args()

    # Setup
    logging.basicConfig(level=logging.INFO)
    db = DatabaseManager(args.db)
    await db.connect()

    vector_service = VectorService()
    analyzer = MemoryAnalyzer(db, enable_offline_mode=True)
    gardener = MemoryGardener(db, vector_service, analyzer)

    # Ex√©cution
    user_id = None if args.all else args.user_id
    if not user_id and not args.all:
        logger.error("--user-id ou --all requis")
        return 1

    await consolidate_all_archives(
        db, gardener, vector_service,
        user_id=user_id,
        limit=args.limit,
        force=args.force
    )

    await db.close()
    return 0

if __name__ == '__main__':
    exit(asyncio.run(main()))
```

**Ex√©cution**:
```bash
# Consolider archives d'un utilisateur sp√©cifique
python src/backend/cli/consolidate_all_archives.py --user-id user_123

# Consolider TOUS archives (admin)
python src/backend/cli/consolidate_all_archives.py --all --limit 5000

# Forcer reconsolidation (migration)
python src/backend/cli/consolidate_all_archives.py --all --force
```

**Validation**:
```bash
# V√©rifier consolidation
python -c "
from src.backend.core.database.manager import DatabaseManager
db = DatabaseManager('emergence.db')
await db.connect()

# Threads archiv√©s
total = await db.fetch_one('SELECT COUNT(*) as c FROM threads WHERE archived=1')
print(f'Total archived: {total[\"c\"]}')

# Threads consolid√©s
consolidated = await db.fetch_one('SELECT COUNT(*) as c FROM threads WHERE archived=1 AND consolidated_at IS NOT NULL')
print(f'Consolidated: {consolidated[\"c\"]}')
"
```

### √âtape 2.3: Ajouter Colonne `consolidated_at`

**Fichier migration**: `migrations/20251018_add_consolidated_at.sql`

```sql
-- Ajouter colonne pour tracker consolidation
ALTER TABLE threads ADD COLUMN consolidated_at TEXT;

-- Index pour requ√™tes de threads non consolid√©s
CREATE INDEX IF NOT EXISTS idx_threads_archived_not_consolidated
ON threads(archived, consolidated_at)
WHERE archived = 1 AND consolidated_at IS NULL;
```

### ‚úÖ Crit√®res de Succ√®s Sprint 2
- [ ] Hook consolidation automatique lors archivage fonctionne
- [ ] Script batch `consolidate_all_archives.py` ex√©cut√© avec succ√®s
- [ ] 100% threads archiv√©s ont `consolidated_at` non NULL
- [ ] Concepts visibles dans ChromaDB pour chaque thread archiv√©
- [ ] Tests unitaires passent (consolidation automatique)
- [ ] Monitoring: m√©trique `threads_consolidated_total` ajout√©e

---

## SPRINT 3: Rappel Proactif Unifi√©
**Dur√©e**: 4-5 jours
**Priorit√©**: üü† HAUTE
**Objectif**: Agent "se souvient" spontan√©ment de conversations pass√©es pertinentes

### √âtape 3.1: Cr√©er `UnifiedMemoryRetriever`

**Fichier √† cr√©er**: `src/backend/features/memory/unified_retriever.py`

**Actions**:

```python
# unified_retriever.py
"""
UnifiedMemoryRetriever - Couche unifi√©e de rappel m√©moire.

R√©cup√®re contexte depuis:
1. STM (session active)
2. LTM (concepts/pr√©f√©rences vectoriels)
3. Archives (conversations pass√©es pertinentes)
"""
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class MemoryContext:
    """Contexte m√©moire unifi√© pour injection dans prompt"""

    def __init__(self):
        self.stm_history: List[Dict] = []
        self.ltm_concepts: List[Dict] = []
        self.ltm_preferences: List[Dict] = []
        self.archived_conversations: List[Dict] = []

    def to_prompt_sections(self) -> List[tuple[str, str]]:
        """Formatte pour injection dans prompt syst√®me"""
        sections = []

        # Pr√©f√©rences actives (prioritaire)
        if self.ltm_preferences:
            prefs_text = "\n".join([
                f"- {p['text']}" for p in self.ltm_preferences[:5]
            ])
            sections.append(("Pr√©f√©rences actives", prefs_text))

        # Conversations pass√©es pertinentes
        if self.archived_conversations:
            conv_text = "\n".join([
                f"- {c['date']}: {c['summary']}"
                for c in self.archived_conversations[:3]
            ])
            sections.append(("Conversations pass√©es pertinentes", conv_text))

        # Concepts pertinents
        if self.ltm_concepts:
            concepts_text = "\n".join([
                f"- {c['text']}" for c in self.ltm_concepts[:5]
            ])
            sections.append(("Connaissances pertinentes", concepts_text))

        return sections

    def to_markdown(self) -> str:
        """Formate en markdown pour injection prompt"""
        sections = self.to_prompt_sections()
        parts = []
        for title, body in sections:
            if body.strip():
                parts.append(f"### {title}\n{body.strip()}")
        return "\n\n".join(parts)


class UnifiedMemoryRetriever:
    """
    R√©cup√©rateur unifi√© de m√©moire agent.
    Centralise acc√®s STM + LTM + Archives.
    """

    def __init__(
        self,
        session_manager,
        vector_service,
        db_manager,
        memory_query_tool
    ):
        self.session_manager = session_manager
        self.vector_service = vector_service
        self.db = db_manager
        self.memory_query_tool = memory_query_tool

    async def retrieve_context(
        self,
        user_id: str,
        agent_id: str,
        session_id: str,
        current_query: str,
        *,
        include_stm: bool = True,
        include_ltm: bool = True,
        include_archives: bool = True,
        top_k_concepts: int = 5,
        top_k_archives: int = 3
    ) -> MemoryContext:
        """
        R√©cup√®re contexte unifi√© pour agent.

        Args:
            user_id: Identifiant utilisateur
            agent_id: Identifiant agent (anima/neo/nexus)
            session_id: Session WebSocket active
            current_query: Requ√™te utilisateur actuelle
            include_stm: Inclure STM (d√©faut: True)
            include_ltm: Inclure LTM concepts (d√©faut: True)
            include_archives: Inclure archives (d√©faut: True)
            top_k_concepts: Nombre concepts LTM (d√©faut: 5)
            top_k_archives: Nombre conversations archiv√©es (d√©faut: 3)

        Returns:
            MemoryContext avec sections remplies
        """
        context = MemoryContext()

        # 1. STM: Historique session active
        if include_stm:
            context.stm_history = await self._get_stm_context(session_id)

        # 2. LTM: Pr√©f√©rences + concepts pertinents
        if include_ltm:
            ltm_results = await self._get_ltm_context(
                user_id, agent_id, current_query, top_k=top_k_concepts
            )
            context.ltm_preferences = ltm_results['preferences']
            context.ltm_concepts = ltm_results['concepts']

        # 3. üÜï Archives: Conversations pass√©es pertinentes
        if include_archives:
            context.archived_conversations = await self._get_archived_context(
                user_id, agent_id, current_query, limit=top_k_archives
            )

        logger.info(
            f"[UnifiedRetriever] Context r√©cup√©r√©: "
            f"STM={len(context.stm_history)} msgs, "
            f"LTM={len(context.ltm_concepts)} concepts, "
            f"Prefs={len(context.ltm_preferences)}, "
            f"Archives={len(context.archived_conversations)} convs"
        )

        return context

    async def _get_stm_context(self, session_id: str) -> List[Dict]:
        """R√©cup√®re historique session active"""
        try:
            return self.session_manager.get_full_history(session_id)
        except Exception as e:
            logger.warning(f"STM retrieval failed: {e}")
            return []

    async def _get_ltm_context(
        self, user_id: str, agent_id: str, query: str, top_k: int
    ) -> Dict[str, List[Dict]]:
        """R√©cup√®re pr√©f√©rences + concepts depuis ChromaDB"""
        try:
            collection = self.vector_service.get_or_create_collection(
                "emergence_knowledge"
            )

            # Pr√©f√©rences actives (confidence >= 0.6)
            prefs = collection.get(
                where={
                    "$and": [
                        {"user_id": user_id},
                        {"agent_id": agent_id},
                        {"type": "preference"},
                        {"confidence": {"$gte": 0.6}}
                    ]
                },
                include=["documents", "metadatas"]
            )

            preferences = [
                {
                    'text': doc,
                    'confidence': meta.get('confidence', 0.5),
                    'topic': meta.get('topic', 'general')
                }
                for doc, meta in zip(
                    prefs.get('documents', []),
                    prefs.get('metadatas', [])
                )
            ]

            # Concepts pertinents (requ√™te vectorielle)
            concepts_results = self.vector_service.query(
                collection=collection,
                query_text=query,
                n_results=top_k,
                where_filter={
                    "$and": [
                        {"user_id": user_id},
                        {"agent_id": agent_id},
                        {"type": "concept"}
                    ]
                }
            )

            concepts = [
                {
                    'text': r.get('text', ''),
                    'score': r.get('score', 0),
                    'metadata': r.get('metadata', {})
                }
                for r in (concepts_results or [])
            ]

            return {
                'preferences': preferences,
                'concepts': concepts
            }

        except Exception as e:
            logger.error(f"LTM retrieval failed: {e}", exc_info=True)
            return {'preferences': [], 'concepts': []}

    async def _get_archived_context(
        self, user_id: str, agent_id: str, query: str, limit: int
    ) -> List[Dict]:
        """
        üÜï R√©cup√®re conversations archiv√©es pertinentes.

        Strat√©gie:
        1. Si threads archiv√©s consolid√©s en LTM ‚Üí d√©j√† dans concepts
        2. Sinon, recherche fulltext dans messages archiv√©s
        """
        try:
            # Recherche temporelle dans messages archiv√©s
            from backend.core.temporal_search import TemporalSearch

            temporal = TemporalSearch(db_manager=self.db)
            messages = await temporal.search_messages(
                query=query,
                limit=limit * 5,  # Fetch more pour filtrer
                user_id=user_id
            )

            # Filtrer messages de threads archiv√©s seulement
            from backend.core.database import queries

            archived_messages = []
            seen_threads = set()

            for msg in messages:
                thread_id = msg.get('thread_id')
                if not thread_id or thread_id in seen_threads:
                    continue

                # V√©rifier si thread archiv√©
                thread = await queries.get_thread_any(
                    self.db, thread_id, session_id=None, user_id=user_id
                )

                if thread and thread.get('archived'):
                    seen_threads.add(thread_id)
                    archived_messages.append({
                        'thread_id': thread_id,
                        'title': thread.get('title', 'Sans titre'),
                        'date': self._format_date(thread.get('archived_at')),
                        'summary': msg.get('content', '')[:200],
                        'relevance': msg.get('score', 0)
                    })

                if len(archived_messages) >= limit:
                    break

            # Trier par pertinence
            archived_messages.sort(
                key=lambda x: x['relevance'],
                reverse=True
            )

            return archived_messages[:limit]

        except Exception as e:
            logger.error(f"Archive retrieval failed: {e}", exc_info=True)
            return []

    @staticmethod
    def _format_date(iso_date: Optional[str]) -> str:
        """Formatte date ISO en fran√ßais naturel"""
        if not iso_date:
            return ""
        try:
            dt = datetime.fromisoformat(iso_date.replace('Z', '+00:00'))
            months = ["", "janv", "f√©v", "mars", "avr", "mai", "juin",
                     "juil", "ao√ªt", "sept", "oct", "nov", "d√©c"]
            month = months[dt.month] if 1 <= dt.month <= 12 else str(dt.month)
            return f"{dt.day} {month}"
        except:
            return iso_date[:10]
```

**Validation**:
```python
# Test unitaire
async def test_unified_retriever_full_context():
    """Test r√©cup√©ration contexte complet"""
    retriever = UnifiedMemoryRetriever(session_mgr, vector, db, query_tool)

    context = await retriever.retrieve_context(
        user_id="user_123",
        agent_id="anima",
        session_id="sess_456",
        current_query="Comment d√©ployer avec Docker?"
    )

    # V√©rifications
    assert isinstance(context, MemoryContext)
    assert len(context.ltm_preferences) > 0  # Pr√©f√©rences pr√©sentes
    assert len(context.ltm_concepts) > 0     # Concepts pertinents
    assert len(context.archived_conversations) > 0  # Archives trouv√©es

    # Format markdown
    markdown = context.to_markdown()
    assert "### Pr√©f√©rences actives" in markdown
    assert "### Conversations pass√©es pertinentes" in markdown
```

### √âtape 3.2: Int√©grer dans `MemoryContextBuilder`

**Fichier √† modifier**: `src/backend/features/chat/memory_ctx.py`

**Actions**:

```python
# memory_ctx.py (ligne ~40)
class MemoryContextBuilder:
    def __init__(self, session_manager, vector_service):
        self.session_manager = session_manager
        self.vector_service = vector_service

        # üÜï Ajouter UnifiedMemoryRetriever
        from backend.features.memory.unified_retriever import UnifiedMemoryRetriever
        from backend.features.memory.memory_query_tool import MemoryQueryTool

        # Obtenir db_manager depuis session_manager
        db_manager = getattr(session_manager, 'db_manager', None)

        if db_manager:
            memory_query_tool = MemoryQueryTool(vector_service)
            self.unified_retriever = UnifiedMemoryRetriever(
                session_manager=session_manager,
                vector_service=vector_service,
                db_manager=db_manager,
                memory_query_tool=memory_query_tool
            )
        else:
            logger.warning("db_manager non disponible, UnifiedRetriever d√©sactiv√©")
            self.unified_retriever = None

        # Cache pr√©f√©rences (existant)
        self._prefs_cache: Dict[str, Tuple[str, datetime]] = {}
        self._cache_ttl = timedelta(minutes=5)

        logger.info(
            "[MemoryContextBuilder] Initialized with UnifiedRetriever "
            f"(enabled={self.unified_retriever is not None})"
        )

    async def build_memory_context(
        self,
        session_id: str,
        last_user_message: str,
        top_k: int = 5,
        agent_id: Optional[str] = None,
        use_unified_retriever: bool = True  # ‚úÖ NOUVEAU: flag activation
    ) -> str:
        """
        Construit contexte m√©moire pour injection prompt.

        üÜï Si use_unified_retriever=True, utilise UnifiedRetriever pour:
        - Pr√©f√©rences (cache 5min)
        - Concepts vectoriels pertinents
        - Conversations archiv√©es pertinentes
        """
        try:
            if not last_user_message:
                return ""

            uid = self.try_get_user_id(session_id)

            # üÜï NOUVEAU: Utiliser UnifiedRetriever si disponible
            if use_unified_retriever and self.unified_retriever and uid and agent_id:
                logger.info("[MemoryContext] Using UnifiedRetriever for context")

                context = await self.unified_retriever.retrieve_context(
                    user_id=uid,
                    agent_id=agent_id,
                    session_id=session_id,
                    current_query=last_user_message,
                    top_k_concepts=top_k,
                    top_k_archives=3  # Top 3 conversations archiv√©es
                )

                return context.to_markdown()

            # FALLBACK: Comportement existant (si unified_retriever d√©sactiv√©)
            logger.info("[MemoryContext] Using legacy retrieval (no UnifiedRetriever)")

            knowledge_name = os.getenv(
                "EMERGENCE_KNOWLEDGE_COLLECTION", "emergence_knowledge"
            )
            knowledge_col = self.vector_service.get_or_create_collection(knowledge_name)

            sections = []

            # Pr√©f√©rences (cache existant)
            if uid:
                prefs = self._fetch_active_preferences_cached(knowledge_col, uid)
                if prefs:
                    sections.append(("Pr√©f√©rences actives", prefs))

            # Meta query detection (existant)
            if uid and self._is_meta_query(last_user_message):
                chronological = await self._build_chronological_context(
                    uid, last_user_message, agent_id=agent_id
                )
                if chronological:
                    sections.append(("Historique des sujets abord√©s", chronological))
                return self.merge_blocks(sections)

            # Vector search (existant)
            where_filter = {"user_id": uid} if uid else None
            results = self.vector_service.query(
                collection=knowledge_col,
                query_text=last_user_message,
                n_results=top_k * 2,
                where_filter=where_filter,
            )

            if results and agent_id:
                results = [
                    r for r in results
                    if self._result_matches_agent(r, agent_id.lower())
                ][:top_k]

            if results:
                weighted = self._apply_temporal_weighting(results)
                lines = [
                    f"- {r.get('text', '').strip()}{self._format_temporal_hint(r.get('metadata', {}))}"
                    for r in weighted[:top_k]
                    if r.get('text', '').strip()
                ]
                if lines:
                    sections.append(("Connaissances pertinentes", "\n".join(lines)))

            return self.merge_blocks(sections)

        except Exception as e:
            logger.warning(f"build_memory_context: {e}")
            return ""
```

**Flag d'activation**:
```python
# Utilisation dans chat service
memory_context = await memory_ctx_builder.build_memory_context(
    session_id=session_id,
    last_user_message=user_message,
    agent_id=agent_id,
    use_unified_retriever=True  # ‚úÖ Activer nouveau syst√®me
)
```

**Validation**:
```python
# Test end-to-end
async def test_memory_context_includes_archives():
    """V√©rifier que contexte inclut conversations archiv√©es"""
    # Setup: Cr√©er conversation archiv√©e avec contenu pertinent
    thread_id = await create_archived_thread_with_content(
        db, user_id="user_123",
        content="Nous avons discut√© de Docker et Kubernetes"
    )

    # Consolider en LTM
    await gardener._tend_single_thread(thread_id, ...)

    # Nouvelle requ√™te similaire
    context = await memory_ctx.build_memory_context(
        session_id="new_session",
        last_user_message="Comment d√©ployer avec Docker?",
        agent_id="anima",
        use_unified_retriever=True
    )

    # V√©rifier mention conversation pass√©e
    assert "Conversations pass√©es pertinentes" in context
    assert "Docker" in context or "Kubernetes" in context
```

### √âtape 3.3: Ajouter Feature Flag & Monitoring

**Fichier**: `.env` et code monitoring

```bash
# .env - Feature flag
ENABLE_UNIFIED_MEMORY_RETRIEVER=true  # Activation progressive
UNIFIED_RETRIEVER_INCLUDE_ARCHIVES=true
UNIFIED_RETRIEVER_TOP_K_ARCHIVES=3
```

**Monitoring Prometheus**:
```python
# unified_retriever.py - Ajouter m√©triques
from prometheus_client import Counter, Histogram

UNIFIED_RETRIEVER_CALLS = Counter(
    'unified_retriever_calls_total',
    'Nombre appels UnifiedRetriever',
    ['agent_id', 'source']  # source: stm, ltm, archives
)

UNIFIED_RETRIEVER_DURATION = Histogram(
    'unified_retriever_duration_seconds',
    'Dur√©e r√©cup√©ration contexte',
    ['source']
)

# Dans retrieve_context()
with UNIFIED_RETRIEVER_DURATION.labels(source='total').time():
    # ... code ...

    if include_archives:
        with UNIFIED_RETRIEVER_DURATION.labels(source='archives').time():
            context.archived_conversations = await self._get_archived_context(...)
        UNIFIED_RETRIEVER_CALLS.labels(
            agent_id=agent_id,
            source='archives'
        ).inc()
```

### ‚úÖ Crit√®res de Succ√®s Sprint 3
- [ ] `UnifiedMemoryRetriever` cr√©√© et test√© unitairement
- [ ] Int√©gration dans `MemoryContextBuilder` fonctionnelle
- [ ] Conversations archiv√©es apparaissent dans contexte agent
- [ ] Feature flag permet activation/d√©sactivation
- [ ] M√©triques Prometheus op√©rationnelles
- [ ] Tests end-to-end passent (rappel proactif)
- [ ] Performance: Latence contexte < 200ms (P95)

---

## SPRINT 4: Isolation Agent Stricte
**Dur√©e**: 2-3 jours
**Priorit√©**: üü° MOYENNE
**Objectif**: Garantir s√©paration stricte m√©moire entre agents (Anima/Neo/Nexus)

### √âtape 4.1: Backfill `agent_id` Concepts Legacy

**Fichier √† cr√©er**: `src/backend/cli/backfill_agent_ids.py`

**Actions**:

```python
# backfill_agent_ids.py
"""
Backfill agent_id pour concepts legacy (sans agent_id).
Inf√©rence bas√©e sur thread_ids source.
"""
import asyncio
import logging
from typing import Optional

from backend.features.memory.vector_service import VectorService
from backend.core.database.manager import DatabaseManager
from backend.core.database import queries

logger = logging.getLogger(__name__)

async def infer_agent_from_thread(db: DatabaseManager, thread_id: str) -> Optional[str]:
    """Inf√®re agent_id depuis thread source"""
    try:
        thread = await queries.get_thread_any(db, thread_id)
        if thread:
            agent_id = thread.get('agent_id')
            if agent_id:
                return agent_id.lower()
        return 'anima'  # D√©faut
    except Exception as e:
        logger.warning(f"Failed to infer agent from thread {thread_id}: {e}")
        return 'anima'

async def backfill_missing_agent_ids(
    vector_service: VectorService,
    db: DatabaseManager,
    *,
    user_id: Optional[str] = None,
    dry_run: bool = False
):
    """Backfill agent_id pour concepts sans agent_id"""

    collection = vector_service.get_or_create_collection("emergence_knowledge")

    # R√©cup√©rer concepts sans agent_id
    where = {"type": "concept"}
    if user_id:
        where = {"$and": [where, {"user_id": user_id}]}

    results = collection.get(
        where=where,
        include=["metadatas"]
    )

    concept_ids = results.get('ids', [])
    metadatas = results.get('metadatas', [])

    logger.info(f"Trouv√© {len(concept_ids)} concepts √† analyser")

    updated = 0
    skipped = 0

    for concept_id, meta in zip(concept_ids, metadatas):
        # Skip si agent_id d√©j√† pr√©sent
        if meta.get('agent_id'):
            skipped += 1
            continue

        # Inf√©rer depuis thread_ids
        thread_ids = meta.get('thread_ids', [])
        if not thread_ids:
            logger.debug(f"Concept {concept_id[:8]}... sans thread_ids, skip")
            skipped += 1
            continue

        # Prendre premier thread_id
        inferred_agent = await infer_agent_from_thread(db, thread_ids[0])

        logger.info(
            f"Concept {concept_id[:8]}... ‚Üí agent_id inf√©r√©: {inferred_agent}"
        )

        if not dry_run:
            # Mettre √† jour
            updated_meta = {**meta, 'agent_id': inferred_agent}
            collection.update(
                ids=[concept_id],
                metadatas=[updated_meta]
            )
            updated += 1
        else:
            logger.info(f"  [DRY-RUN] Aurait mis √† jour avec agent_id={inferred_agent}")
            updated += 1

    logger.info(f"""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë  BACKFILL AGENT_ID TERMIN√â            ‚ïë
    ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
    ‚ïë  Total concepts: {len(concept_ids):4d}              ‚ïë
    ‚ïë  Updated:        {updated:4d}              ‚ïë
    ‚ïë  Skipped:        {skipped:4d}              ‚ïë
    ‚ïë  Dry-run:        {str(dry_run):5s}             ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

async def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--user-id', help="User ID √† traiter")
    parser.add_argument('--dry-run', action='store_true', help="Simulation")
    parser.add_argument('--db', default='emergence.db')
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    db = DatabaseManager(args.db)
    await db.connect()

    vector_service = VectorService()

    await backfill_missing_agent_ids(
        vector_service, db,
        user_id=args.user_id,
        dry_run=args.dry_run
    )

    await db.close()

if __name__ == '__main__':
    asyncio.run(main())
```

**Ex√©cution**:
```bash
# Dry-run (simulation)
python src/backend/cli/backfill_agent_ids.py --dry-run

# Ex√©cution r√©elle
python src/backend/cli/backfill_agent_ids.py

# Pour un utilisateur sp√©cifique
python src/backend/cli/backfill_agent_ids.py --user-id user_123
```

### √âtape 4.2: Modifier Filtrage pour Mode Strict

**Fichier √† modifier**: `src/backend/features/chat/memory_ctx.py`

**Actions**:

```python
# memory_ctx.py (ligne ~642)
@staticmethod
def _result_matches_agent(
    result: Dict[str, Any],
    agent_id: str,
    strict_mode: bool = None  # ‚úÖ NOUVEAU param√®tre
) -> bool:
    """
    V√©rifie si r√©sultat correspond √† agent demand√©.

    Args:
        result: R√©sultat vectoriel
        agent_id: Agent ID normalis√© (lowercase)
        strict_mode:
            - True: Filtrage strict (ignore concepts sans agent_id)
            - False: Filtrage permissif (inclut concepts sans agent_id)
            - None: Auto (lire depuis env STRICT_AGENT_ISOLATION)
    """
    # Auto-d√©tection mode depuis env
    if strict_mode is None:
        import os
        strict_mode = os.getenv('STRICT_AGENT_ISOLATION', 'false').lower() == 'true'

    metadata = result.get("metadata", {})
    if not isinstance(metadata, dict):
        return not strict_mode  # Mode permissif = inclure, strict = exclure

    result_agent_id = metadata.get("agent_id")

    # Cas 1: Pas d'agent_id
    if not result_agent_id:
        # Mode strict: EXCLURE concepts sans agent_id
        # Mode permissif: INCLURE (comportement legacy)
        return not strict_mode

    # Cas 2: Agent ID correspond
    if isinstance(result_agent_id, str) and result_agent_id.lower() == agent_id:
        return True

    # Cas 3: Ne correspond pas
    return False
```

**Configuration**:
```bash
# .env - Activer mode strict progressivement
STRICT_AGENT_ISOLATION=false  # Phase transition (d√©faut)
# STRICT_AGENT_ISOLATION=true  # Phase finale (apr√®s backfill)
```

**Tests**:
```python
# Test isolation stricte
async def test_strict_agent_isolation():
    """V√©rifier filtrage strict agent_id"""
    # Setup: Concepts avec/sans agent_id
    collection.add(
        ids=['c1', 'c2', 'c3'],
        documents=['Doc Anima', 'Doc Neo', 'Doc Legacy'],
        metadatas=[
            {'agent_id': 'anima', 'user_id': 'u1'},
            {'agent_id': 'neo', 'user_id': 'u1'},
            {'user_id': 'u1'}  # Pas d'agent_id
        ]
    )

    # Mode strict = True
    results = vector_service.query(
        query_text="test",
        where_filter={'user_id': 'u1'}
    )
    filtered_strict = [
        r for r in results
        if _result_matches_agent(r, 'anima', strict_mode=True)
    ]
    assert len(filtered_strict) == 1  # Seulement c1
    assert filtered_strict[0]['id'] == 'c1'

    # Mode permissif = False
    filtered_permissive = [
        r for r in results
        if _result_matches_agent(r, 'anima', strict_mode=False)
    ]
    assert len(filtered_permissive) == 2  # c1 + c3 (legacy)
```

### √âtape 4.3: Plan Migration Strict Mode

**Roadmap activation**:

```
Phase 1 (Semaine 1-2): PR√âPARATION
‚îú‚îÄ Backfill agent_id concepts legacy
‚îú‚îÄ Tests isolation stricte
‚îî‚îÄ STRICT_AGENT_ISOLATION=false (permissif)

Phase 2 (Semaine 3): ACTIVATION BETA
‚îú‚îÄ STRICT_AGENT_ISOLATION=true pour 10% utilisateurs
‚îú‚îÄ Monitoring m√©triques isolation
‚îî‚îÄ Tests utilisateurs beta

Phase 3 (Semaine 4): ROLLOUT COMPLET
‚îú‚îÄ STRICT_AGENT_ISOLATION=true pour 100%
‚îú‚îÄ D√©pr√©cier mode permissif
‚îî‚îÄ Cleanup code legacy
```

**Monitoring**:
```python
# M√©triques isolation agent
from prometheus_client import Counter

AGENT_ISOLATION_VIOLATIONS = Counter(
    'agent_isolation_violations_total',
    'Concepts cross-agent d√©tect√©s',
    ['agent_requesting', 'agent_concept']
)

# Dans _result_matches_agent()
if strict_mode and result_agent_id and result_agent_id != agent_id:
    AGENT_ISOLATION_VIOLATIONS.labels(
        agent_requesting=agent_id,
        agent_concept=result_agent_id
    ).inc()
```

### ‚úÖ Crit√®res de Succ√®s Sprint 4
- [ ] Script backfill ex√©cut√©: 100% concepts ont agent_id
- [ ] Mode strict impl√©ment√© et test√©
- [ ] Feature flag `STRICT_AGENT_ISOLATION` op√©rationnel
- [ ] Monitoring violations isolation actif
- [ ] Tests unitaires passent (mode strict/permissif)
- [ ] Documentation mise √† jour (migration guide)

---

## SPRINT 5: Interface Utilisateur (BONUS)
**Dur√©e**: 5-7 jours
**Priorit√©**: üü¢ BASSE (BONUS)
**Objectif**: Dashboard utilisateur pour visualiser/g√©rer sa m√©moire

### Fonctionnalit√©s Propos√©es

#### 1. Dashboard M√©moire
```
/api/memory/user/dashboard

Response:
{
  "stats": {
    "conversations_total": 47,
    "conversations_active": 5,
    "conversations_archived": 42,
    "concepts_total": 156,
    "preferences_active": 12,
    "memory_size_mb": 4.2
  },
  "timeline": [
    {
      "period": "Cette semaine",
      "topics": ["Docker", "CI/CD", "Kubernetes"],
      "conversation_count": 3
    }
  ],
  "top_concepts": [
    {
      "concept": "Docker containerization",
      "mentions": 8,
      "last_mentioned": "2025-10-15"
    }
  ]
}
```

#### 2. Gestion Conversations
```
GET /api/threads?archived=true&search=docker
PATCH /api/threads/{id}/unarchive
DELETE /api/threads/{id}?permanent=true
```

#### 3. Gestion Concepts
```
GET /api/memory/concepts?sort=frequent&limit=50
PATCH /api/memory/concepts/{id}  # Modifier description/tags
POST /api/memory/concepts/merge  # Fusionner concepts similaires
DELETE /api/memory/concepts/{id}
```

#### 4. Export/Import M√©moire
```
GET /api/memory/export  # Export JSON complet
POST /api/memory/import  # Import depuis backup
```

**Note**: Routes d√©j√† impl√©ment√©es dans `src/backend/features/memory/router.py` (lignes 936-2122)

### Composants Frontend (React)

**Fichiers √† cr√©er**:
```
src/frontend/components/memory/
‚îú‚îÄ‚îÄ MemoryDashboard.tsx        # Dashboard principal
‚îú‚îÄ‚îÄ ConversationsList.tsx      # Liste conversations archiv√©es
‚îú‚îÄ‚îÄ ConceptsGraph.tsx          # Graph visualisation concepts
‚îú‚îÄ‚îÄ PreferencesManager.tsx     # Gestion pr√©f√©rences
‚îî‚îÄ‚îÄ MemoryExport.tsx           # Export/Import
```

### ‚úÖ Crit√®res de Succ√®s Sprint 5
- [ ] Dashboard m√©moire accessible et responsive
- [ ] Recherche fulltext conversations fonctionnelle
- [ ] Visualisation graph concepts op√©rationnelle
- [ ] Export/import m√©moire test√©
- [ ] Documentation utilisateur r√©dig√©e

---

## üß™ VALIDATION & TESTS

### Tests Unitaires Requis

```python
# tests/features/memory/test_unified_retriever.py
async def test_unified_retriever_stm():
    """Test r√©cup√©ration STM"""

async def test_unified_retriever_ltm():
    """Test r√©cup√©ration LTM (prefs + concepts)"""

async def test_unified_retriever_archives():
    """Test r√©cup√©ration conversations archiv√©es"""

async def test_unified_retriever_full_context():
    """Test contexte complet (STM + LTM + Archives)"""

# tests/features/memory/test_consolidation.py
async def test_auto_consolidation_on_archive():
    """Test consolidation auto lors archivage"""

async def test_batch_consolidation_archives():
    """Test script batch consolidation"""

# tests/core/database/test_conversation_id.py
async def test_conversation_id_creation():
    """Test cr√©ation conversation avec conversation_id"""

async def test_get_threads_by_conversation():
    """Test r√©cup√©ration par conversation_id"""

# tests/features/memory/test_agent_isolation.py
async def test_strict_agent_filtering():
    """Test filtrage strict agent_id"""

async def test_agent_id_backfill():
    """Test backfill agent_id legacy"""
```

### Tests d'Int√©gration

```python
# tests/integration/test_memory_e2e.py
async def test_full_memory_lifecycle():
    """
    Test cycle complet:
    1. Cr√©er conversation
    2. Ajouter messages
    3. Archiver (d√©clenche consolidation)
    4. Nouvelle session
    5. Requ√™te ‚Üí V√©rifier rappel archives
    """

async def test_cross_session_memory_recall():
    """
    Test rappel m√©moire entre sessions:
    1. Session A: Parler de Docker
    2. Archiver conversation
    3. Session B (nouveau WS): Demander "Docker?"
    4. V√©rifier contexte inclut conversation A
    """

async def test_agent_memory_isolation():
    """
    Test isolation entre agents:
    1. Anima: Cr√©er concept "Docker"
    2. Neo: Requ√™te "Docker"
    3. V√©rifier Neo ne voit pas concept Anima (mode strict)
    """
```

### M√©triques de Succ√®s

```
Performance:
‚îú‚îÄ Latence rappel contexte: < 200ms (P95)
‚îú‚îÄ Latence consolidation: < 2s par thread
‚îî‚îÄ Latence batch consolidation: < 5s pour 100 threads

Fiabilit√©:
‚îú‚îÄ 100% threads archiv√©s consolid√©s
‚îú‚îÄ 0 violation isolation agent (mode strict)
‚îî‚îÄ 99.9% uptime endpoints m√©moire

Qualit√©:
‚îú‚îÄ Coverage tests: > 85%
‚îú‚îÄ 0 r√©gression fonctionnelle
‚îî‚îÄ Documentation compl√®te (100% endpoints)
```

---

## üìö R√âF√âRENCES FICHIERS

### Fichiers Critiques √† Modifier

```
src/backend/core/
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ schema.py (L85-149)          # Ajout conversation_id, consolidated_at
‚îÇ   ‚îî‚îÄ‚îÄ queries.py (L798-925)         # Modifications create/update/get_threads
‚îú‚îÄ‚îÄ session_manager.py (L51-943)      # Int√©gration conversation_id
‚îî‚îÄ‚îÄ memory/
    ‚îî‚îÄ‚îÄ memory_sync.py (L289-315)     # Isolation agent

src/backend/features/
‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îî‚îÄ‚îÄ memory_ctx.py (L40-675)       # Int√©gration UnifiedRetriever
‚îî‚îÄ‚îÄ memory/
    ‚îú‚îÄ‚îÄ router.py (L2026-2122)        # Endpoints consolidation
    ‚îú‚îÄ‚îÄ gardener.py (L1-300+)         # Consolidation auto
    ‚îú‚îÄ‚îÄ analyzer.py (L1-300+)         # Extraction concepts
    ‚îî‚îÄ‚îÄ unified_retriever.py (NEW)    # ‚úÖ √Ä cr√©er

migrations/
‚îú‚îÄ‚îÄ 20251018_add_conversation_id.sql  # ‚úÖ √Ä cr√©er
‚îî‚îÄ‚îÄ 20251018_add_consolidated_at.sql  # ‚úÖ √Ä cr√©er

src/backend/cli/
‚îú‚îÄ‚îÄ consolidate_all_archives.py (NEW) # ‚úÖ √Ä cr√©er
‚îî‚îÄ‚îÄ backfill_agent_ids.py (NEW)       # ‚úÖ √Ä cr√©er
```

### Fichiers de Configuration

```
.env
‚îú‚îÄ‚îÄ ENABLE_UNIFIED_MEMORY_RETRIEVER=true
‚îú‚îÄ‚îÄ UNIFIED_RETRIEVER_INCLUDE_ARCHIVES=true
‚îú‚îÄ‚îÄ UNIFIED_RETRIEVER_TOP_K_ARCHIVES=3
‚îî‚îÄ‚îÄ STRICT_AGENT_ISOLATION=false  # ‚Üí true apr√®s backfill
```

---

## üöÄ QUICKSTART - PROCHAINE INSTANCE

### Commandes Rapides

```bash
# 1. V√©rifier √©tat actuel
python -c "
from src.backend.core.database.manager import DatabaseManager
db = DatabaseManager('emergence.db')
await db.connect()
rows = await db.fetch_all('PRAGMA table_info(threads)')
print('Colonnes threads:', [r['name'] for r in rows])
"

# 2. Appliquer migrations Sprint 1
python src/backend/core/database/manager.py run_migrations migrations/

# 3. Lancer consolidation batch (Sprint 2)
python src/backend/cli/consolidate_all_archives.py --user-id <USER_ID>

# 4. Backfill agent_id (Sprint 4)
python src/backend/cli/backfill_agent_ids.py --dry-run  # Test
python src/backend/cli/backfill_agent_ids.py             # R√©el

# 5. Tests validation
pytest tests/features/memory/test_unified_retriever.py -v
pytest tests/integration/test_memory_e2e.py -v
```

### Ordre d'Ex√©cution Recommand√©

```
1. Sprint 1 (CRITIQUE) - 2-3 jours
   ‚îî‚îÄ> Clarification Session vs Conversation
       ‚Üí Permet acc√®s facile conversations utilisateur

2. Sprint 2 (HAUTE) - 3-4 jours
   ‚îî‚îÄ> Consolidation Auto Archives
       ‚Üí Garantit aucun souvenir perdu

3. Sprint 3 (HAUTE) - 4-5 jours
   ‚îî‚îÄ> Rappel Proactif Unifi√©
       ‚Üí Agent "se souvient" spontan√©ment

4. Sprint 4 (MOYENNE) - 2-3 jours
   ‚îî‚îÄ> Isolation Agent Stricte
       ‚Üí Pr√©vient confusion entre agents

5. Sprint 5 (BONUS) - 5-7 jours
   ‚îî‚îÄ> Interface Utilisateur
       ‚Üí Contr√¥le utilisateur sur m√©moire
```

---

## üìû SUPPORT & QUESTIONS

### D√©cisions Architecture √† Valider

- [ ] Approbation ajout `conversation_id` (breaking change schema)
- [ ] Validation strat√©gie consolidation automatique
- [ ] Choix mode isolation agent (strict vs permissif)
- [ ] Priorisation Sprint 5 (UI) vs autres features

### Points d'Attention

‚ö†Ô∏è **Migration DB**: Backups avant modifications schema
‚ö†Ô∏è **Performance**: Consolidation batch peut √™tre lente (>1000 threads)
‚ö†Ô∏è **R√©trocompat**: Garder `session_id` fonctionnel pendant transition
‚ö†Ô∏è **Tests**: Valider sur environnement staging avant prod

---

**Fin de Roadmap** - Pr√™t pour ex√©cution Sprint 1 üöÄ
