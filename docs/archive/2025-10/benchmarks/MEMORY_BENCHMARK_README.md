# üß† Benchmark de R√©tention M√©moire - √âMERGENCE

## üìã Vue d'ensemble

Ce module permet de **mesurer la capacit√© de r√©tention de contexte** des trois agents d'√âMERGENCE (Neo, Anima, Nexus) sur trois √©ch√©ances temporelles :

- **T+1h** : R√©tention √† court terme
- **T+24h** : R√©tention √† moyen terme
- **T+7j** : R√©tention √† long terme

Le syst√®me injecte des **faits de r√©f√©rence** dans la m√©moire de chaque agent, puis teste leur rappel √† intervalles r√©guliers. Les r√©sultats sont agr√©g√©s dans des graphiques comparatifs.

---

## üìÅ Fichiers du module

```
emergenceV8/
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ ground_truth.yml          # Faits de r√©f√©rence √† m√©moriser
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ memory_probe.py           # Script de test de r√©tention
‚îÇ   ‚îî‚îÄ‚îÄ plot_retention.py         # G√©n√©ration des graphiques
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances (PyYAML, matplotlib, pandas)
‚îú‚îÄ‚îÄ MEMORY_BENCHMARK_README.md    # Ce fichier
‚îî‚îÄ‚îÄ memory_results_*.csv          # R√©sultats (g√©n√©r√©s)
```

---

## üöÄ Installation

### 1. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

Les d√©pendances sp√©cifiques au benchmark :
- **PyYAML** : Lecture du fichier `ground_truth.yml`
- **matplotlib** : G√©n√©ration des graphiques
- **pandas** : Agr√©gation des r√©sultats CSV
- **httpx** : Appels HTTP au backend (d√©j√† install√©)

### 2. Configurer le backend

Le benchmark n√©cessite un backend √âMERGENCE accessible (local ou Cloud Run).

**Backend local** :
```bash
pwsh -File scripts/run-backend.ps1
```

**Backend Cloud Run** :
```bash
export BACKEND_URL="https://emergence-app-486095406755.europe-west1.run.app"
export JWT_TOKEN="votre_token_jwt"
```

---

## üß™ Utilisation

### Mode production (d√©lais r√©els : 1h, 24h, 7j)

**Lancer le test pour un agent** :

```bash
# Neo
AGENT_NAME=Neo python scripts/memory_probe.py

# Anima
AGENT_NAME=Anima python scripts/memory_probe.py

# Nexus
AGENT_NAME=Nexus python scripts/memory_probe.py
```

**‚ö†Ô∏è Attention** : Le test complet dure **7 jours** ! Il est recommand√© d'utiliser le mode debug pour des tests rapides.

### Mode debug (d√©lais courts : 1min, 2min, 3min)

Pour valider rapidement que le syst√®me fonctionne :

```bash
DEBUG_MODE=true AGENT_NAME=Neo python scripts/memory_probe.py
```

Le test complet dure seulement **3 minutes** au lieu de 7 jours.

### Lancer les tests pour tous les agents en parall√®le

**Windows PowerShell** :
```powershell
# Mode debug (3 min)
$env:DEBUG_MODE="true"

Start-Job -ScriptBlock { $env:AGENT_NAME="Neo"; python scripts/memory_probe.py }
Start-Job -ScriptBlock { $env:AGENT_NAME="Anima"; python scripts/memory_probe.py }
Start-Job -ScriptBlock { $env:AGENT_NAME="Nexus"; python scripts/memory_probe.py }

# Surveiller les jobs
Get-Job | Wait-Job
Get-Job | Receive-Job
```

**Linux/macOS** :
```bash
DEBUG_MODE=true AGENT_NAME=Neo python scripts/memory_probe.py &
DEBUG_MODE=true AGENT_NAME=Anima python scripts/memory_probe.py &
DEBUG_MODE=true AGENT_NAME=Nexus python scripts/memory_probe.py &
wait
```

### G√©n√©rer le graphique comparatif

Une fois les tests termin√©s pour au moins un agent :

```bash
# Graphique simple (moyenne par agent)
python scripts/plot_retention.py

# Graphique d√©taill√© (par fait F1/F2/F3)
DETAILED=true python scripts/plot_retention.py

# Mode debug (pour ticks courts)
DEBUG_MODE=true python scripts/plot_retention.py
```

**Sortie** :
- `retention_curve_all.png` : Graphique comparatif
- `retention_curve_detailed.png` : Graphique d√©taill√© (si `DETAILED=true`)

---

## üìä Format des r√©sultats

### Fichiers CSV g√©n√©r√©s

Chaque ex√©cution de `memory_probe.py` g√©n√®re un fichier CSV :

```
memory_results_neo.csv
memory_results_anima.csv
memory_results_nexus.csv
```

**Format** :
```csv
timestamp_utc,agent,session,tick,fact_id,score,truth,prediction
2025-10-21T12:00:00.000000,Neo,session-neo-20251021120000,T+1h,F1,1.00,iris-47,iris-47
2025-10-21T12:00:01.000000,Neo,session-neo-20251021120000,T+1h,F2,0.50,Orph√©e SA,Le client est Orph√©e SA
2025-10-21T12:00:02.000000,Neo,session-neo-20251021120000,T+1h,F3,0.00,7788,Je ne me souviens pas
```

**Calcul du score** :
- `1.0` : Correspondance exacte (apr√®s normalisation)
- `0.5` : V√©rit√© contenue dans la pr√©diction
- `0.0` : Aucune correspondance

### Graphique de r√©tention

**Exemple** :

```
Score (0-1)
    ^
1.0 |     ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚óè    Neo
    |    /         \
0.5 |   ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óè    Anima
    |  /           \
0.0 | ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ  Nexus
    |________________________> Temps
      T+1h   T+24h   T+7j
```

---

## üéØ Personnalisation

### Ajouter des faits de r√©f√©rence

√âditer [`prompts/ground_truth.yml`](prompts/ground_truth.yml) :

```yaml
facts:
  - id: F4
    prompt: "La version actuelle du syst√®me est V8.2.1"
    answer: "V8.2.1"

  - id: F5
    prompt: "Le nom du projet est √âMERGENCE"
    answer: "√âMERGENCE"
```

### Modifier les √©ch√©ances

√âditer [`scripts/memory_probe.py`](scripts/memory_probe.py) :

```python
# Ligne 37-41
DELTAS = [
    ("T+15min", 900),      # 15 minutes
    ("T+2h", 7200),        # 2 heures
    ("T+12h", 43200)       # 12 heures
]
```

### Adapter le scoring

Modifier la fonction `score()` dans [`scripts/memory_probe.py`](scripts/memory_probe.py:67) pour impl√©menter un scoring personnalis√© (ex: Levenshtein, embedding similarity, etc.)

---

## üîó Int√©gration future (Phase P3)

### Stockage automatique dans ChromaDB

```python
# Dans memory_probe.py
from chromadb import Client

chroma_client = Client()
collection = chroma_client.get_or_create_collection("emergence_benchmarks")

# Apr√®s chaque test
collection.add(
    documents=[f"{fact_id}:{prediction}"],
    metadatas=[{
        "agent": AGENT_NAME,
        "tick": tick,
        "score": score_val,
        "session_id": SESSION_ID
    }],
    ids=[f"{SESSION_ID}_{tick}_{fact_id}"]
)
```

### Corr√©lation avec m√©triques Prometheus

Croiser les r√©sultats du benchmark avec les m√©triques de production :

- `memory_analysis_duration_seconds` : Temps d'analyse m√©moire
- `memory_cache_operations_total` : Op√©rations de cache
- `memory_proactive_hints_generated_total` : Hints g√©n√©r√©s

```python
import requests

# R√©cup√©rer m√©triques Prometheus
metrics_url = f"{BACKEND_URL}/metrics"
response = requests.get(metrics_url)

# Analyser corr√©lations
# Ex: Score de r√©tention VS latence d'analyse m√©moire
```

### Automatisation via API `/api/benchmarks/runs`

Cr√©er un endpoint d√©di√© pour lancer les benchmarks :

```python
# src/backend/features/benchmarks/router.py
@router.post("/runs")
async def create_benchmark_run(
    agent: str,
    mode: str = "debug"  # debug | production
):
    # Lance memory_probe.py en arri√®re-plan
    # Enregistre run_id dans BDD
    # Retourne status + ETA
    pass
```

---

## üìö R√©f√©rences

**Architecture √âMERGENCE** :
- [`docs/architecture/00-Overview.md`](docs/architecture/00-Overview.md)
- [`docs/architecture/10-Components.md`](docs/architecture/10-Components.md)

**Syst√®me m√©moire** :
- [`src/backend/features/memory/`](src/backend/features/memory/)
- Prompts agents : [`prompts/neo_system_v3.md`](prompts/neo_system_v3.md), [`prompts/anima_system_v2.md`](prompts/anima_system_v2.md), [`prompts/nexus_system_v2.md`](prompts/nexus_system_v2.md)

**M√©triques Prometheus** :
- Endpoint : `/metrics`
- Dashboard Grafana (si configur√©) : voir `scripts/setup_gcp_memory_alerts.py`

---

## ‚úÖ Validation du module

Pour v√©rifier que tout est correctement configur√© :

```bash
# 1. V√©rifier la syntaxe des scripts
python -m py_compile scripts/memory_probe.py
python -m py_compile scripts/plot_retention.py

# 2. V√©rifier les d√©pendances
python -c "import yaml, matplotlib, pandas, httpx; print('Toutes les d√©pendances sont install√©es ‚úÖ')"

# 3. Lancer un test rapide (3 min)
DEBUG_MODE=true AGENT_NAME=Neo python scripts/memory_probe.py

# 4. G√©n√©rer le graphique
DEBUG_MODE=true python scripts/plot_retention.py

# 5. V√©rifier les fichiers g√©n√©r√©s
ls -lh memory_results_*.csv retention_curve_*.png
```

---

## üêõ Troubleshooting

### Erreur "Backend not reachable"

**Sympt√¥me** :
```
‚ùå Erreur de connexion: All connection attempts failed
```

**Solution** :
1. V√©rifier que le backend est bien d√©marr√© :
   ```bash
   pwsh -File scripts/run-backend.ps1
   ```

2. Tester manuellement l'endpoint :
   ```bash
   curl http://localhost:8000/api/health
   ```

3. V√©rifier les variables d'environnement :
   ```bash
   echo $BACKEND_URL
   echo $JWT_TOKEN
   ```

### Erreur "ground_truth.yml not found"

**Sympt√¥me** :
```
‚ùå Fichier ground truth introuvable: prompts/ground_truth.yml
```

**Solution** :
Le script doit √™tre ex√©cut√© depuis la **racine du projet** :
```bash
cd c:\dev\emergenceV8
python scripts/memory_probe.py
```

### Score toujours √† 0.0

**Sympt√¥me** :
```
‚ùå F1: score=0.00 | attendu='iris-47' | obtenu='Je ne sais pas'
```

**Causes possibles** :
1. **Agent ne m√©morise pas** : V√©rifier que le syst√®me m√©moire est activ√© dans le backend
2. **Session diff√©rente** : Le `SESSION_ID` change entre injection et rappel
3. **Prompt incomplet** : L'agent ne re√ßoit pas le contexte m√©moire

**Debug** :
1. Activer les logs backend :
   ```bash
   LOG_LEVEL=DEBUG pwsh -File scripts/run-backend.ps1
   ```

2. V√©rifier dans les logs :
   - Injection du contexte : `POST /api/chat` avec le message d'injection
   - Requ√™tes de rappel : `POST /api/chat` avec les questions
   - Analyse m√©moire : `MemoryService.analyze_and_retrieve()`

---

## ü§ù Contribution

Pour am√©liorer le benchmark :

1. **Ajouter de nouveaux faits** dans `prompts/ground_truth.yml`
2. **Am√©liorer le scoring** dans `memory_probe.py:score()`
3. **Cr√©er de nouveaux graphiques** dans `plot_retention.py`
4. **Documenter les r√©sultats** dans `docs/passation.md`

**Consignes** :
- Suivre les conventions de [`CLAUDE.md`](CLAUDE.md)
- Tester en mode debug avant production
- Mettre √† jour [`AGENT_SYNC.md`](AGENT_SYNC.md) apr√®s modifications

---

## üìù Changelog

**v1.0.0 (2025-10-21)** :
- ‚úÖ Cr√©ation du module de benchmark
- ‚úÖ Scripts `memory_probe.py` et `plot_retention.py`
- ‚úÖ Mode debug avec d√©lais raccourcis
- ‚úÖ Graphiques comparatifs multi-agents
- ‚úÖ Documentation compl√®te

---

**Auteur** : Claude Code
**Derni√®re mise √† jour** : 2025-10-21
**Licence** : Voir LICENSE du projet √âMERGENCE
