# üß† PROMPT P1 : ENRICHISSEMENT M√âMOIRE & D√âPORTATION ASYNCHRONE

**Date** : 2025-10-09
**Phase** : P1 - Hors boucle WS & enrichissement conceptuel
**Pr√©requis** : P0 valid√©e ‚úÖ, Phase 2 valid√©e ‚úÖ, Phase 3 d√©ploy√©e ‚úÖ
**Contexte** : [docs/memory-roadmap.md](docs/memory-roadmap.md)

---

## üìã CONTEXTE

Bonjour Claude,

Tu travailles sur √âMERGENCE V8, un syst√®me de conversation avec m√©moire s√©mantique √† long terme.

### √âtat actuel (P0 + Phases 2-3 valid√©es)

**‚úÖ Ce qui fonctionne** :
- Persistance messages & restauration session (P0)
- Analyse m√©moire avec `neo_analysis` (gpt-4o-mini) : 4-6s
- Cache BDD : 0.177s (26x plus rapide)
- M√©triques Prometheus : 13 m√©triques expos√©es en production
- `MemoryGardener` extrait concepts "mot-code" et les vectorise
- M√©canisme d'oubli par vitalit√© (decay + purge)

**‚ö†Ô∏è Limitations identifi√©es** :
1. Analyse/vectorisation dans la boucle WS ‚Üí risque blocage event loop
2. Extraction limit√©e aux "mot-code" ‚Üí manque pr√©f√©rences, intentions, contraintes
3. Pas de signalement proactif des concepts r√©currents
4. M√©triques cache non instrument√©es (WIP session parall√®le)

---

## üéØ MISSION P1

Impl√©menter **Phase P1 - Enrichissement conceptuel & d√©portation asynchrone** en 3 volets :

### üî¥ P1.1 - D√©portation hors boucle WS (PRIORIT√â HAUTE)
√âviter blocages event loop en d√©portant t√¢ches lourdes.

### üü° P1.2 - Extension extraction de faits (PRIORIT√â MOYENNE)
Capturer pr√©f√©rences, intentions, contraintes utilisateur.

### üü¢ P1.3 - Instrumentation m√©triques (PRIORIT√â BASSE)
Compl√©ter m√©triques cache + nouveau pipeline pr√©f√©rences.

---

## üì¶ P1.1 - D√âPORTATION ASYNCHRONE (3-4h)

### Objectif
D√©porter `MemoryAnalyzer` et `MemoryGardener` dans une file de t√¢ches pour √©viter blocage event loop WebSocket.

### Approche recommand√©e

**Option A : asyncio.create_task() + Queue (Simple)**
- File `asyncio.Queue` pour t√¢ches d'analyse
- Worker background consomme la queue
- Pas de d√©pendance externe

**Option B : Celery + Redis (Production)**
- File distribu√©e Redis
- Workers multi-processus
- Retry automatique
- Monitoring int√©gr√©

**Recommandation** : Commencer par **Option A** (asyncio), migrer vers **Option B** si scaling requis.

### Fichiers √† modifier

#### 1. Cr√©er `src/backend/features/memory/task_queue.py`

```python
"""
File de t√¢ches asynchrone pour MemoryAnalyzer et MemoryGardener.
√âvite blocage event loop WebSocket.
"""

import asyncio
import logging
from typing import Callable, Any
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class MemoryTask:
    """T√¢che d'analyse/jardinage m√©moire"""
    task_type: str  # "analyze" | "garden"
    payload: dict
    callback: Callable | None = None
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()


class MemoryTaskQueue:
    """
    File de t√¢ches asynchrone pour op√©rations m√©moire lourdes.

    Usage:
        queue = MemoryTaskQueue()
        await queue.start()
        await queue.enqueue("analyze", {"session_id": "..."})
    """

    def __init__(self, max_workers: int = 2):
        self.queue: asyncio.Queue = asyncio.Queue()
        self.max_workers = max_workers
        self.workers: list[asyncio.Task] = []
        self.running = False

    async def start(self):
        """D√©marre les workers de traitement"""
        if self.running:
            return

        self.running = True
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._worker(i))
            self.workers.append(worker)
        logger.info(f"MemoryTaskQueue started with {self.max_workers} workers")

    async def stop(self):
        """Arr√™te proprement les workers"""
        self.running = False

        # Envoyer signal arr√™t
        for _ in range(self.max_workers):
            await self.queue.put(None)

        # Attendre fin workers
        await asyncio.gather(*self.workers, return_exceptions=True)
        logger.info("MemoryTaskQueue stopped")

    async def enqueue(self, task_type: str, payload: dict, callback: Callable = None):
        """Ajoute une t√¢che √† la file"""
        task = MemoryTask(task_type=task_type, payload=payload, callback=callback)
        await self.queue.put(task)
        logger.debug(f"Task enqueued: {task_type} - {payload.get('session_id', 'N/A')}")

    async def _worker(self, worker_id: int):
        """Worker qui consomme la file"""
        logger.info(f"Worker {worker_id} started")

        while self.running:
            try:
                # Attendre t√¢che (timeout pour v√©rifier self.running)
                task = await asyncio.wait_for(self.queue.get(), timeout=1.0)

                if task is None:  # Signal arr√™t
                    break

                # Traiter t√¢che
                await self._process_task(task, worker_id)

            except asyncio.TimeoutError:
                continue  # Pas de t√¢che, continuer
            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}", exc_info=True)

        logger.info(f"Worker {worker_id} stopped")

    async def _process_task(self, task: MemoryTask, worker_id: int):
        """Traite une t√¢che m√©moire"""
        start = datetime.utcnow()

        try:
            if task.task_type == "analyze":
                result = await self._run_analysis(task.payload)
            elif task.task_type == "garden":
                result = await self._run_gardening(task.payload)
            else:
                logger.warning(f"Unknown task type: {task.task_type}")
                return

            duration = (datetime.utcnow() - start).total_seconds()
            logger.info(f"Worker {worker_id} completed {task.task_type} in {duration:.2f}s")

            # Callback si fourni
            if task.callback:
                await task.callback(result)

        except Exception as e:
            logger.error(f"Task {task.task_type} failed: {e}", exc_info=True)

    async def _run_analysis(self, payload: dict):
        """Ex√©cute MemoryAnalyzer.analyze_session"""
        from backend.features.memory.analyzer import MemoryAnalyzer
        from backend.containers import Container

        analyzer = Container.memory_analyzer()
        session_id = payload["session_id"]
        force = payload.get("force", False)

        result = await analyzer.analyze_session(session_id, force=force)
        return result

    async def _run_gardening(self, payload: dict):
        """Ex√©cute MemoryGardener.garden_thread"""
        from backend.features.memory.gardener import MemoryGardener
        from backend.containers import Container

        gardener = Container.memory_gardener()
        thread_id = payload["thread_id"]
        user_sub = payload.get("user_sub")

        await gardener.garden_thread(thread_id, user_sub=user_sub)
        return {"status": "gardened", "thread_id": thread_id}


# Singleton global
_task_queue: MemoryTaskQueue | None = None

def get_memory_queue() -> MemoryTaskQueue:
    """R√©cup√®re l'instance globale de la file"""
    global _task_queue
    if _task_queue is None:
        _task_queue = MemoryTaskQueue(max_workers=2)
    return _task_queue
```

#### 2. Modifier `src/backend/features/memory/analyzer.py`

**Ajouter m√©thode asynchrone** :

```python
# Vers ligne 200, apr√®s analyze_session()

async def analyze_session_async(
    self,
    session_id: str,
    force: bool = False,
    callback: Callable = None
) -> None:
    """
    Version asynchrone non-bloquante de analyze_session.
    Enqueue t√¢che dans MemoryTaskQueue.

    Args:
        session_id: ID session √† analyser
        force: Forcer nouvelle analyse
        callback: Fonction appel√©e avec r√©sultat
    """
    from backend.features.memory.task_queue import get_memory_queue

    queue = get_memory_queue()
    await queue.enqueue(
        task_type="analyze",
        payload={"session_id": session_id, "force": force},
        callback=callback
    )

    logger.info(f"Analyse session {session_id} enqueued (async)")
```

#### 3. Modifier `src/backend/main.py` (Lifecycle)

**D√©marrer/arr√™ter queue** :

```python
# Apr√®s ligne 50 (startup)

@app.on_event("startup")
async def startup():
    """D√©marrage application"""
    logger.info("Starting Emergence Backend...")

    # D√©marrer MemoryTaskQueue
    from backend.features.memory.task_queue import get_memory_queue
    queue = get_memory_queue()
    await queue.start()
    logger.info("MemoryTaskQueue started")

@app.on_event("shutdown")
async def shutdown():
    """Arr√™t application"""
    logger.info("Shutting down Emergence Backend...")

    # Arr√™ter MemoryTaskQueue
    from backend.features.memory.task_queue import get_memory_queue
    queue = get_memory_queue()
    await queue.stop()
    logger.info("MemoryTaskQueue stopped")
```

#### 4. Modifier appels bloquants (WebSocket, routes)

**Exemple dans `src/backend/features/chat/service.py`** :

```python
# Remplacer (ligne ~850) :
# result = await memory_analyzer.analyze_session(session_id)

# Par :
await memory_analyzer.analyze_session_async(
    session_id=session_id,
    callback=lambda res: logger.info(f"Analysis completed: {res['status']}")
)
```

### Tests P1.1

```python
# tests/memory/test_task_queue.py

import pytest
import asyncio
from backend.features.memory.task_queue import MemoryTaskQueue, MemoryTask

@pytest.mark.asyncio
async def test_queue_starts_workers():
    queue = MemoryTaskQueue(max_workers=2)
    await queue.start()

    assert len(queue.workers) == 2
    assert queue.running is True

    await queue.stop()

@pytest.mark.asyncio
async def test_enqueue_analyze_task():
    queue = MemoryTaskQueue()
    await queue.start()

    result = []

    async def callback(res):
        result.append(res)

    await queue.enqueue(
        "analyze",
        {"session_id": "test-123", "force": True},
        callback=callback
    )

    # Attendre traitement (max 5s)
    for _ in range(50):
        if result:
            break
        await asyncio.sleep(0.1)

    await queue.stop()

    assert len(result) == 1
    assert result[0]["session_id"] == "test-123"
```

### Crit√®res de succ√®s P1.1

- ‚úÖ File `MemoryTaskQueue` cr√©√©e et test√©e
- ‚úÖ Workers d√©marrent/arr√™tent proprement
- ‚úÖ `analyze_session_async()` enqueue sans bloquer
- ‚úÖ Tests unitaires passent
- ‚úÖ Aucun blocage event loop WebSocket (mesure latence WS <100ms)

---

## üì¶ P1.2 - EXTENSION EXTRACTION DE FAITS (6-8h)

### Objectif
Capturer pr√©f√©rences, intentions, contraintes utilisateur au-del√† des "mot-code".

### Pipeline hybride

**√âtape 1 : Filtrage lexical**
D√©tecter phrases avec verbes cibles avant appel LLM.

**√âtape 2 : Classification LLM**
Cat√©goriser en `preference`, `intent`, `constraint`, `neutral`.

**√âtape 3 : Normalisation**
Extraire `topic`, `action`, `timeframe`, `entities`.

### Fichiers √† cr√©er

#### 1. `src/backend/features/memory/preference_extractor.py`

```python
"""
Extracteur de pr√©f√©rences, intentions et contraintes utilisateur.
Pipeline hybride : filtrage lexical + classification LLM + normalisation.
"""

import re
import logging
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)

# Verbes cibles pour filtrage lexical
PREFERENCE_VERBS = {
    "fr": ["pr√©f√©r", "aim", "d√©teste", "adore", "appr√©ci", "favoris"],
    "en": ["prefer", "like", "love", "hate", "enjoy", "favorite"]
}

INTENT_VERBS = {
    "fr": ["vouloir", "veux", "souhaite", "planifie", "pr√©voi", "d√©cide", "compte"],
    "en": ["want", "wish", "plan", "intend", "decide", "will", "going to"]
}

CONSTRAINT_VERBS = {
    "fr": ["√©vite", "refuse", "jamais", "interdit", "ne pas", "impossible"],
    "en": ["avoid", "refuse", "never", "forbidden", "don't", "cannot"]
}


@dataclass
class PreferenceRecord:
    """Enregistrement pr√©f√©rence/intention/contrainte"""
    id: str  # Hash MD5 court
    type: str  # "preference" | "intent" | "constraint" | "neutral"
    topic: str  # Sujet normalis√©
    action: str  # Verbe infinitif
    text: str  # Texte original
    timeframe: str  # ISO 8601 ou "ongoing"
    sentiment: str  # "positive" | "negative" | "neutral"
    confidence: float  # 0.0-1.0
    entities: List[str]  # Personnes, outils, lieux
    source_message_id: str
    thread_id: str
    captured_at: str  # ISO timestamp

    @staticmethod
    def generate_id(user_sub: str, topic: str, type_: str) -> str:
        """G√©n√®re ID unique bas√© sur (user_sub, topic, type)"""
        key = f"{user_sub}:{topic}:{type_}"
        return hashlib.md5(key.encode()).hexdigest()[:12]


class PreferenceExtractor:
    """
    Extracteur de pr√©f√©rences/intentions/contraintes.

    Usage:
        extractor = PreferenceExtractor(llm_client)
        records = await extractor.extract(messages, user_sub="user123")
    """

    def __init__(self, llm_client):
        self.llm = llm_client
        self.stats = {"extracted": 0, "filtered": 0, "classified": 0}

    async def extract(
        self,
        messages: List[Dict],
        user_sub: str,
        thread_id: str
    ) -> List[PreferenceRecord]:
        """
        Extrait pr√©f√©rences/intentions depuis messages.

        Args:
            messages: Liste messages (role, content, id)
            user_sub: ID utilisateur
            thread_id: ID thread

        Returns:
            Liste PreferenceRecord avec confidence > 0.6
        """
        records = []

        # Filtrer messages utilisateur
        user_messages = [m for m in messages if m.get("role") == "user"]

        for msg in user_messages:
            content = msg.get("content", "")
            msg_id = msg.get("id", "unknown")

            # √âtape 1 : Filtrage lexical
            if not self._contains_target_verbs(content):
                self.stats["filtered"] += 1
                continue

            # √âtape 2 : Classification LLM
            classification = await self._classify_llm(content)
            self.stats["classified"] += 1

            if classification["type"] == "neutral":
                continue

            if classification["confidence"] < 0.6:
                logger.debug(f"Low confidence {classification['confidence']:.2f}: {content[:50]}")
                continue

            # √âtape 3 : Normalisation
            record = PreferenceRecord(
                id=PreferenceRecord.generate_id(user_sub, classification["topic"], classification["type"]),
                type=classification["type"],
                topic=classification["topic"],
                action=classification["action"],
                text=content,
                timeframe=classification.get("timeframe", "ongoing"),
                sentiment=classification.get("sentiment", "neutral"),
                confidence=classification["confidence"],
                entities=classification.get("entities", []),
                source_message_id=msg_id,
                thread_id=thread_id,
                captured_at=datetime.utcnow().isoformat()
            )

            records.append(record)
            self.stats["extracted"] += 1

        logger.info(f"Extracted {len(records)} preferences/intents (filtered: {self.stats['filtered']}, classified: {self.stats['classified']})")
        return records

    def _contains_target_verbs(self, text: str) -> bool:
        """Filtre lexical : contient verbes cibles ?"""
        text_lower = text.lower()

        all_verbs = (
            PREFERENCE_VERBS["fr"] + PREFERENCE_VERBS["en"] +
            INTENT_VERBS["fr"] + INTENT_VERBS["en"] +
            CONSTRAINT_VERBS["fr"] + CONSTRAINT_VERBS["en"]
        )

        return any(verb in text_lower for verb in all_verbs)

    async def _classify_llm(self, text: str) -> Dict[str, Any]:
        """
        Classification LLM (gpt-4o-mini ou claude-3-haiku).

        Returns:
            {
                "type": "preference" | "intent" | "constraint" | "neutral",
                "topic": "programmation",
                "action": "apprendre",
                "timeframe": "ongoing",
                "sentiment": "positive",
                "confidence": 0.85,
                "entities": ["Python", "FastAPI"]
            }
        """
        prompt = f"""Tu es un extracteur de pr√©f√©rences utilisateur.

Analyse ce message et extrait :
- **type** : "preference" (go√ªt, habitude), "intent" (action future), "constraint" (limite), ou "neutral"
- **topic** : Sujet principal (1-3 mots)
- **action** : Verbe principal (infinitif)
- **timeframe** : Date ISO 8601 si mentionn√©e, sinon "ongoing"
- **sentiment** : "positive", "negative", "neutral"
- **confidence** : Score 0.0-1.0 (certitude extraction)
- **entities** : Noms propres, outils, lieux (liste)

Message : "{text}"

R√©ponds UNIQUEMENT en JSON valide :
{{"type": "...", "topic": "...", "action": "...", "timeframe": "...", "sentiment": "...", "confidence": 0.0, "entities": []}}"""

        try:
            # Appel LLM (gpt-4o-mini par d√©faut)
            response = await self.llm.generate(
                prompt=prompt,
                model="gpt-4o-mini",
                temperature=0.1,
                response_format="json"
            )

            import json
            result = json.loads(response)

            # Normalisation cl√©s (pr√©vention localisation)
            return {
                "type": result.get("type", "neutral"),
                "topic": result.get("topic", "unknown"),
                "action": result.get("action", ""),
                "timeframe": result.get("timeframe", "ongoing"),
                "sentiment": result.get("sentiment", "neutral"),
                "confidence": float(result.get("confidence", 0.5)),
                "entities": result.get("entities", [])
            }

        except Exception as e:
            logger.error(f"LLM classification failed: {e}")
            return {
                "type": "neutral",
                "topic": "unknown",
                "action": "",
                "timeframe": "ongoing",
                "sentiment": "neutral",
                "confidence": 0.0,
                "entities": []
            }
```

#### 2. Modifier `src/backend/features/memory/gardener.py`

**Ajouter extraction pr√©f√©rences** :

```python
# Vers ligne 150, dans garden_thread()

async def garden_thread(self, thread_id: str, user_sub: str = None):
    """Jardinage thread : concepts + pr√©f√©rences"""

    # Extraction concepts (existant)
    concepts = await self.extract_concepts(nodes)

    # üÜï Extraction pr√©f√©rences/intentions
    from backend.features.memory.preference_extractor import PreferenceExtractor

    extractor = PreferenceExtractor(self.llm_client)
    preferences = await extractor.extract(
        messages=nodes,
        user_sub=user_sub or "anonymous",
        thread_id=thread_id
    )

    # Vectoriser pr√©f√©rences
    if preferences:
        await self._vectorize_preferences(preferences, user_sub)
        logger.info(f"Vectorized {len(preferences)} preferences for thread {thread_id}")

async def _vectorize_preferences(self, records: List[PreferenceRecord], user_sub: str):
    """Vectorise pr√©f√©rences dans collection d√©di√©e"""
    from backend.features.memory.preference_extractor import PreferenceRecord

    collection_name = f"memory_preferences_{user_sub}"

    for rec in records:
        # Embedding du texte + topic
        text_to_embed = f"{rec.topic}: {rec.text}"
        embedding = await self.embedder.embed(text_to_embed)

        # Stocker avec m√©tadonn√©es
        await self.vector_store.upsert(
            collection=collection_name,
            id=rec.id,
            vector=embedding,
            metadata={
                "type": rec.type,
                "topic": rec.topic,
                "action": rec.action,
                "timeframe": rec.timeframe,
                "sentiment": rec.sentiment,
                "confidence": rec.confidence,
                "entities": rec.entities,
                "source_message_id": rec.source_message_id,
                "thread_id": rec.thread_id,
                "captured_at": rec.captured_at
            }
        )
```

### Tests P1.2

```python
# tests/memory/test_preference_extractor.py

import pytest
from backend.features.memory.preference_extractor import PreferenceExtractor, PreferenceRecord

@pytest.fixture
def mock_llm():
    class MockLLM:
        async def generate(self, **kwargs):
            return '{"type": "preference", "topic": "Python", "action": "apprendre", "timeframe": "ongoing", "sentiment": "positive", "confidence": 0.9, "entities": ["FastAPI"]}'
    return MockLLM()

@pytest.mark.asyncio
async def test_extract_preference(mock_llm):
    extractor = PreferenceExtractor(mock_llm)

    messages = [
        {"role": "user", "content": "Je pr√©f√®re Python √† Java", "id": "msg1"},
        {"role": "assistant", "content": "Compris", "id": "msg2"}
    ]

    records = await extractor.extract(messages, user_sub="user123", thread_id="thread1")

    assert len(records) == 1
    assert records[0].type == "preference"
    assert records[0].topic == "Python"
    assert records[0].confidence >= 0.6

@pytest.mark.asyncio
async def test_lexical_filtering(mock_llm):
    extractor = PreferenceExtractor(mock_llm)

    messages = [
        {"role": "user", "content": "Bonjour comment √ßa va ?", "id": "msg1"}
    ]

    records = await extractor.extract(messages, user_sub="user123", thread_id="thread1")

    assert len(records) == 0  # Filtr√© (pas de verbe cible)
    assert extractor.stats["filtered"] == 1
```

### Crit√®res de succ√®s P1.2

- ‚úÖ `PreferenceExtractor` cr√©√© et test√©
- ‚úÖ Filtrage lexical r√©duit appels LLM de >70%
- ‚úÖ Classification LLM atteint pr√©cision >0.85 (corpus validation)
- ‚úÖ Vectorisation dans collection `memory_preferences_{user_sub}`
- ‚úÖ D√©duplication par `(user_sub, topic, type)` fonctionne
- ‚úÖ Tests unitaires passent

---

## üì¶ P1.3 - INSTRUMENTATION M√âTRIQUES (1-2h)

### Objectif
Compl√©ter m√©triques Prometheus pour cache + nouveau pipeline pr√©f√©rences.

### M√©triques √† ajouter

#### Cache (3 m√©triques manquantes)

```python
# src/backend/features/memory/analyzer.py

from prometheus_client import Counter, Gauge

CACHE_HITS_TOTAL = Counter(
    "memory_analysis_cache_hits_total",
    "Nombre total de cache hits"
)

CACHE_MISSES_TOTAL = Counter(
    "memory_analysis_cache_misses_total",
    "Nombre total de cache misses"
)

CACHE_SIZE = Gauge(
    "memory_analysis_cache_size",
    "Taille actuelle du cache in-memory"
)

# Dans analyze_session() :
if hash_key in _ANALYSIS_CACHE:
    CACHE_HITS_TOTAL.inc()  # üÜï
    return cached_result
else:
    CACHE_MISSES_TOTAL.inc()  # üÜï

# Apr√®s ajout au cache :
CACHE_SIZE.set(len(_ANALYSIS_CACHE))  # üÜï
```

#### Pr√©f√©rences (5 nouvelles m√©triques)

```python
# src/backend/features/memory/preference_extractor.py

from prometheus_client import Counter, Histogram

PREFERENCES_EXTRACTED_TOTAL = Counter(
    "memory_preferences_extracted_total",
    "Total pr√©f√©rences/intentions extraites",
    ["type"]  # preference, intent, constraint
)

PREFERENCES_CONFIDENCE = Histogram(
    "memory_preferences_confidence",
    "Distribution scores de confiance",
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

PREFERENCES_EXTRACTION_DURATION = Histogram(
    "memory_preferences_extraction_duration_seconds",
    "Dur√©e extraction pr√©f√©rences",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0]
)

# Dans extract() :
PREFERENCES_EXTRACTED_TOTAL.labels(type=record.type).inc()
PREFERENCES_CONFIDENCE.observe(record.confidence)
PREFERENCES_EXTRACTION_DURATION.observe(duration)
```

### Tests m√©triques

```bash
# G√©n√©rer donn√©es
curl -X POST http://localhost:8000/api/memory/analyze \
  -d '{"session_id":"test", "force":true}'

# V√©rifier m√©triques
curl http://localhost:8000/api/metrics | grep -E "cache|preference"

# Attendu :
# memory_analysis_cache_hits_total 1.0
# memory_analysis_cache_misses_total 2.0
# memory_preferences_extracted_total{type="preference"} 5.0
```

### Crit√®res de succ√®s P1.3

- ‚úÖ 3 m√©triques cache instrument√©es
- ‚úÖ 5 m√©triques pr√©f√©rences instrument√©es
- ‚úÖ Compteurs incr√©mentent correctement en tests
- ‚úÖ Histogrammes capturent distributions
- ‚úÖ Dashboard Grafana mis √† jour

---

## üìä ORDRE D'EX√âCUTION RECOMMAND√â

### Semaine 1 : D√©portation asynchrone (P1.1)
**Dur√©e** : 3-4h
1. Cr√©er `task_queue.py` (1h)
2. Ajouter `analyze_session_async()` (30min)
3. Modifier lifecycle `main.py` (30min)
4. Tests unitaires (1h)
5. Tests int√©gration WebSocket (1h)

### Semaine 2 : Extension extraction (P1.2)
**Dur√©e** : 6-8h
1. Cr√©er `preference_extractor.py` (2h)
2. Impl√©menter filtrage lexical (1h)
3. Impl√©menter classification LLM (2h)
4. Int√©grer dans `MemoryGardener` (1h)
5. Vectorisation collection d√©di√©e (1h)
6. Tests + validation corpus (2h)

### Semaine 3 : Instrumentation (P1.3)
**Dur√©e** : 1-2h
1. M√©triques cache (30min)
2. M√©triques pr√©f√©rences (30min)
3. Tests m√©triques (30min)
4. Dashboard Grafana update (30min)

---

## ‚úÖ CHECKLIST FINALE P1

### P1.1 - D√©portation
- [ ] `MemoryTaskQueue` cr√©√©e
- [ ] Workers d√©marrent/arr√™tent
- [ ] `analyze_session_async()` impl√©ment√©e
- [ ] Tests unitaires passent
- [ ] Latence WebSocket <100ms valid√©e

### P1.2 - Extraction
- [ ] `PreferenceExtractor` cr√©√©
- [ ] Filtrage lexical op√©rationnel
- [ ] Classification LLM >0.85 pr√©cision
- [ ] Vectorisation fonctionnelle
- [ ] D√©duplication test√©e
- [ ] Corpus validation (100+50 messages)

### P1.3 - M√©triques
- [ ] Cache metrics instrument√©es (3)
- [ ] Preferences metrics instrument√©es (5)
- [ ] Tests m√©triques valid√©s
- [ ] Dashboard Grafana mis √† jour

---

## üöÄ VALIDATION POST-IMPL√âMENTATION

### Tests end-to-end

```bash
# 1. D√©marrer app
python -m uvicorn backend.main:app --reload

# 2. Tester extraction pr√©f√©rences
curl -X POST http://localhost:8000/api/memory/garden \
  -H "Content-Type: application/json" \
  -d '{"thread_id":"thread123","user_sub":"user456"}'

# 3. V√©rifier collection vector store
# (doit contenir memory_preferences_user456)

# 4. V√©rifier m√©triques
curl http://localhost:8000/api/metrics | grep preferences

# 5. Tester queue async
# (latence WS <100ms m√™me avec analyse en cours)
```

### Crit√®res globaux de succ√®s P1

- ‚úÖ Event loop WebSocket non bloqu√© (latence <100ms constante)
- ‚úÖ Pr√©f√©rences/intentions extraites automatiquement
- ‚úÖ Collection `memory_preferences_*` peupl√©e
- ‚úÖ Pr√©cision extraction >0.85, rappel >0.75
- ‚úÖ 8 nouvelles m√©triques Prometheus expos√©es
- ‚úÖ Tests unitaires + int√©gration passent
- ‚úÖ Documentation mise √† jour

---

## üìö RESSOURCES

### Fichiers √† lire

1. [docs/memory-roadmap.md](docs/memory-roadmap.md) - Roadmap compl√®te
2. [src/backend/features/memory/analyzer.py](src/backend/features/memory/analyzer.py) - Analyseur actuel
3. [src/backend/features/memory/gardener.py](src/backend/features/memory/gardener.py) - Jardinier actuel
4. [docs/deployments/2025-10-09-validation-phase3-complete.md](docs/deployments/2025-10-09-validation-phase3-complete.md) - √âtat Phase 3

### Documentation √† cr√©er

- `docs/memory/p1-task-queue.md` - Architecture file de t√¢ches
- `docs/memory/p1-preference-extraction.md` - Pipeline pr√©f√©rences
- `docs/memory/p1-metrics.md` - Nouvelles m√©triques

### Commits recommand√©s

```bash
git commit -m "feat(P1.1): file de t√¢ches asynchrone MemoryTaskQueue"
git commit -m "feat(P1.2): extraction pr√©f√©rences/intentions - pipeline hybride"
git commit -m "feat(P1.3): instrumentation m√©triques cache + pr√©f√©rences"
git commit -m "docs: P1 enrichissement m√©moire - sp√©cification compl√®te"
```

---

## üÜò TROUBLESHOOTING

### Probl√®me : Workers ne d√©marrent pas

```python
# V√©rifier lifecycle
import logging
logging.basicConfig(level=logging.DEBUG)

# Logs attendus :
# INFO: MemoryTaskQueue started with 2 workers
# INFO: Worker 0 started
# INFO: Worker 1 started
```

### Probl√®me : LLM classification √©choue

```python
# V√©rifier prompt + response_format
response = await llm.generate(
    prompt=prompt,
    response_format="json",  # üî• Critique pour OpenAI
    temperature=0.1
)

# Ajouter fallback
try:
    result = json.loads(response)
except json.JSONDecodeError:
    logger.error(f"Invalid JSON: {response}")
    return {"type": "neutral", ...}
```

### Probl√®me : Collection vector store non cr√©√©e

```python
# V√©rifier backend vector store
if vector_store.backend == "chroma":
    # Collections auto-cr√©√©es
    pass
elif vector_store.backend == "qdrant":
    # Cr√©er manuellement
    await vector_store.create_collection(
        name=f"memory_preferences_{user_sub}",
        vector_size=1536  # text-embedding-3-large
    )
```

---

## üéØ OBJECTIF FINAL

√Ä la fin de P1, tu auras :

1. ‚úÖ **File de t√¢ches asynchrone** √©vitant blocages event loop
2. ‚úÖ **Pipeline enrichi** capturant pr√©f√©rences/intentions/contraintes
3. ‚úÖ **Collection vectorielle** d√©di√©e avec m√©tadonn√©es riches
4. ‚úÖ **8 nouvelles m√©triques** Prometheus pour monitoring
5. ‚úÖ **Tests valid√©s** (unitaires + int√©gration + corpus)
6. ‚úÖ **Documentation compl√®te** architecture + usage

**Phase P1 pr√©pare P2 (r√©activit√© proactive)** o√π les pr√©f√©rences/intentions seront utilis√©es pour d√©clencher suggestions contextuelles.

---

**Bon courage pour P1 ! üöÄ**

*G√©n√©r√© par Claude Code - 2025-10-09*
*Bas√© sur [docs/memory-roadmap.md](docs/memory-roadmap.md)*
