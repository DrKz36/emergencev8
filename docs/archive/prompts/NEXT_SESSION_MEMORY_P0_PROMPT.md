# Prompt Session Prochaine Instance - ImplÃ©mentation MÃ©moire P0

**Date crÃ©ation** : 2025-10-10 17:00 UTC
**Agent cible** : Claude Code / Codex
**PrioritÃ©** : ğŸ”´ **CRITIQUE** - Gaps bloquants mÃ©moire LTM
**DurÃ©e estimÃ©e** : 4-6h (session complÃ¨te)

---

## ğŸ¯ Objectif de la session

RÃ©soudre les **3 gaps critiques** empÃªchant la mÃ©moire Ã  long terme (LTM) de fonctionner correctement, identifiÃ©s dans [docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md](docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md).

**SymptÃ´me utilisateur** :
> "Quand je demande aux agents de quoi nous avons parlÃ© jusqu'Ã  maintenant, les conversations archivÃ©es ne sont jamais Ã©voquÃ©es et les concepts associÃ©s ne ressortent pas."

**Impact business** : La mÃ©moire P1 (extraction prÃ©fÃ©rences) est dÃ©ployÃ©e mais **NON UTILISABLE** car les threads archivÃ©s ne sont jamais consolidÃ©s dans ChromaDB.

---

## ğŸ“š Lecture OBLIGATOIRE avant de commencer

**Ordre de lecture** (â±ï¸ 15-20 minutes) :

1. **[AGENT_SYNC.md](AGENT_SYNC.md)** (sections clÃ©s)
   - Ã‰tat actuel du dÃ©pÃ´t (branche, commits rÃ©cents, dÃ©ploiements)
   - Zones de travail en cours (sessions Claude Code + Codex rÃ©centes)
   - Checklist de travail obligatoire

2. **[docs/passation.md](docs/passation.md)** (3 derniÃ¨res entrÃ©es)
   - [2025-10-10 16:45] Optimisations Performance Frontend
   - [2025-10-10 14:30] Hotfix P1.3 - user_sub Context
   - [2025-10-10 03:20] DÃ©ploiement P1+P0 production

3. **[docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md](docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md)** â­
   - **DOCUMENT CLÃ‰** : Analyse dÃ©taillÃ©e des 3 gaps critiques
   - Preuves code, impact utilisateur, mÃ©triques manquantes
   - Solutions proposÃ©es pour chaque gap

4. **[docs/memory-roadmap.md](docs/memory-roadmap.md)**
   - Vue d'ensemble architecture mÃ©moire (STM, LTM, vitalitÃ©)
   - Ã‰tat Phase P0 (persistance cross-device) âœ… COMPLÃ‰TÃ‰
   - Ã‰tat Phase P1 (dÃ©portation async + prÃ©fÃ©rences) âœ… COMPLÃ‰TÃ‰
   - Phase P2 Ã  venir (rÃ©activitÃ© proactive)

5. **`git status` + `git log --oneline -10`**
   - VÃ©rifier Ã©tat working tree
   - Derniers commits (c550fac perf(frontend), 2523713 fix(P1.3), etc.)

---

## ğŸ”´ Gap #1 : Threads archivÃ©s JAMAIS consolidÃ©s dans LTM

### ProblÃ¨me

**Workflow actuel** :
```
1. User archive conversation
   â””â”€> UPDATE threads SET archived = 1
2. Consolidation mÃ©moire (tend-garden)
   â””â”€> queries.get_threads(include_archived=False)  â† PAR DÃ‰FAUT !
   â””â”€> RÃ©cupÃ¨re uniquement threads actifs
3. Extraction concepts
   â””â”€> Analyse uniquement conversations actives
   â””â”€> Threads archivÃ©s IGNORÃ‰S
4. ChromaDB (LTM)
   â””â”€> Ne contient JAMAIS les concepts des threads archivÃ©s
```

### Solution Ã  implÃ©menter

#### 1. Nouvel endpoint `/api/memory/consolidate-archived` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/memory/router.py`

```python
@router.post("/consolidate-archived")
async def consolidate_archived_threads(
    background_tasks: BackgroundTasks,
    user_id: Optional[str] = Header(None, alias="x-user-id"),
    batch_size: int = Query(10, ge=1, le=50),
    force: bool = Query(False),
    gardener: MemoryGardener = Depends(get_memory_gardener),
) -> Dict[str, Any]:
    """
    Consolide tous les threads archivÃ©s dans ChromaDB.

    - batch_size : nombre de threads traitÃ©s par batch (dÃ©faut 10)
    - force : si True, re-consolide mÃªme si dÃ©jÃ  fait (dÃ©faut False)
    - Retourne : {total_archived, consolidated, skipped, errors, duration_ms}
    """
    start_time = time.time()

    # RÃ©cupÃ©rer tous threads archivÃ©s non consolidÃ©s
    threads = await queries.get_threads(
        db=gardener.db,
        session_id=None,  # Tous utilisateurs ou filtrer par user_id
        user_id=user_id,
        archived_only=True,  # Nouveau paramÃ¨tre Ã  ajouter dans queries.py
        limit=batch_size
    )

    total_archived = len(threads)
    consolidated = 0
    skipped = 0
    errors = []

    for thread in threads:
        try:
            # VÃ©rifier si dÃ©jÃ  consolidÃ© (metadata archived_consolidated_at)
            if not force and thread.get("metadata", {}).get("archived_consolidated_at"):
                skipped += 1
                continue

            # Consolider via tend_single_thread
            result = await gardener._tend_single_thread(
                thread_id=thread["id"],
                user_id=thread.get("user_id"),
                session_id=thread.get("session_id")
            )

            # Marquer comme consolidÃ© dans metadata
            if result.get("status") == "completed":
                await queries.update_thread_metadata(
                    db=gardener.db,
                    thread_id=thread["id"],
                    metadata={
                        **thread.get("metadata", {}),
                        "archived_consolidated_at": datetime.now(UTC).isoformat()
                    }
                )
                consolidated += 1
            else:
                errors.append({
                    "thread_id": thread["id"],
                    "error": result.get("error", "Unknown error")
                })
        except Exception as e:
            logger.error(f"Error consolidating archived thread {thread['id']}: {e}")
            errors.append({
                "thread_id": thread["id"],
                "error": str(e)
            })

    duration_ms = int((time.time() - start_time) * 1000)

    # MÃ©triques Prometheus
    MEMORY_ARCHIVED_CONSOLIDATED.inc(consolidated)

    return {
        "status": "completed",
        "total_archived": total_archived,
        "consolidated": consolidated,
        "skipped": skipped,
        "errors": errors,
        "duration_ms": duration_ms
    }
```

#### 2. Modifier `queries.get_threads()` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/core/database/queries.py`

Ajouter paramÃ¨tre `archived_only`:

```python
async def get_threads(
    db: DatabaseManager,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    type_: Optional[str] = None,
    include_archived: bool = False,
    archived_only: bool = False,  # â† NOUVEAU PARAMÃˆTRE
    limit: int = 20,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """
    RÃ©cupÃ¨re les threads selon filtres.

    - archived_only : si True, ne retourne QUE les threads archivÃ©s (archived = 1)
    - include_archived : si True (et archived_only False), inclut archivÃ©s et actifs
    - Par dÃ©faut : uniquement threads actifs (archived = 0)
    """
    clauses = []
    params = []

    # ... autres filtres (session_id, user_id, type_) ...

    # Filtre archivage
    if archived_only:
        clauses.append("archived = 1")
    elif not include_archived:
        clauses.append("archived = 0")

    # ... reste de la fonction ...
```

#### 3. Endpoint trigger auto aprÃ¨s archivage (PRIORITÃ‰ 2)

**Fichier** : `src/backend/features/threads/router.py`

Dans l'endpoint `PUT /api/threads/{thread_id}` (archivage) :

```python
@router.put("/{thread_id}")
async def update_thread(
    thread_id: str,
    update: ThreadUpdate,
    background_tasks: BackgroundTasks,
    db: DatabaseManager = Depends(get_db),
    gardener: MemoryGardener = Depends(get_memory_gardener),
    user_id: str = Depends(get_current_user_id),
):
    # ... validation et update thread ...

    # Si archivage dÃ©tectÃ© â†’ trigger consolidation immÃ©diate
    if update.archived and not existing_thread.get("archived"):
        logger.info(f"Thread {thread_id} archived, scheduling consolidation")

        # Background task asynchrone (ne bloque pas la rÃ©ponse)
        background_tasks.add_task(
            gardener._tend_single_thread,
            thread_id=thread_id,
            user_id=user_id,
            session_id=existing_thread.get("session_id")
        )

        # MÃ©trique Prometheus
        MEMORY_ARCHIVED_AUTO_TRIGGERED.inc()

    return updated_thread
```

#### 4. Tests unitaires (PRIORITÃ‰ 1)

**Fichier** : `tests/backend/features/test_memory_archived_consolidation.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_consolidate_archived_threads_success():
    """Test consolidation batch threads archivÃ©s."""
    # Setup mock threads archivÃ©s
    mock_threads = [
        {"id": "thread1", "archived": 1, "metadata": {}},
        {"id": "thread2", "archived": 1, "metadata": {}},
    ]

    with patch("src.backend.core.database.queries.get_threads", return_value=mock_threads):
        with patch.object(gardener, "_tend_single_thread", return_value={"status": "completed"}):
            result = await consolidate_archived_threads(
                background_tasks=BackgroundTasks(),
                batch_size=10,
                force=False,
                gardener=gardener
            )

    assert result["status"] == "completed"
    assert result["consolidated"] == 2
    assert result["skipped"] == 0
    assert len(result["errors"]) == 0

@pytest.mark.asyncio
async def test_consolidate_archived_threads_skip_already_done():
    """Test skip threads dÃ©jÃ  consolidÃ©s."""
    mock_threads = [
        {"id": "thread1", "archived": 1, "metadata": {"archived_consolidated_at": "2025-10-10T12:00:00Z"}},
    ]

    with patch("src.backend.core.database.queries.get_threads", return_value=mock_threads):
        result = await consolidate_archived_threads(
            background_tasks=BackgroundTasks(),
            batch_size=10,
            force=False,
            gardener=gardener
        )

    assert result["consolidated"] == 0
    assert result["skipped"] == 1

@pytest.mark.asyncio
async def test_auto_trigger_on_archive():
    """Test trigger automatique lors archivage thread."""
    # ... test que background task est bien schedulÃ©e ...
```

#### 5. MÃ©triques Prometheus (PRIORITÃ‰ 2)

**Fichier** : `src/backend/features/memory/gardener.py`

```python
from prometheus_client import Counter

MEMORY_ARCHIVED_CONSOLIDATED = Counter(
    "memory_archived_consolidated_total",
    "Nombre total de threads archivÃ©s consolidÃ©s dans LTM"
)

MEMORY_ARCHIVED_AUTO_TRIGGERED = Counter(
    "memory_archived_auto_triggered_total",
    "Nombre de consolidations auto-dÃ©clenchÃ©es lors archivage"
)

MEMORY_ARCHIVED_CONSOLIDATION_ERRORS = Counter(
    "memory_archived_consolidation_errors_total",
    "Erreurs lors consolidation threads archivÃ©s",
    labelnames=["error_type"]
)
```

---

## ğŸŸ¡ Gap #2 : PrÃ©fÃ©rences extraites JAMAIS sauvÃ©es dans ChromaDB

### ProblÃ¨me

**Workflow actuel** :
```
1. PreferenceExtractor.extract()
   â””â”€> Extraction prÃ©fÃ©rences depuis messages âœ…
   â””â”€> Retourne liste PreferenceRecord âœ…

2. MemoryAnalyzer.analyze_session()
   â””â”€> Appelle preference_extractor.extract() âœ…
   â””â”€> Stocke rÃ©sultats dans session.metadata["preferences"] âœ…
   â””â”€> NE SAUVEGARDE PAS dans ChromaDB âŒ

3. ChromaDB collection 'memory_preferences'
   â””â”€> Reste VIDE (0 embeddings)
```

### Solution Ã  implÃ©menter

#### 1. MÃ©thode `save_preferences_to_vector_db()` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

Ajouter aprÃ¨s `analyze_session()` :

```python
async def save_preferences_to_vector_db(
    self,
    preferences: List[Dict[str, Any]],
    user_id: str,
    thread_id: str,
    session_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Sauvegarde prÃ©fÃ©rences extraites dans ChromaDB.

    Args:
        preferences : liste prÃ©fÃ©rences depuis PreferenceExtractor
        user_id : identifiant utilisateur
        thread_id : identifiant thread
        session_id : identifiant session (optionnel)

    Returns:
        {saved: int, skipped: int, errors: List[str]}
    """
    if not preferences:
        logger.info("No preferences to save")
        return {"saved": 0, "skipped": 0, "errors": []}

    if not self.vector_service:
        logger.warning("VectorService not available, skipping preference save")
        return {"saved": 0, "skipped": len(preferences), "errors": ["VectorService unavailable"]}

    saved = 0
    skipped = 0
    errors = []

    for pref in preferences:
        try:
            # GÃ©nÃ©rer embedding du texte prÃ©fÃ©rence
            text = pref.get("topic", "") + " " + pref.get("action", "")
            embedding = await self._generate_embedding(text)

            if not embedding:
                skipped += 1
                continue

            # ID unique : {user_id}_{topic}_{type}
            pref_id = f"{user_id}_{pref['topic']}_{pref['type']}"

            # Metadata complÃ¨te
            metadata = {
                "user_id": user_id,
                "thread_id": thread_id,
                "session_id": session_id or "unknown",
                "type": pref["type"],  # preference | intent | constraint
                "topic": pref["topic"],
                "action": pref.get("action", ""),
                "timeframe": pref.get("timeframe", "ongoing"),
                "sentiment": pref.get("sentiment", "neutral"),
                "confidence": pref.get("confidence", 0.5),
                "source_message_id": pref.get("source_message_id", ""),
                "captured_at": datetime.now(UTC).isoformat(),
                "entities": json.dumps(pref.get("entities", [])),
            }

            # Sauvegarder dans collection 'memory_preferences'
            await self.vector_service.add(
                collection_name="memory_preferences",
                ids=[pref_id],
                embeddings=[embedding],
                metadatas=[metadata],
                documents=[text]
            )

            saved += 1
            logger.info(f"Saved preference {pref_id} to ChromaDB")

            # MÃ©trique Prometheus
            MEMORY_PREFERENCES_SAVED.labels(type=pref["type"]).inc()

        except Exception as e:
            logger.error(f"Error saving preference to ChromaDB: {e}")
            errors.append(str(e))
            MEMORY_PREFERENCES_SAVE_ERRORS.inc()

    return {
        "saved": saved,
        "skipped": skipped,
        "errors": errors
    }
```

#### 2. Appeler `save_preferences_to_vector_db()` dans `analyze_session()` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

Dans la mÃ©thode `analyze_session()`, aprÃ¨s extraction prÃ©fÃ©rences :

```python
async def analyze_session(
    self,
    session: Session,
    user_sub: Optional[str] = None,
    user_id: Optional[str] = None,
    persist: bool = False,
) -> Dict[str, Any]:
    # ... code existant (extraction concepts, prÃ©fÃ©rences, etc.) ...

    # Nouvelle section : Sauvegarde prÃ©fÃ©rences dans ChromaDB
    if persist and self.preference_extractor and preferences:
        logger.info(f"Saving {len(preferences)} preferences to ChromaDB")

        pref_save_result = await self.save_preferences_to_vector_db(
            preferences=preferences,
            user_id=user_identifier,  # user_sub or user_id
            thread_id=session.thread_id or "unknown",
            session_id=session.session_id
        )

        result["preferences_saved"] = pref_save_result["saved"]
        result["preferences_skipped"] = pref_save_result["skipped"]
        result["preferences_errors"] = pref_save_result["errors"]

        logger.info(
            f"Preferences saved: {pref_save_result['saved']}, "
            f"skipped: {pref_save_result['skipped']}, "
            f"errors: {len(pref_save_result['errors'])}"
        )

    return result
```

#### 3. MÃ©thode helper `_generate_embedding()` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

```python
async def _generate_embedding(self, text: str) -> Optional[List[float]]:
    """GÃ©nÃ¨re embedding pour texte via ChatService."""
    if not self.chat_service:
        logger.warning("ChatService not available for embeddings")
        return None

    try:
        # Utiliser text-embedding-3-large (ou fallback)
        response = await self.chat_service.generate_embedding(
            text=text,
            model="text-embedding-3-large"
        )
        return response.get("embedding")
    except Exception as e:
        logger.error(f"Error generating embedding: {e}")
        return None
```

#### 4. Tests unitaires (PRIORITÃ‰ 1)

**Fichier** : `tests/backend/features/test_preference_persistence.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_save_preferences_to_vector_db_success():
    """Test sauvegarde prÃ©fÃ©rences dans ChromaDB."""
    preferences = [
        {
            "type": "preference",
            "topic": "python",
            "action": "prÃ©fÃ©rer",
            "confidence": 0.85,
            "source_message_id": "msg123"
        }
    ]

    with patch.object(analyzer, "_generate_embedding", return_value=[0.1] * 1536):
        with patch.object(analyzer.vector_service, "add", return_value=None):
            result = await analyzer.save_preferences_to_vector_db(
                preferences=preferences,
                user_id="user123",
                thread_id="thread456",
                session_id="session789"
            )

    assert result["saved"] == 1
    assert result["skipped"] == 0
    assert len(result["errors"]) == 0

@pytest.mark.asyncio
async def test_save_preferences_vector_service_unavailable():
    """Test graceful degradation si VectorService indisponible."""
    analyzer.vector_service = None

    result = await analyzer.save_preferences_to_vector_db(
        preferences=[{"type": "preference", "topic": "test"}],
        user_id="user123",
        thread_id="thread456"
    )

    assert result["saved"] == 0
    assert result["skipped"] == 1
    assert "VectorService unavailable" in result["errors"]
```

#### 5. MÃ©triques Prometheus (PRIORITÃ‰ 2)

**Fichier** : `src/backend/features/memory/analyzer.py`

```python
MEMORY_PREFERENCES_SAVED = Counter(
    "memory_preferences_saved_total",
    "Nombre de prÃ©fÃ©rences sauvegardÃ©es dans ChromaDB",
    labelnames=["type"]  # preference | intent | constraint
)

MEMORY_PREFERENCES_SAVE_ERRORS = Counter(
    "memory_preferences_save_errors_total",
    "Erreurs lors sauvegarde prÃ©fÃ©rences dans ChromaDB"
)
```

---

## ğŸŸ¢ Gap #3 : Aucune recherche prÃ©fÃ©rences lors rappel LTM

### ProblÃ¨me

**Workflow actuel** :
```
1. Agent gÃ©nÃ¨re rÃ©ponse
   â””â”€> context_builder.build_context()
   â””â”€> Recherche dans collection 'emergence_knowledge' (concepts) âœ…
   â””â”€> NE recherche PAS dans 'memory_preferences' âŒ

2. Context final envoyÃ© au LLM
   â””â”€> Contient concepts gÃ©nÃ©raux âœ…
   â””â”€> NE contient PAS prÃ©fÃ©rences utilisateur âŒ
```

### Solution Ã  implÃ©menter

#### 1. MÃ©thode `search_preferences()` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/memory/vector_service.py`

```python
async def search_preferences(
    self,
    query: str,
    user_id: str,
    limit: int = 5,
    min_confidence: float = 0.6
) -> List[Dict[str, Any]]:
    """
    Recherche prÃ©fÃ©rences utilisateur dans ChromaDB.

    Args:
        query : texte requÃªte (embedded)
        user_id : filtre par utilisateur
        limit : nombre max rÃ©sultats
        min_confidence : seuil confidence minimum

    Returns:
        Liste prÃ©fÃ©rences {type, topic, action, confidence, ...}
    """
    try:
        # GÃ©nÃ©rer embedding requÃªte
        query_embedding = await self._generate_embedding(query)

        if not query_embedding:
            logger.warning("Could not generate embedding for preference search")
            return []

        # Recherche dans collection 'memory_preferences'
        results = await self.collection_preferences.query(
            query_embeddings=[query_embedding],
            n_results=limit,
            where={"user_id": user_id}  # Filtre par utilisateur
        )

        # Parser rÃ©sultats
        preferences = []
        if results and results.get("metadatas"):
            for i, metadata in enumerate(results["metadatas"][0]):
                confidence = metadata.get("confidence", 0.5)

                # Filtre par confidence minimum
                if confidence >= min_confidence:
                    preferences.append({
                        "type": metadata.get("type"),
                        "topic": metadata.get("topic"),
                        "action": metadata.get("action"),
                        "confidence": confidence,
                        "sentiment": metadata.get("sentiment"),
                        "captured_at": metadata.get("captured_at"),
                        "distance": results["distances"][0][i] if results.get("distances") else 0.0
                    })

        logger.info(f"Found {len(preferences)} preferences for user {user_id}")
        return preferences

    except Exception as e:
        logger.error(f"Error searching preferences: {e}")
        return []
```

#### 2. IntÃ©grer dans `ContextBuilder` (PRIORITÃ‰ 1)

**Fichier** : `src/backend/features/chat/context_builder.py`

Dans la mÃ©thode `build_context()` :

```python
async def build_context(
    self,
    user_message: str,
    session: Session,
    user_id: Optional[str] = None,
    include_preferences: bool = True,  # â† NOUVEAU PARAMÃˆTRE
) -> str:
    """
    Construit contexte complet pour agent.

    - STM : rÃ©sumÃ© conversation courante
    - LTM concepts : recherche vectorielle concepts gÃ©nÃ©raux
    - LTM prÃ©fÃ©rences : recherche prÃ©fÃ©rences utilisateur (NOUVEAU)
    """
    context_parts = []

    # 1. STM (court terme)
    stm_summary = self._build_stm_summary(session)
    if stm_summary:
        context_parts.append(f"## Contexte court terme\n{stm_summary}")

    # 2. LTM Concepts (concepts gÃ©nÃ©raux)
    concepts = await self.vector_service.search_concepts(
        query=user_message,
        limit=5
    )
    if concepts:
        concepts_text = "\n".join([f"- {c['text']}" for c in concepts])
        context_parts.append(f"## Concepts pertinents\n{concepts_text}")

    # 3. LTM PrÃ©fÃ©rences (NOUVEAU)
    if include_preferences and user_id:
        preferences = await self.vector_service.search_preferences(
            query=user_message,
            user_id=user_id,
            limit=5,
            min_confidence=0.6
        )

        if preferences:
            prefs_text = "\n".join([
                f"- {p['type'].upper()}: {p['topic']} ({p['action']}) "
                f"[confiance: {p['confidence']:.2f}]"
                for p in preferences
            ])
            context_parts.append(f"## PrÃ©fÃ©rences utilisateur\n{prefs_text}")

    return "\n\n".join(context_parts)
```

#### 3. Tests unitaires (PRIORITÃ‰ 1)

**Fichier** : `tests/backend/features/test_preference_retrieval.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_search_preferences_success():
    """Test recherche prÃ©fÃ©rences ChromaDB."""
    mock_results = {
        "metadatas": [[
            {
                "type": "preference",
                "topic": "python",
                "action": "prÃ©fÃ©rer",
                "confidence": 0.85,
                "sentiment": "positive",
                "captured_at": "2025-10-10T12:00:00Z"
            }
        ]],
        "distances": [[0.15]]
    }

    with patch.object(vector_service.collection_preferences, "query", return_value=mock_results):
        preferences = await vector_service.search_preferences(
            query="python development",
            user_id="user123",
            limit=5,
            min_confidence=0.6
        )

    assert len(preferences) == 1
    assert preferences[0]["type"] == "preference"
    assert preferences[0]["topic"] == "python"
    assert preferences[0]["confidence"] == 0.85

@pytest.mark.asyncio
async def test_context_builder_includes_preferences():
    """Test intÃ©gration prÃ©fÃ©rences dans contexte."""
    mock_preferences = [
        {"type": "preference", "topic": "python", "action": "prÃ©fÃ©rer", "confidence": 0.85}
    ]

    with patch.object(context_builder.vector_service, "search_preferences", return_value=mock_preferences):
        context = await context_builder.build_context(
            user_message="Comment coder en Python ?",
            session=mock_session,
            user_id="user123",
            include_preferences=True
        )

    assert "PrÃ©fÃ©rences utilisateur" in context
    assert "python" in context.lower()
```

---

## ğŸ“‹ Checklist de travail

### Avant de commencer

- [ ] Lecture AGENT_SYNC.md (Ã©tat dÃ©pÃ´t, commits rÃ©cents, dÃ©ploiements)
- [ ] Lecture docs/passation.md (3 derniÃ¨res entrÃ©es)
- [ ] Lecture MEMORY_LTM_GAPS_ANALYSIS.md â­ (document clÃ©)
- [ ] Lecture docs/memory-roadmap.md (contexte architecture)
- [ ] `git fetch --all --prune` (sync avec remote)
- [ ] `git status` (vÃ©rifier working tree propre)
- [ ] `git log --oneline -10` (derniers commits)

### Pendant le dÃ©veloppement

#### Gap #1 : Threads archivÃ©s

- [ ] CrÃ©er endpoint `POST /api/memory/consolidate-archived` (router.py)
- [ ] Modifier `queries.get_threads()` (ajouter `archived_only` param)
- [ ] Ajouter trigger auto dans `PUT /api/threads/{id}` (archivage)
- [ ] CrÃ©er tests `test_memory_archived_consolidation.py` (4-5 tests)
- [ ] Ajouter mÃ©triques Prometheus (MEMORY_ARCHIVED_*)
- [ ] Tests locaux : `pytest tests/backend/features/test_memory_archived_consolidation.py -v`

#### Gap #2 : PrÃ©fÃ©rences ChromaDB

- [ ] CrÃ©er `save_preferences_to_vector_db()` dans analyzer.py
- [ ] CrÃ©er `_generate_embedding()` helper dans analyzer.py
- [ ] Appeler `save_preferences_to_vector_db()` dans `analyze_session()`
- [ ] CrÃ©er tests `test_preference_persistence.py` (3-4 tests)
- [ ] Ajouter mÃ©triques Prometheus (MEMORY_PREFERENCES_SAVED)
- [ ] Tests locaux : `pytest tests/backend/features/test_preference_persistence.py -v`

#### Gap #3 : Recherche prÃ©fÃ©rences LTM

- [ ] CrÃ©er `search_preferences()` dans vector_service.py
- [ ] IntÃ©grer dans `ContextBuilder.build_context()` (context_builder.py)
- [ ] CrÃ©er tests `test_preference_retrieval.py` (2-3 tests)
- [ ] Tests locaux : `pytest tests/backend/features/test_preference_retrieval.py -v`

### Tests & QualitÃ©

- [ ] `pytest tests/backend/features/test_memory_*.py -v` (tous tests mÃ©moire)
- [ ] `pytest` (suite complÃ¨te backend)
- [ ] `ruff check src/backend` (linting)
- [ ] `mypy src/backend --ignore-missing-imports` (types)
- [ ] `npm run build` (frontend, vÃ©rifier 0 rÃ©gression)

### Documentation

- [ ] Mise Ã  jour `AGENT_SYNC.md` (section "Zones de travail en cours")
- [ ] Nouvelle entrÃ©e `docs/passation.md` (contexte, actions, tests, rÃ©sultats)
- [ ] Mise Ã  jour `docs/memory-roadmap.md` (marquer P0 gaps rÃ©solus)
- [ ] CrÃ©er `docs/deployments/2025-10-10-p0-gaps-resolution.md` (rapport complet)

### Commit & Push

- [ ] `git add` (fichiers modifiÃ©s + nouveaux tests + docs)
- [ ] `git commit -m "feat(P0): rÃ©solution 3 gaps critiques mÃ©moire LTM"` (message dÃ©taillÃ©)
- [ ] `git push origin main`
- [ ] VÃ©rifier push GitHub OK

---

## ğŸ¯ RÃ©sultat attendu

### FonctionnalitÃ©s opÃ©rationnelles

1. **Endpoint `/api/memory/consolidate-archived`**
   - Batch consolidation threads archivÃ©s
   - Skip threads dÃ©jÃ  consolidÃ©s (sauf force=True)
   - MÃ©triques exposÃ©es

2. **Trigger auto lors archivage**
   - `PUT /api/threads/{id}` avec `archived=True`
   - Background task consolidation immÃ©diate
   - MÃ©trique auto-trigger incrÃ©mentÃ©e

3. **PrÃ©fÃ©rences sauvÃ©es ChromaDB**
   - Collection `memory_preferences` populÃ©e
   - DÃ©duplication par `(user_id, topic, type)`
   - MÃ©triques saved/errors exposÃ©es

4. **Recherche prÃ©fÃ©rences LTM**
   - `ContextBuilder.build_context()` inclut prÃ©fÃ©rences
   - Filtrage par user_id + confidence >= 0.6
   - Format lisible dans contexte agent

### Tests

- âœ… Tous tests mÃ©moire passent (>15 tests)
- âœ… 0 rÃ©gression tests existants
- âœ… Ruff + mypy clean
- âœ… Frontend build OK

### MÃ©triques nouvelles exposÃ©es

```
# Gap #1 - Threads archivÃ©s
memory_archived_consolidated_total
memory_archived_auto_triggered_total
memory_archived_consolidation_errors_total{error_type}

# Gap #2 - PrÃ©fÃ©rences ChromaDB
memory_preferences_saved_total{type}
memory_preferences_save_errors_total

# Existantes (P1)
memory_preferences_extracted_total{type}
memory_preferences_confidence (histogram)
memory_preferences_extraction_duration_seconds (histogram)
```

### Documentation

- `AGENT_SYNC.md` : section session P0 ajoutÃ©e
- `docs/passation.md` : entrÃ©e complÃ¨te avec rÃ©sultats
- `docs/memory-roadmap.md` : P0 gaps marquÃ©s rÃ©solved
- `docs/deployments/2025-10-10-p0-gaps-resolution.md` : rapport technique

---

## ğŸš€ Commande commit finale

```bash
git add \
  src/backend/features/memory/router.py \
  src/backend/features/memory/analyzer.py \
  src/backend/features/memory/vector_service.py \
  src/backend/features/threads/router.py \
  src/backend/features/chat/context_builder.py \
  src/backend/core/database/queries.py \
  tests/backend/features/test_memory_archived_consolidation.py \
  tests/backend/features/test_preference_persistence.py \
  tests/backend/features/test_preference_retrieval.py \
  docs/passation.md \
  docs/memory-roadmap.md \
  docs/deployments/2025-10-10-p0-gaps-resolution.md \
  AGENT_SYNC.md

git commit -m "feat(P0): rÃ©solution 3 gaps critiques mÃ©moire LTM

Gap #1 - Threads archivÃ©s consolidÃ©s
- Endpoint POST /api/memory/consolidate-archived (batch processing)
- Trigger auto lors archivage (background task)
- Param archived_only dans queries.get_threads()
- Metadata archived_consolidated_at pour skip duplicatas
- MÃ©triques: memory_archived_consolidated_total, auto_triggered_total

Gap #2 - PrÃ©fÃ©rences sauvÃ©es ChromaDB
- MÃ©thode save_preferences_to_vector_db() dans MemoryAnalyzer
- Helper _generate_embedding() pour embeddings
- Appel automatique dans analyze_session(persist=True)
- Collection memory_preferences populÃ©e
- MÃ©triques: memory_preferences_saved_total{type}, save_errors_total

Gap #3 - Recherche prÃ©fÃ©rences LTM
- MÃ©thode search_preferences() dans VectorService
- IntÃ©gration ContextBuilder.build_context()
- Filtrage user_id + confidence >= 0.6
- Section 'PrÃ©fÃ©rences utilisateur' dans contexte agent

Tests: 15+ nouveaux tests (100% passants), 0 rÃ©gression
Docs: passation + roadmap + deployment report
Impact: LTM mÃ©moire maintenant FONCTIONNELLE (threads archivÃ©s + prÃ©fÃ©rences)"

git push origin main
```

---

## ğŸ“ Contact & Questions

**Si blocage technique** :
1. Consulter `AGENT_SYNC.md` (section "Conflits & RÃ©solution")
2. Documenter dans `docs/passation.md` (section "Blocages")
3. Continuer sur tÃ¢ches non bloquantes
4. Ping FG (architecte) pour arbitrage

**Principe clÃ©** : Tests > Documentation > Communication

---

**Ce prompt est exhaustif et autonome.** La prochaine instance peut commencer immÃ©diatement aprÃ¨s lecture des documents obligatoires.

**Bonne session ! ğŸš€**
