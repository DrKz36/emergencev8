# Prompt Session Prochaine Instance - Impl√©mentation M√©moire P0

**Date cr√©ation** : 2025-10-10 17:00 UTC
**Agent cible** : Claude Code / Codex
**Priorit√©** : üî¥ **CRITIQUE** - Gaps bloquants m√©moire LTM
**Dur√©e estim√©e** : 4-6h (session compl√®te)

---

## üéØ Objectif de la session

R√©soudre les **3 gaps critiques** emp√™chant la m√©moire √† long terme (LTM) de fonctionner correctement, identifi√©s dans [docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md](docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md).

**Sympt√¥me utilisateur** :
> "Quand je demande aux agents de quoi nous avons parl√© jusqu'√† maintenant, les conversations archiv√©es ne sont jamais √©voqu√©es et les concepts associ√©s ne ressortent pas."

**Impact business** : La m√©moire P1 (extraction pr√©f√©rences) est d√©ploy√©e mais **NON UTILISABLE** car les threads archiv√©s ne sont jamais consolid√©s dans ChromaDB.

---

## üìö Lecture OBLIGATOIRE avant de commencer

**Ordre de lecture** (‚è±Ô∏è 15-20 minutes) :

1. **[AGENT_SYNC.md](AGENT_SYNC.md)** (sections cl√©s)
   - √âtat actuel du d√©p√¥t (branche, commits r√©cents, d√©ploiements)
   - Zones de travail en cours (sessions Claude Code + Codex r√©centes)
   - Checklist de travail obligatoire

2. **[docs/passation.md](docs/passation.md)** (3 derni√®res entr√©es)
   - [2025-10-10 16:45] Optimisations Performance Frontend
   - [2025-10-10 14:30] Hotfix P1.3 - user_sub Context
   - [2025-10-10 03:20] D√©ploiement P1+P0 production

3. **[docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md](docs/architecture/MEMORY_LTM_GAPS_ANALYSIS.md)** ‚≠ê
   - **DOCUMENT CL√â** : Analyse d√©taill√©e des 3 gaps critiques
   - Preuves code, impact utilisateur, m√©triques manquantes
   - Solutions propos√©es pour chaque gap

4. **[docs/memory-roadmap.md](docs/memory-roadmap.md)**
   - Vue d'ensemble architecture m√©moire (STM, LTM, vitalit√©)
   - √âtat Phase P0 (persistance cross-device) ‚úÖ COMPL√âT√â
   - √âtat Phase P1 (d√©portation async + pr√©f√©rences) ‚úÖ COMPL√âT√â
   - Phase P2 √† venir (r√©activit√© proactive)

5. **`git status` + `git log --oneline -10`**
   - V√©rifier √©tat working tree
   - Derniers commits (c550fac perf(frontend), 2523713 fix(P1.3), etc.)

---

## üî¥ Gap #1 : Threads archiv√©s JAMAIS consolid√©s dans LTM

### Probl√®me

**Workflow actuel** :
```
1. User archive conversation
   ‚îî‚îÄ> UPDATE threads SET archived = 1
2. Consolidation m√©moire (tend-garden)
   ‚îî‚îÄ> queries.get_threads(include_archived=False)  ‚Üê PAR D√âFAUT !
   ‚îî‚îÄ> R√©cup√®re uniquement threads actifs
3. Extraction concepts
   ‚îî‚îÄ> Analyse uniquement conversations actives
   ‚îî‚îÄ> Threads archiv√©s IGNOR√âS
4. ChromaDB (LTM)
   ‚îî‚îÄ> Ne contient JAMAIS les concepts des threads archiv√©s
```

### Solution √† impl√©menter

#### 1. Nouvel endpoint `/api/memory/consolidate-archived` (PRIORIT√â 1)

**Fichier** : `src/backend/features/memory/router.py`

```python
@router.post("/consolidate-archived")
async def consolidate_archived_threads(
    background_tasks: BackgroundTasks,
    user_id: Optional[str] = Header(None, alias="x-user-id"),
    batch_size: int = Query(10, ge=1, le=50),
    force: bool = Query(False),
    gardener: MemoryGardener = Depends(get_memory_gardener),
) -> Dict[str, Any]:
    """
    Consolide tous les threads archiv√©s dans ChromaDB.

    - batch_size : nombre de threads trait√©s par batch (d√©faut 10)
    - force : si True, re-consolide m√™me si d√©j√† fait (d√©faut False)
    - Retourne : {total_archived, consolidated, skipped, errors, duration_ms}
    """
    start_time = time.time()

    # R√©cup√©rer tous threads archiv√©s non consolid√©s
    threads = await queries.get_threads(
        db=gardener.db,
        session_id=None,  # Tous utilisateurs ou filtrer par user_id
        user_id=user_id,
        archived_only=True,  # Nouveau param√®tre √† ajouter dans queries.py
        limit=batch_size
    )

    total_archived = len(threads)
    consolidated = 0
    skipped = 0
    errors = []

    for thread in threads:
        try:
            # V√©rifier si d√©j√† consolid√© (metadata archived_consolidated_at)
            if not force and thread.get("metadata", {}).get("archived_consolidated_at"):
                skipped += 1
                continue

            # Consolider via tend_single_thread
            result = await gardener._tend_single_thread(
                thread_id=thread["id"],
                user_id=thread.get("user_id"),
                session_id=thread.get("session_id")
            )

            # Marquer comme consolid√© dans metadata
            if result.get("status") == "completed":
                await queries.update_thread_metadata(
                    db=gardener.db,
                    thread_id=thread["id"],
                    metadata={
                        **thread.get("metadata", {}),
                        "archived_consolidated_at": datetime.now(UTC).isoformat()
                    }
                )
                consolidated += 1
            else:
                errors.append({
                    "thread_id": thread["id"],
                    "error": result.get("error", "Unknown error")
                })
        except Exception as e:
            logger.error(f"Error consolidating archived thread {thread['id']}: {e}")
            errors.append({
                "thread_id": thread["id"],
                "error": str(e)
            })

    duration_ms = int((time.time() - start_time) * 1000)

    # M√©triques Prometheus
    MEMORY_ARCHIVED_CONSOLIDATED.inc(consolidated)

    return {
        "status": "completed",
        "total_archived": total_archived,
        "consolidated": consolidated,
        "skipped": skipped,
        "errors": errors,
        "duration_ms": duration_ms
    }
```

#### 2. Modifier `queries.get_threads()` (PRIORIT√â 1)

**Fichier** : `src/backend/core/database/queries.py`

Ajouter param√®tre `archived_only`:

```python
async def get_threads(
    db: DatabaseManager,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    type_: Optional[str] = None,
    include_archived: bool = False,
    archived_only: bool = False,  # ‚Üê NOUVEAU PARAM√àTRE
    limit: int = 20,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """
    R√©cup√®re les threads selon filtres.

    - archived_only : si True, ne retourne QUE les threads archiv√©s (archived = 1)
    - include_archived : si True (et archived_only False), inclut archiv√©s et actifs
    - Par d√©faut : uniquement threads actifs (archived = 0)
    """
    clauses = []
    params = []

    # ... autres filtres (session_id, user_id, type_) ...

    # Filtre archivage
    if archived_only:
        clauses.append("archived = 1")
    elif not include_archived:
        clauses.append("archived = 0")

    # ... reste de la fonction ...
```

#### 3. Endpoint trigger auto apr√®s archivage (PRIORIT√â 2)

**Fichier** : `src/backend/features/threads/router.py`

Dans l'endpoint `PUT /api/threads/{thread_id}` (archivage) :

```python
@router.put("/{thread_id}")
async def update_thread(
    thread_id: str,
    update: ThreadUpdate,
    background_tasks: BackgroundTasks,
    db: DatabaseManager = Depends(get_db),
    gardener: MemoryGardener = Depends(get_memory_gardener),
    user_id: str = Depends(get_current_user_id),
):
    # ... validation et update thread ...

    # Si archivage d√©tect√© ‚Üí trigger consolidation imm√©diate
    if update.archived and not existing_thread.get("archived"):
        logger.info(f"Thread {thread_id} archived, scheduling consolidation")

        # Background task asynchrone (ne bloque pas la r√©ponse)
        background_tasks.add_task(
            gardener._tend_single_thread,
            thread_id=thread_id,
            user_id=user_id,
            session_id=existing_thread.get("session_id")
        )

        # M√©trique Prometheus
        MEMORY_ARCHIVED_AUTO_TRIGGERED.inc()

    return updated_thread
```

#### 4. Tests unitaires (PRIORIT√â 1)

**Fichier** : `tests/backend/features/test_memory_archived_consolidation.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_consolidate_archived_threads_success():
    """Test consolidation batch threads archiv√©s."""
    # Setup mock threads archiv√©s
    mock_threads = [
        {"id": "thread1", "archived": 1, "metadata": {}},
        {"id": "thread2", "archived": 1, "metadata": {}},
    ]

    with patch("src.backend.core.database.queries.get_threads", return_value=mock_threads):
        with patch.object(gardener, "_tend_single_thread", return_value={"status": "completed"}):
            result = await consolidate_archived_threads(
                background_tasks=BackgroundTasks(),
                batch_size=10,
                force=False,
                gardener=gardener
            )

    assert result["status"] == "completed"
    assert result["consolidated"] == 2
    assert result["skipped"] == 0
    assert len(result["errors"]) == 0

@pytest.mark.asyncio
async def test_consolidate_archived_threads_skip_already_done():
    """Test skip threads d√©j√† consolid√©s."""
    mock_threads = [
        {"id": "thread1", "archived": 1, "metadata": {"archived_consolidated_at": "2025-10-10T12:00:00Z"}},
    ]

    with patch("src.backend.core.database.queries.get_threads", return_value=mock_threads):
        result = await consolidate_archived_threads(
            background_tasks=BackgroundTasks(),
            batch_size=10,
            force=False,
            gardener=gardener
        )

    assert result["consolidated"] == 0
    assert result["skipped"] == 1

@pytest.mark.asyncio
async def test_auto_trigger_on_archive():
    """Test trigger automatique lors archivage thread."""
    # ... test que background task est bien schedul√©e ...
```

#### 5. M√©triques Prometheus (PRIORIT√â 2)

**Fichier** : `src/backend/features/memory/gardener.py`

```python
from prometheus_client import Counter

MEMORY_ARCHIVED_CONSOLIDATED = Counter(
    "memory_archived_consolidated_total",
    "Nombre total de threads archiv√©s consolid√©s dans LTM"
)

MEMORY_ARCHIVED_AUTO_TRIGGERED = Counter(
    "memory_archived_auto_triggered_total",
    "Nombre de consolidations auto-d√©clench√©es lors archivage"
)

MEMORY_ARCHIVED_CONSOLIDATION_ERRORS = Counter(
    "memory_archived_consolidation_errors_total",
    "Erreurs lors consolidation threads archiv√©s",
    labelnames=["error_type"]
)
```

---

## üü° Gap #2 : Pr√©f√©rences extraites JAMAIS sauv√©es dans ChromaDB

### Probl√®me

**Workflow actuel** :
```
1. PreferenceExtractor.extract()
   ‚îî‚îÄ> Extraction pr√©f√©rences depuis messages ‚úÖ
   ‚îî‚îÄ> Retourne liste PreferenceRecord ‚úÖ

2. MemoryAnalyzer.analyze_session()
   ‚îî‚îÄ> Appelle preference_extractor.extract() ‚úÖ
   ‚îî‚îÄ> Stocke r√©sultats dans session.metadata["preferences"] ‚úÖ
   ‚îî‚îÄ> NE SAUVEGARDE PAS dans ChromaDB ‚ùå

3. ChromaDB collection 'memory_preferences'
   ‚îî‚îÄ> Reste VIDE (0 embeddings)
```

### Solution √† impl√©menter

#### 1. M√©thode `save_preferences_to_vector_db()` (PRIORIT√â 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

Ajouter apr√®s `analyze_session()` :

```python
async def save_preferences_to_vector_db(
    self,
    preferences: List[Dict[str, Any]],
    user_id: str,
    thread_id: str,
    session_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Sauvegarde pr√©f√©rences extraites dans ChromaDB.

    Args:
        preferences : liste pr√©f√©rences depuis PreferenceExtractor
        user_id : identifiant utilisateur
        thread_id : identifiant thread
        session_id : identifiant session (optionnel)

    Returns:
        {saved: int, skipped: int, errors: List[str]}
    """
    if not preferences:
        logger.info("No preferences to save")
        return {"saved": 0, "skipped": 0, "errors": []}

    if not self.vector_service:
        logger.warning("VectorService not available, skipping preference save")
        return {"saved": 0, "skipped": len(preferences), "errors": ["VectorService unavailable"]}

    saved = 0
    skipped = 0
    errors = []

    for pref in preferences:
        try:
            # G√©n√©rer embedding du texte pr√©f√©rence
            text = pref.get("topic", "") + " " + pref.get("action", "")
            embedding = await self._generate_embedding(text)

            if not embedding:
                skipped += 1
                continue

            # ID unique : {user_id}_{topic}_{type}
            pref_id = f"{user_id}_{pref['topic']}_{pref['type']}"

            # Metadata compl√®te
            metadata = {
                "user_id": user_id,
                "thread_id": thread_id,
                "session_id": session_id or "unknown",
                "type": pref["type"],  # preference | intent | constraint
                "topic": pref["topic"],
                "action": pref.get("action", ""),
                "timeframe": pref.get("timeframe", "ongoing"),
                "sentiment": pref.get("sentiment", "neutral"),
                "confidence": pref.get("confidence", 0.5),
                "source_message_id": pref.get("source_message_id", ""),
                "captured_at": datetime.now(UTC).isoformat(),
                "entities": json.dumps(pref.get("entities", [])),
            }

            # Sauvegarder dans collection 'memory_preferences'
            await self.vector_service.add(
                collection_name="memory_preferences",
                ids=[pref_id],
                embeddings=[embedding],
                metadatas=[metadata],
                documents=[text]
            )

            saved += 1
            logger.info(f"Saved preference {pref_id} to ChromaDB")

            # M√©trique Prometheus
            MEMORY_PREFERENCES_SAVED.labels(type=pref["type"]).inc()

        except Exception as e:
            logger.error(f"Error saving preference to ChromaDB: {e}")
            errors.append(str(e))
            MEMORY_PREFERENCES_SAVE_ERRORS.inc()

    return {
        "saved": saved,
        "skipped": skipped,
        "errors": errors
    }
```

#### 2. Appeler `save_preferences_to_vector_db()` dans `analyze_session()` (PRIORIT√â 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

Dans la m√©thode `analyze_session()`, apr√®s extraction pr√©f√©rences :

```python
async def analyze_session(
    self,
    session: Session,
    user_sub: Optional[str] = None,
    user_id: Optional[str] = None,
    persist: bool = False,
) -> Dict[str, Any]:
    # ... code existant (extraction concepts, pr√©f√©rences, etc.) ...

    # Nouvelle section : Sauvegarde pr√©f√©rences dans ChromaDB
    if persist and self.preference_extractor and preferences:
        logger.info(f"Saving {len(preferences)} preferences to ChromaDB")

        pref_save_result = await self.save_preferences_to_vector_db(
            preferences=preferences,
            user_id=user_identifier,  # user_sub or user_id
            thread_id=session.thread_id or "unknown",
            session_id=session.session_id
        )

        result["preferences_saved"] = pref_save_result["saved"]
        result["preferences_skipped"] = pref_save_result["skipped"]
        result["preferences_errors"] = pref_save_result["errors"]

        logger.info(
            f"Preferences saved: {pref_save_result['saved']}, "
            f"skipped: {pref_save_result['skipped']}, "
            f"errors: {len(pref_save_result['errors'])}"
        )

    return result
```

#### 3. M√©thode helper `_generate_embedding()` (PRIORIT√â 1)

**Fichier** : `src/backend/features/memory/analyzer.py`

```python
async def _generate_embedding(self, text: str) -> Optional[List[float]]:
    """G√©n√®re embedding pour texte via ChatService."""
    if not self.chat_service:
        logger.warning("ChatService not available for embeddings")
        return None

    try:
        # Utiliser text-embedding-3-large (ou fallback)
        response = await self.chat_service.generate_embedding(
            text=text,
            model="text-embedding-3-large"
        )
        return response.get("embedding")
    except Exception as e:
        logger.error(f"Error generating embedding: {e}")
        return None
```

#### 4. Tests unitaires (PRIORIT√â 1)

**Fichier** : `tests/backend/features/test_preference_persistence.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_save_preferences_to_vector_db_success():
    """Test sauvegarde pr√©f√©rences dans ChromaDB."""
    preferences = [
        {
            "type": "preference",
            "topic": "python",
            "action": "pr√©f√©rer",
            "confidence": 0.85,
            "source_message_id": "msg123"
        }
    ]

    with patch.object(analyzer, "_generate_embedding", return_value=[0.1] * 1536):
        with patch.object(analyzer.vector_service, "add", return_value=None):
            result = await analyzer.save_preferences_to_vector_db(
                preferences=preferences,
                user_id="user123",
                thread_id="thread456",
                session_id="session789"
            )

    assert result["saved"] == 1
    assert result["skipped"] == 0
    assert len(result["errors"]) == 0

@pytest.mark.asyncio
async def test_save_preferences_vector_service_unavailable():
    """Test graceful degradation si VectorService indisponible."""
    analyzer.vector_service = None

    result = await analyzer.save_preferences_to_vector_db(
        preferences=[{"type": "preference", "topic": "test"}],
        user_id="user123",
        thread_id="thread456"
    )

    assert result["saved"] == 0
    assert result["skipped"] == 1
    assert "VectorService unavailable" in result["errors"]
```

#### 5. M√©triques Prometheus (PRIORIT√â 2)

**Fichier** : `src/backend/features/memory/analyzer.py`

```python
MEMORY_PREFERENCES_SAVED = Counter(
    "memory_preferences_saved_total",
    "Nombre de pr√©f√©rences sauvegard√©es dans ChromaDB",
    labelnames=["type"]  # preference | intent | constraint
)

MEMORY_PREFERENCES_SAVE_ERRORS = Counter(
    "memory_preferences_save_errors_total",
    "Erreurs lors sauvegarde pr√©f√©rences dans ChromaDB"
)
```

---

## üü¢ Gap #3 : Aucune recherche pr√©f√©rences lors rappel LTM

### Probl√®me

**Workflow actuel** :
```
1. Agent g√©n√®re r√©ponse
   ‚îî‚îÄ> context_builder.build_context()
   ‚îî‚îÄ> Recherche dans collection 'emergence_knowledge' (concepts) ‚úÖ
   ‚îî‚îÄ> NE recherche PAS dans 'memory_preferences' ‚ùå

2. Context final envoy√© au LLM
   ‚îî‚îÄ> Contient concepts g√©n√©raux ‚úÖ
   ‚îî‚îÄ> NE contient PAS pr√©f√©rences utilisateur ‚ùå
```

### Solution √† impl√©menter

#### 1. M√©thode `search_preferences()` (PRIORIT√â 1)

**Fichier** : `src/backend/features/memory/vector_service.py`

```python
async def search_preferences(
    self,
    query: str,
    user_id: str,
    limit: int = 5,
    min_confidence: float = 0.6
) -> List[Dict[str, Any]]:
    """
    Recherche pr√©f√©rences utilisateur dans ChromaDB.

    Args:
        query : texte requ√™te (embedded)
        user_id : filtre par utilisateur
        limit : nombre max r√©sultats
        min_confidence : seuil confidence minimum

    Returns:
        Liste pr√©f√©rences {type, topic, action, confidence, ...}
    """
    try:
        # G√©n√©rer embedding requ√™te
        query_embedding = await self._generate_embedding(query)

        if not query_embedding:
            logger.warning("Could not generate embedding for preference search")
            return []

        # Recherche dans collection 'memory_preferences'
        results = await self.collection_preferences.query(
            query_embeddings=[query_embedding],
            n_results=limit,
            where={"user_id": user_id}  # Filtre par utilisateur
        )

        # Parser r√©sultats
        preferences = []
        if results and results.get("metadatas"):
            for i, metadata in enumerate(results["metadatas"][0]):
                confidence = metadata.get("confidence", 0.5)

                # Filtre par confidence minimum
                if confidence >= min_confidence:
                    preferences.append({
                        "type": metadata.get("type"),
                        "topic": metadata.get("topic"),
                        "action": metadata.get("action"),
                        "confidence": confidence,
                        "sentiment": metadata.get("sentiment"),
                        "captured_at": metadata.get("captured_at"),
                        "distance": results["distances"][0][i] if results.get("distances") else 0.0
                    })

        logger.info(f"Found {len(preferences)} preferences for user {user_id}")
        return preferences

    except Exception as e:
        logger.error(f"Error searching preferences: {e}")
        return []
```

#### 2. Int√©grer dans `ContextBuilder` (PRIORIT√â 1)

**Fichier** : `src/backend/features/chat/context_builder.py`

Dans la m√©thode `build_context()` :

```python
async def build_context(
    self,
    user_message: str,
    session: Session,
    user_id: Optional[str] = None,
    include_preferences: bool = True,  # ‚Üê NOUVEAU PARAM√àTRE
) -> str:
    """
    Construit contexte complet pour agent.

    - STM : r√©sum√© conversation courante
    - LTM concepts : recherche vectorielle concepts g√©n√©raux
    - LTM pr√©f√©rences : recherche pr√©f√©rences utilisateur (NOUVEAU)
    """
    context_parts = []

    # 1. STM (court terme)
    stm_summary = self._build_stm_summary(session)
    if stm_summary:
        context_parts.append(f"## Contexte court terme\n{stm_summary}")

    # 2. LTM Concepts (concepts g√©n√©raux)
    concepts = await self.vector_service.search_concepts(
        query=user_message,
        limit=5
    )
    if concepts:
        concepts_text = "\n".join([f"- {c['text']}" for c in concepts])
        context_parts.append(f"## Concepts pertinents\n{concepts_text}")

    # 3. LTM Pr√©f√©rences (NOUVEAU)
    if include_preferences and user_id:
        preferences = await self.vector_service.search_preferences(
            query=user_message,
            user_id=user_id,
            limit=5,
            min_confidence=0.6
        )

        if preferences:
            prefs_text = "\n".join([
                f"- {p['type'].upper()}: {p['topic']} ({p['action']}) "
                f"[confiance: {p['confidence']:.2f}]"
                for p in preferences
            ])
            context_parts.append(f"## Pr√©f√©rences utilisateur\n{prefs_text}")

    return "\n\n".join(context_parts)
```

#### 3. Tests unitaires (PRIORIT√â 1)

**Fichier** : `tests/backend/features/test_preference_retrieval.py` (nouveau)

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_search_preferences_success():
    """Test recherche pr√©f√©rences ChromaDB."""
    mock_results = {
        "metadatas": [[
            {
                "type": "preference",
                "topic": "python",
                "action": "pr√©f√©rer",
                "confidence": 0.85,
                "sentiment": "positive",
                "captured_at": "2025-10-10T12:00:00Z"
            }
        ]],
        "distances": [[0.15]]
    }

    with patch.object(vector_service.collection_preferences, "query", return_value=mock_results):
        preferences = await vector_service.search_preferences(
            query="python development",
            user_id="user123",
            limit=5,
            min_confidence=0.6
        )

    assert len(preferences) == 1
    assert preferences[0]["type"] == "preference"
    assert preferences[0]["topic"] == "python"
    assert preferences[0]["confidence"] == 0.85

@pytest.mark.asyncio
async def test_context_builder_includes_preferences():
    """Test int√©gration pr√©f√©rences dans contexte."""
    mock_preferences = [
        {"type": "preference", "topic": "python", "action": "pr√©f√©rer", "confidence": 0.85}
    ]

    with patch.object(context_builder.vector_service, "search_preferences", return_value=mock_preferences):
        context = await context_builder.build_context(
            user_message="Comment coder en Python ?",
            session=mock_session,
            user_id="user123",
            include_preferences=True
        )

    assert "Pr√©f√©rences utilisateur" in context
    assert "python" in context.lower()
```

---

## üìã Checklist de travail

### Avant de commencer

- [ ] Lecture AGENT_SYNC.md (√©tat d√©p√¥t, commits r√©cents, d√©ploiements)
- [ ] Lecture docs/passation.md (3 derni√®res entr√©es)
- [ ] Lecture MEMORY_LTM_GAPS_ANALYSIS.md ‚≠ê (document cl√©)
- [ ] Lecture docs/memory-roadmap.md (contexte architecture)
- [ ] `git fetch --all --prune` (sync avec remote)
- [ ] `git status` (v√©rifier working tree propre)
- [ ] `git log --oneline -10` (derniers commits)

### Pendant le d√©veloppement

#### Gap #1 : Threads archiv√©s

- [ ] Cr√©er endpoint `POST /api/memory/consolidate-archived` (router.py)
- [ ] Modifier `queries.get_threads()` (ajouter `archived_only` param)
- [ ] Ajouter trigger auto dans `PUT /api/threads/{id}` (archivage)
- [ ] Cr√©er tests `test_memory_archived_consolidation.py` (4-5 tests)
- [ ] Ajouter m√©triques Prometheus (MEMORY_ARCHIVED_*)
- [ ] Tests locaux : `pytest tests/backend/features/test_memory_archived_consolidation.py -v`

#### Gap #2 : Pr√©f√©rences ChromaDB

- [ ] Cr√©er `save_preferences_to_vector_db()` dans analyzer.py
- [ ] Cr√©er `_generate_embedding()` helper dans analyzer.py
- [ ] Appeler `save_preferences_to_vector_db()` dans `analyze_session()`
- [ ] Cr√©er tests `test_preference_persistence.py` (3-4 tests)
- [ ] Ajouter m√©triques Prometheus (MEMORY_PREFERENCES_SAVED)
- [ ] Tests locaux : `pytest tests/backend/features/test_preference_persistence.py -v`

#### Gap #3 : Recherche pr√©f√©rences LTM

- [ ] Cr√©er `search_preferences()` dans vector_service.py
- [ ] Int√©grer dans `ContextBuilder.build_context()` (context_builder.py)
- [ ] Cr√©er tests `test_preference_retrieval.py` (2-3 tests)
- [ ] Tests locaux : `pytest tests/backend/features/test_preference_retrieval.py -v`

### Tests & Qualit√©

- [ ] `pytest tests/backend/features/test_memory_*.py -v` (tous tests m√©moire)
- [ ] `pytest` (suite compl√®te backend)
- [ ] `ruff check src/backend` (linting)
- [ ] `mypy src/backend --ignore-missing-imports` (types)
- [ ] `npm run build` (frontend, v√©rifier 0 r√©gression)

### Documentation

- [ ] Mise √† jour `AGENT_SYNC.md` (section "Zones de travail en cours")
- [ ] Nouvelle entr√©e `docs/passation.md` (contexte, actions, tests, r√©sultats)
- [ ] Mise √† jour `docs/memory-roadmap.md` (marquer P0 gaps r√©solus)
- [ ] Cr√©er `docs/deployments/2025-10-10-p0-gaps-resolution.md` (rapport complet)

### Commit & Push

- [ ] `git add` (fichiers modifi√©s + nouveaux tests + docs)
- [ ] `git commit -m "feat(P0): r√©solution 3 gaps critiques m√©moire LTM"` (message d√©taill√©)
- [ ] `git push origin main`
- [ ] V√©rifier push GitHub OK

---

## üéØ R√©sultat attendu

### Fonctionnalit√©s op√©rationnelles

1. **Endpoint `/api/memory/consolidate-archived`**
   - Batch consolidation threads archiv√©s
   - Skip threads d√©j√† consolid√©s (sauf force=True)
   - M√©triques expos√©es

2. **Trigger auto lors archivage**
   - `PUT /api/threads/{id}` avec `archived=True`
   - Background task consolidation imm√©diate
   - M√©trique auto-trigger incr√©ment√©e

3. **Pr√©f√©rences sauv√©es ChromaDB**
   - Collection `memory_preferences` popul√©e
   - D√©duplication par `(user_id, topic, type)`
   - M√©triques saved/errors expos√©es

4. **Recherche pr√©f√©rences LTM**
   - `ContextBuilder.build_context()` inclut pr√©f√©rences
   - Filtrage par user_id + confidence >= 0.6
   - Format lisible dans contexte agent

### Tests

- ‚úÖ Tous tests m√©moire passent (>15 tests)
- ‚úÖ 0 r√©gression tests existants
- ‚úÖ Ruff + mypy clean
- ‚úÖ Frontend build OK

### M√©triques nouvelles expos√©es

```
# Gap #1 - Threads archiv√©s
memory_archived_consolidated_total
memory_archived_auto_triggered_total
memory_archived_consolidation_errors_total{error_type}

# Gap #2 - Pr√©f√©rences ChromaDB
memory_preferences_saved_total{type}
memory_preferences_save_errors_total

# Existantes (P1)
memory_preferences_extracted_total{type}
memory_preferences_confidence (histogram)
memory_preferences_extraction_duration_seconds (histogram)
```

### Documentation

- `AGENT_SYNC.md` : section session P0 ajout√©e
- `docs/passation.md` : entr√©e compl√®te avec r√©sultats
- `docs/memory-roadmap.md` : P0 gaps marqu√©s r√©solved
- `docs/deployments/2025-10-10-p0-gaps-resolution.md` : rapport technique

---

## üöÄ Commande commit finale

```bash
git add \
  src/backend/features/memory/router.py \
  src/backend/features/memory/analyzer.py \
  src/backend/features/memory/vector_service.py \
  src/backend/features/threads/router.py \
  src/backend/features/chat/context_builder.py \
  src/backend/core/database/queries.py \
  tests/backend/features/test_memory_archived_consolidation.py \
  tests/backend/features/test_preference_persistence.py \
  tests/backend/features/test_preference_retrieval.py \
  docs/passation.md \
  docs/memory-roadmap.md \
  docs/deployments/2025-10-10-p0-gaps-resolution.md \
  AGENT_SYNC.md

git commit -m "feat(P0): r√©solution 3 gaps critiques m√©moire LTM

Gap #1 - Threads archiv√©s consolid√©s
- Endpoint POST /api/memory/consolidate-archived (batch processing)
- Trigger auto lors archivage (background task)
- Param archived_only dans queries.get_threads()
- Metadata archived_consolidated_at pour skip duplicatas
- M√©triques: memory_archived_consolidated_total, auto_triggered_total

Gap #2 - Pr√©f√©rences sauv√©es ChromaDB
- M√©thode save_preferences_to_vector_db() dans MemoryAnalyzer
- Helper _generate_embedding() pour embeddings
- Appel automatique dans analyze_session(persist=True)
- Collection memory_preferences popul√©e
- M√©triques: memory_preferences_saved_total{type}, save_errors_total

Gap #3 - Recherche pr√©f√©rences LTM
- M√©thode search_preferences() dans VectorService
- Int√©gration ContextBuilder.build_context()
- Filtrage user_id + confidence >= 0.6
- Section 'Pr√©f√©rences utilisateur' dans contexte agent

Tests: 15+ nouveaux tests (100% passants), 0 r√©gression
Docs: passation + roadmap + deployment report
Impact: LTM m√©moire maintenant FONCTIONNELLE (threads archiv√©s + pr√©f√©rences)"

git push origin main
```

---

## üìû Contact & Questions

**Si blocage technique** :
1. Consulter `AGENT_SYNC.md` (section "Conflits & R√©solution")
2. Documenter dans `docs/passation.md` (section "Blocages")
3. Continuer sur t√¢ches non bloquantes
4. Ping FG (architecte) pour arbitrage

**Principe cl√©** : Tests > Documentation > Communication

---

**Ce prompt est exhaustif et autonome.** La prochaine instance peut commencer imm√©diatement apr√®s lecture des documents obligatoires.

**Bonne session ! üöÄ**
