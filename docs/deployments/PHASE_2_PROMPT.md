# üöÄ PHASE 2 : Optimisation Performance M√©moire & Agents

## üìã CONTEXTE

Suite √† la **Phase 1** (termin√©e ‚úÖ), le syst√®me de m√©moire temporelle est maintenant fonctionnel :
- ‚úÖ Fix timeout Gemini + triple fallback (Neo ‚Üí Nexus ‚Üí Anima)
- ‚úÖ Injection horodatages dans RAG (ex: "Docker (1√®re mention: 5 oct, 3 fois)")
- ‚úÖ D√©tection proactive connexions conceptuelles

**Probl√®me restant** : L'agent **N√©o (Gemini)** est **notablement plus lent** que les autres agents (Nexus/Anima), impactant l'UX des d√©bats et analyses.

---

## üéØ OBJECTIFS PHASE 2

Optimiser les performances pour r√©duire la latence des agents et des analyses m√©moire.

### Gains attendus :
- **-50% latence d√©bats** (6s ‚Üí 2-3s pour 3 agents)
- **-40% latence analyses** (gr√¢ce √† mod√®le d√©di√©)
- **-60% co√ªt API** (cache r√©sum√©s identiques)

---

## üìù T√ÇCHES √Ä R√âALISER

### ‚úÖ **T√¢che 1 : Mod√®le D√©di√© pour Analyses M√©moire**
**Dur√©e estim√©e** : 2h | **Priorit√©** : P1

#### Probl√®me
- Les analyses m√©moire (r√©sum√©s/concepts) utilisent Neo (Gemini) par d√©faut
- Gemini est lent pour g√©n√©ration JSON structur√© (~4-6s)
- OpenAI GPT-4o-mini est **3x plus rapide** pour ce use case (~1-2s)

#### Solution
1. Cr√©er un agent virtuel `neo_analysis` dans la config utilisant GPT-4o-mini
2. Modifier `analyzer.py` pour utiliser `neo_analysis` au lieu de `neo`
3. Conserver les fallbacks Nexus/Anima

#### Fichiers √† modifier :

**`src/backend/shared/config.py`** (ligne ~40)
```python
DEFAULT_AGENT_CONFIGS: Dict[str, Dict[str, str]] = {
    "default": {"provider": "google", "model": DEFAULT_GOOGLE_MODEL},
    "neo": {"provider": "google", "model": DEFAULT_GOOGLE_MODEL},
    "neo_analysis": {"provider": "openai", "model": "gpt-4o-mini"},  # ‚ö° NOUVEAU
    "nexus": {"provider": "anthropic", "model": "claude-3-5-haiku-20241022"},
    "anima": {"provider": "openai", "model": "gpt-4o-mini"},
}
```

**`src/backend/features/memory/analyzer.py`** (ligne ~159)
```python
# Tentative primaire : neo_analysis (GPT-4o-mini - rapide pour JSON)
try:
    analysis_result = await chat_service.get_structured_llm_response(
        agent_id="neo_analysis",  # ‚ö° CHANG√â (√©tait "neo")
        prompt=prompt,
        json_schema=ANALYSIS_JSON_SCHEMA
    )
    logger.info(f"[MemoryAnalyzer] Analyse r√©ussie avec neo_analysis pour session {session_id}")
except Exception as e:
    primary_error = e
    error_type = type(e).__name__
    logger.warning(
        f"[MemoryAnalyzer] Analyse neo_analysis √©chou√©e ({error_type}) ‚Äî fallback Nexus",
        exc_info=True,
    )
```

#### Tests
```bash
# 1. V√©rifier config charg√©e
python -c "from backend.shared.config import DEFAULT_AGENT_CONFIGS; print(DEFAULT_AGENT_CONFIGS.get('neo_analysis'))"

# 2. Tester analyse m√©moire (observer logs provider utilis√©)
# POST /api/memory/analyze avec session_id
# Logs attendus: "[MemoryAnalyzer] Analyse r√©ussie avec neo_analysis"
```

---

### ‚úÖ **T√¢che 2 : Parall√©lisation Appels Agents dans D√©bats**
**Dur√©e estim√©e** : 3h | **Priorit√©** : P1

#### Probl√®me
- Les d√©bats appellent les 3 agents **s√©quentiellement** (Neo ‚Üí Nexus ‚Üí Anima)
- Latence totale = somme latences (3s + 2s + 2s = **7s**)
- Possibilit√© d'ex√©cuter en **parall√®le** avec `asyncio.gather()`

#### Solution
Modifier le service de d√©bat pour lancer les appels LLM en parall√®le.

#### Fichier √† identifier et modifier :

**√âtape 1** : Localiser le service de d√©bat
```bash
# Chercher le fichier g√©rant les d√©bats
grep -r "debate" src/backend/features/ --include="*.py" | grep -i "service\|router"
```

**√âtape 2** : Identifier la logique d'appel s√©quentiel
Chercher un pattern comme :
```python
# ‚ùå ACTUEL (s√©quentiel)
response_neo = await chat_service.call_agent("neo", ...)
response_nexus = await chat_service.call_agent("nexus", ...)
response_anima = await chat_service.call_agent("anima", ...)
```

**√âtape 3** : Remplacer par appels parall√®les
```python
# ‚úÖ OPTIMIS√â (parall√®le)
import asyncio

responses = await asyncio.gather(
    chat_service.call_agent("neo", ...),
    chat_service.call_agent("nexus", ...),
    chat_service.call_agent("anima", ...),
    return_exceptions=True  # Continue si un agent fail
)

response_neo, response_nexus, response_anima = responses

# G√©rer erreurs individuelles
for i, resp in enumerate(responses):
    if isinstance(resp, Exception):
        logger.warning(f"Agent {['neo', 'nexus', 'anima'][i]} √©chou√©: {resp}")
```

#### Tests
```bash
# 1. Lancer d√©bat 3 agents
# POST /api/chat/debate ou √©quivalent

# 2. Mesurer latence totale (logs ou m√©triques)
# Avant: ~7s | Apr√®s: ~3s (latence max agent le plus lent)
```

---

### ‚úÖ **T√¢che 3 : Cache Redis pour R√©sum√©s Sessions**
**Dur√©e estim√©e** : 3h | **Priorit√©** : P2

#### Probl√®me
- Les analyses m√©moire (r√©sum√©s/concepts) sont **recalcul√©es** √† chaque fois
- Si user demande 2x le m√™me r√©sum√© ‚Üí 2 appels LLM inutiles
- Co√ªt API et latence √©vitables

#### Solution
Impl√©menter un cache Redis simple pour les r√©sum√©s de sessions.

#### Pr√©requis
V√©rifier si Redis est d√©j√† configur√© :
```bash
# Chercher configuration Redis existante
grep -r "redis\|Redis\|REDIS" src/backend/ --include="*.py" | head -20

# V√©rifier .env
cat .env | grep -i redis
```

#### Impl√©mentation

**Option A : Redis d√©j√† disponible**

**`src/backend/features/memory/analyzer.py`** (dans m√©thode `_analyze`, ligne ~150)
```python
import json
import hashlib

async def _analyze(self, session_id: str, history: List[Dict[str, Any]], ...):
    # ... code existant ...

    # üÜï CACHE LAYER
    if persist and not force:
        # G√©n√©rer cl√© cache bas√©e sur hash historique
        history_text = "\n".join([f"{m.get('role')}:{m.get('content')}" for m in history])
        cache_key = f"memory_analysis:{session_id}:{hashlib.md5(history_text.encode()).hexdigest()[:8]}"

        # Check cache
        try:
            import redis.asyncio as aioredis
            redis_client = await aioredis.from_url(
                os.getenv("REDIS_URL", "redis://localhost:6379"),
                encoding="utf-8",
                decode_responses=True
            )
            cached = await redis_client.get(cache_key)
            if cached:
                logger.info(f"[MemoryAnalyzer] Cache HIT pour session {session_id}")
                return json.loads(cached)
        except Exception as cache_err:
            logger.debug(f"Cache unavailable: {cache_err}")

    # ... code analyse existant ...

    # üÜï SAVE TO CACHE (apr√®s analyse r√©ussie)
    if analysis_result and persist:
        try:
            await redis_client.setex(
                cache_key,
                3600,  # TTL 1h
                json.dumps(analysis_result)
            )
            logger.info(f"[MemoryAnalyzer] Cache SAVED pour session {session_id}")
        except Exception:
            pass

    return analysis_result
```

**Option B : Redis non disponible ‚Üí Skip cette t√¢che**

Si Redis n'est pas configur√© et que l'ajout est complexe, **reporter cette t√¢che √† Phase 3** et se concentrer sur les 2 premi√®res.

#### Tests
```bash
# 1. Analyser une session 2 fois
# POST /api/memory/analyze {"session_id": "xxx"}
# POST /api/memory/analyze {"session_id": "xxx"}  # Devrait √™tre instantan√©

# 2. V√©rifier logs
# Attendu: "[MemoryAnalyzer] Cache HIT pour session xxx"
```

---

## üìä M√âTRIQUES DE SUCC√àS

| M√©trique | Avant | Cible Apr√®s | Validation |
|----------|-------|-------------|------------|
| **Latence analyse m√©moire** | 4-6s | 1-2s | Logs `[MemoryAnalyzer]` |
| **Latence d√©bat 3 agents** | 6-7s | 2-3s | M√©triques endpoint d√©bat |
| **Cache hit rate** | 0% | 40%+ | Logs cache HIT/MISS |
| **Co√ªt API analyses** | 100% | 60% | Prometheus metrics |

---

## üß™ PLAN DE TEST COMPLET

### Test 1 : Analyse M√©moire Rapide
```bash
# 1. Cr√©er session avec historique
# 2. POST /api/memory/analyze {"session_id": "test_perf", "force": true}
# 3. Mesurer temps r√©ponse (target < 2s)
# 4. V√©rifier logs provider = "neo_analysis" (OpenAI)
```

### Test 2 : D√©bat Parall√®le
```bash
# 1. Lancer d√©bat avec question complexe
# 2. Mesurer temps total (target < 3s)
# 3. V√©rifier logs : appels agents doivent se chevaucher temporellement
```

### Test 3 : Cache Redis
```bash
# 1. Analyser session A ‚Üí MISS (calcul)
# 2. Analyser session A ‚Üí HIT (cache, < 100ms)
# 3. Analyser session B ‚Üí MISS (diff√©rent hash)
# 4. Wait 1h ‚Üí Analyser session A ‚Üí MISS (TTL expir√©)
```

---

## üîß ENVIRONNEMENT

### Variables ENV requises :
```bash
# D√©j√† configur√©es (Phase 1)
MEMORY_ANALYSIS_TIMEOUT=30

# Nouvelles (Phase 2)
REDIS_URL=redis://localhost:6379  # Si cache Redis impl√©ment√©
```

### D√©pendances Python (v√©rifier requirements.txt) :
```bash
# Si Redis utilis√©
redis>=5.0.0
```

---

## ‚ö†Ô∏è POINTS D'ATTENTION

1. **Mod√®le neo_analysis** : S'assurer que GPT-4o-mini supporte `response_format={"type": "json_object"}` (oui, v√©rifi√©)

2. **D√©bats parall√®les** : G√©rer correctement les exceptions individuelles avec `return_exceptions=True`

3. **Cache Redis** :
   - Si Redis non dispo, skip cette t√¢che sans bloquer
   - Hash MD5 court (8 chars) pour √©viter cl√©s trop longues
   - TTL 1h (ajustable selon usage)

4. **Logs** : Enrichir avec provider utilis√© pour monitoring post-d√©ploiement

---

## üì¶ LIVRABLE ATTENDU

√Ä la fin de cette phase, tu devrais avoir :

‚úÖ **Code modifi√©** :
- `config.py` : Agent `neo_analysis` ajout√©
- `analyzer.py` : Utilise `neo_analysis` + cache (optionnel)
- `debate/service.py` : Appels agents parall√®les

‚úÖ **Tests valid√©s** :
- Analyse < 2s (vs 4-6s avant)
- D√©bat < 3s (vs 6-7s avant)
- Cache fonctionne (si impl√©ment√©)

‚úÖ **Documentation** :
- Update `AGENT_SYNC.md` avec nouvelles perfs
- Cr√©er `docs/deployments/2025-10-08-phase2-perf.md` avec m√©triques

---

## üöÄ COMMANDES RAPIDES

```bash
# V√©rifier structure projet
ls -la src/backend/features/

# Chercher service d√©bat
grep -r "debate" src/backend/features/ --include="*.py"

# Tester analyse m√©moire
curl -X POST http://localhost:8000/api/memory/analyze \
  -H "Content-Type: application/json" \
  -d '{"session_id": "test_perf", "force": true}'

# V√©rifier logs temps r√©el
tail -f logs/backend.log | grep -i "memoryanalyzer\|debate"
```

---

## üìö RESSOURCES

### Fichiers cl√©s √† consulter :
- `src/backend/shared/config.py` (config agents)
- `src/backend/features/memory/analyzer.py` (analyses m√©moire)
- `src/backend/features/debate/service.py` (d√©bats - √† localiser)
- `src/backend/features/chat/service.py` (r√©f√©rence appels LLM)

### Documentation :
- [OpenAI JSON mode](https://platform.openai.com/docs/guides/structured-outputs)
- [asyncio.gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather)
- [Redis Python asyncio](https://redis.readthedocs.io/en/stable/examples/asyncio_examples.html)

---

**Bonne chance ! üöÄ**

Si tu bloques sur un point, concentre-toi sur les **T√¢ches 1 & 2** (les plus impactantes). Le cache Redis est optionnel si complexe √† impl√©menter.
