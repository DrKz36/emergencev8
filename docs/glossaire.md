# Glossaire IA - √âMERGENCE

> **Votre dictionnaire de l'intelligence artificielle**
> Ce glossaire explique les termes cl√©s utilis√©s dans √âMERGENCE et le monde de l'IA conversationnelle, avec des d√©finitions accessibles au grand public.

---

## Table des mati√®res

- [Agent Conversationnel](#agent-conversationnel)
- [LLM (Large Language Model)](#llm-large-language-model)
- [Prompt](#prompt)
- [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
- [M√©moire (STM / LTM)](#memoire-stm-ltm)
- [Vectorisation](#vectorisation)
- [√âmergence](#emergence)
- [Hallucination (IA)](#hallucination-ia)
- [Token](#token)
- [Temp√©rature](#temperature)
- [Contexte (fen√™tre de contexte)](#contexte-fenetre-de-contexte)
- [Embedding](#embedding)
- [Fine-tuning](#fine-tuning)
- [Prompt Engineering](#prompt-engineering)
- [Zero-shot / Few-shot Learning](#zero-shot-few-shot-learning)
- [Chunking](#chunking)
- [Similarit√© Cosinus](#similarite-cosinus)

---

## Agent Conversationnel

### D√©finition
Un **agent conversationnel** (ou chatbot IA) est un programme informatique bas√© sur un **LLM** (voir ci-dessous), con√ßu pour dialoguer en langage naturel et accomplir des t√¢ches sp√©cifiques.

### Dans √âMERGENCE
Anima, Neo et Nexus sont trois agents conversationnels avec des personnalit√©s et missions distinctes :
- **Anima** : empathique et clarificatrice
- **Neo** : analytique et structurante
- **Nexus** : coordonnatrice et organisatrice

### Analogie
Imaginez trois coll√®gues experts, chacun avec une sp√©cialit√©, qui collaborent pour vous aider sur un projet.

### Pourquoi c'est important
Les agents conversationnels d√©mocratisent l'acc√®s √† l'expertise : plus besoin d'√™tre analyste de donn√©es pour obtenir une synth√®se, ni √©crivain pour r√©diger une lettre.

---

## LLM (Large Language Model)

### D√©finition
Un **LLM** (Mod√®le de Langage de Grande Taille) est une intelligence artificielle entra√Æn√©e sur des milliards de mots pour comprendre et g√©n√©rer du texte humain.

### Exemples c√©l√®bres
- **GPT-4** (OpenAI)
- **Claude** (Anthropic)
- **Mistral** (Mistral AI)
- **LLaMA** (Meta)
- **Gemini** (Google)

### Comment √ßa fonctionne ?
Un LLM est un r√©seau de neurones artificiels entra√Æn√© sur d'immenses corpus de textes (livres, articles, sites web). Il apprend √† pr√©dire le mot suivant dans une phrase, ce qui lui permet ensuite de g√©n√©rer du texte coh√©rent.

### Analogie
Imaginez une biblioth√®que universelle qui a "lu" tout internet et peut discuter de n'importe quel sujet. Le LLM ne "comprend" pas vraiment, mais il reconna√Æt des motifs statistiques dans le langage.

### Limites
- Peut **halluciner** (inventer des faits faux)
- N'a pas de conscience ou de jugement moral
- D√©pend de la qualit√© de ses donn√©es d'entra√Ænement

---

## Prompt

### D√©finition
Le **prompt** est la consigne, la question ou l'instruction que vous donnez √† un agent conversationnel.

### Exemples
- **Prompt simple** : "Explique-moi le changement climatique."
- **Prompt d√©taill√©** : "Explique-moi les causes du changement climatique en 200 mots, avec un ton p√©dagogique pour des lyc√©ens."

### Bonnes pratiques
- **Soyez pr√©cis** : Plus votre prompt est clair, meilleure sera la r√©ponse.
- **Donnez du contexte** : "Je pr√©pare une pr√©sentation pour mon entreprise..." aide l'IA √† adapter le ton.
- **It√©rez** : Si la premi√®re r√©ponse n'est pas parfaite, reformulez ou pr√©cisez.

### Prompt Engineering
L'art d'√©crire des prompts efficaces s'appelle le **prompt engineering** (voir section d√©di√©e ci-dessous).

---

## RAG (Retrieval-Augmented Generation)

### D√©finition
Le **RAG** (G√©n√©ration Augment√©e par Recherche) est une technique o√π l'IA cherche d'abord des documents pertinents dans une base de donn√©es, puis g√©n√®re une r√©ponse en s'appuyant sur ces documents.

### Comment √ßa fonctionne ?
1. **Vous posez une question** : "Quelles sont les recommandations du rapport sur le climat ?"
2. **L'IA cherche** : Elle parcourt vos documents t√©l√©charg√©s et trouve les passages pertinents.
3. **L'IA g√©n√®re** : Elle r√©dige une r√©ponse en citant les sources trouv√©es.

### Avantages
- **R√©duit les hallucinations** : L'IA s'appuie sur des faits r√©els (vos documents).
- **Permet la tra√ßabilit√©** : Vous savez d'o√π vient l'information (page, document).
- **Personnalise** : L'IA r√©pond avec VOS donn√©es, pas des informations g√©n√©riques.

### Dans √âMERGENCE
Activez le mode RAG depuis l'interface de chat pour interroger vos documents PDF, Word, etc. Les agents citeront automatiquement les passages pertinents.

### Analogie
C'est comme demander √† un assistant de parcourir votre biblioth√®que avant de r√©pondre, au lieu de r√©pondre de m√©moire (o√π il pourrait se tromper).

---

## M√©moire (STM / LTM)

### D√©finition
La **m√©moire conversationnelle** est le m√©canisme par lequel un agent se souvient des √©changes pass√©s pour offrir de la continuit√© et de la personnalisation.

### Les deux types dans √âMERGENCE

#### STM (Short-Term Memory - M√©moire √† Court Terme)
- **Ce qu'elle fait** : R√©sume la session en cours.
- **Dur√©e** : Valable uniquement pendant la conversation active.
- **Exemple** : "L'utilisateur parle aujourd'hui d'un voyage en Italie."
- **Analogie** : Comme votre m√©moire imm√©diate quand vous discutez avec quelqu'un.

#### LTM (Long-Term Memory - M√©moire √† Long Terme)
- **Ce qu'elle fait** : Base de connaissances persistante qui se souvient entre les sessions.
- **Dur√©e** : Permanente, enrichie au fil du temps.
- **Exemple** : "L'utilisateur aime l'Italie, pr√©f√®re les cuisines authentiques."
- **Analogie** : Comme vos souvenirs qui restent apr√®s avoir quitt√© la conversation.

### Pourquoi c'est r√©volutionnaire ?
Sans m√©moire, chaque conversation recommence √† z√©ro. Avec m√©moire, l'agent "apprend" vos pr√©f√©rences et adapte ses r√©ponses au fil du temps.

### Dans √âMERGENCE
Vous pouvez :
- **Consulter** la m√©moire (graphe de connaissances)
- **Consolider** manuellement apr√®s une session importante
- **Effacer** si n√©cessaire
- **√âditer** les concepts m√©moris√©s

---

## Vectorisation

### D√©finition
La **vectorisation** (ou embedding) est la mani√®re de repr√©senter du texte sous forme de nombres (vecteurs) pour que l'IA puisse les comparer math√©matiquement.

### Comment √ßa fonctionne ?
Chaque mot, phrase ou document est transform√© en un tableau de nombres (par ex. 1536 nombres pour GPT-4).

**Exemple simplifi√©** :
- "Chat" ‚Üí [0.2, 0.8, 0.1, ...]
- "Chien" ‚Üí [0.25, 0.75, 0.15, ...] (proche de "chat")
- "Voiture" ‚Üí [0.9, 0.1, 0.05, ...] (loin de "chat")

### Analogie
Imaginez que chaque concept est un point dans un espace multidimensionnel. Les concepts similaires (chat, chien) sont proches, les concepts diff√©rents (chat, voiture) sont √©loign√©s.

### Pourquoi c'est utile ?
La vectorisation permet √† l'IA de :
- **Chercher rapidement** des documents pertinents (RAG)
- **Retrouver** des souvenirs similaires (m√©moire LTM)
- **Comparer** des id√©es (similarit√© cosinus - voir ci-dessous)

### Dans √âMERGENCE
Tous vos documents et concepts m√©moris√©s sont vectoris√©s pour permettre une recherche ultra-rapide.

---

## √âmergence

### D√©finition
L'**√©mergence** est un concept o√π la collaboration de plusieurs √©l√©ments simples produit un r√©sultat complexe, sup√©rieur √† la somme de leurs parties.

### Exemples dans la nature
- Les **fourmis** : Individuellement simples, mais en colonie elles construisent des structures complexes.
- Le **cerveau** : Des milliards de neurones simples cr√©ent la conscience.

### Dans √âMERGENCE (le projet)
- **Anima** (empathie) + **Neo** (analyse) + **Nexus** (coordination) ensemble offrent une r√©ponse plus riche que chacun s√©par√©ment.
- La m√©moire collective des agents devient plus intelligente au fil du temps.

### Philosophie
C'est le c≈ìur du projet : l'id√©e que la **coop√©ration augmente l'intelligence**. Plusieurs agents sp√©cialis√©s collaborant valent mieux qu'un seul agent g√©n√©raliste.

---

## Hallucination (IA)

### D√©finition
Une **hallucination** se produit quand un LLM invente des informations fausses mais les pr√©sente avec assurance.

### Exemples
- "Albert Einstein a d√©couvert la p√©nicilline." (Faux, c'√©tait Alexander Fleming)
- "Le sommet du Mont Blanc culmine √† 5000 m√®tres." (Faux, c'est 4808 m√®tres)
- Inventer des citations ou des r√©f√©rences bibliographiques inexistantes.

### Pourquoi √ßa arrive ?
Les LLM sont entra√Æn√©s √† pr√©dire du texte plausible, pas forc√©ment vrai. Si le mod√®le n'a pas la bonne information, il peut "combler les trous" avec des inventions.

### Comment √©viter ?
- **Activez le RAG** : L'IA s'appuie sur vos documents (faits v√©rifi√©s).
- **V√©rifiez les sources** : Demandez "D'o√π vient cette information ?"
- **Croisez les r√©ponses** : Demandez √† plusieurs agents ou sources.
- **Utilisez le mode strict** : √âMERGENCE peut refuser de r√©pondre si aucun document pertinent n'est trouv√©.

---

## Token

### D√©finition
Un **token** est l'unit√© de texte pour un LLM. Un token peut √™tre un mot, une partie de mot, ou un caract√®re.

### Exemples
- "Bonjour" ‚Üí 1 token
- "Bonjour !" ‚Üí 2 tokens (mot + ponctuation)
- "Intelligence artificielle" ‚Üí 2-3 tokens selon le mod√®le

### Pourquoi c'est important ?
- **Limites de contexte** : Les LLM ont des limites (ex. 4000, 16000, 128000 tokens). Plus vous √©crivez, plus vous consommez de tokens.
- **Co√ªt** : Les LLM cloud facturent au token (ex. 0.03$ / 1000 tokens).
- **Performance** : Plus de tokens = plus de temps de calcul.

### Dans √âMERGENCE
Le Dashboard affiche votre consommation de tokens par agent et par p√©riode.

---

## Temp√©rature

### D√©finition
La **temp√©rature** est un param√®tre qui contr√¥le la cr√©ativit√© (ou impr√©visibilit√©) d'un LLM.

### √âchelle
- **Basse (0.0-0.3)** : R√©ponses pr√©cises, d√©terministes et pr√©visibles.
  - **Bon pour** : Faits, r√©sum√©s, analyses techniques.
  - **Exemple** : "Quelle est la capitale de la France ?" ‚Üí "Paris" (toujours la m√™me r√©ponse).

- **Moyenne (0.5-0.7)** : √âquilibre cr√©ativit√©/pr√©cision.
  - **Bon pour** : Conversation g√©n√©rale, conseils.

- **Haute (0.8-1.0)** : R√©ponses cr√©atives, vari√©es et parfois surprenantes.
  - **Bon pour** : √âcriture cr√©ative, brainstorming, po√©sie.
  - **Exemple** : "√âcris-moi un po√®me sur l'automne" ‚Üí R√©ponse diff√©rente √† chaque fois.

### Dans √âMERGENCE
Vous pouvez ajuster la temp√©rature de chaque agent dans les Param√®tres.

---

## Contexte (fen√™tre de contexte)

### D√©finition
Le **contexte** (ou fen√™tre de contexte) est la quantit√© de texte que l'IA peut "voir" et "se rappeler" en une seule fois.

### Exemples
- **GPT-3.5** : ~4000 tokens (~3000 mots)
- **GPT-4** : ~8000 ou 32000 tokens selon la version
- **Claude 2** : ~100 000 tokens (~75 000 mots)

### Pourquoi c'est une limite ?
Si votre conversation d√©passe la fen√™tre de contexte, l'IA "oublie" le d√©but. C'est comme si vous discutiez avec quelqu'un qui ne se souvient que des 10 derni√®res minutes.

### Dans √âMERGENCE
La **m√©moire LTM** contourne cette limite en r√©sumant et stockant les informations importantes m√™me apr√®s la fin de la session.

---

## Embedding

### D√©finition
Un **embedding** est la repr√©sentation vectorielle d'un texte (voir **Vectorisation** ci-dessus). C'est le terme technique pour "transformer du texte en nombres".

### Mod√®les d'embedding populaires
- **text-embedding-ada-002** (OpenAI)
- **sentence-transformers** (Open source)
- **all-MiniLM-L6-v2** (L√©ger et rapide)

### Utilisation dans √âMERGENCE
Tous vos documents et concepts sont convertis en embeddings pour permettre la recherche s√©mantique (chercher par sens, pas juste par mots-cl√©s).

---

## Fine-tuning

### D√©finition
Le **fine-tuning** (affinage) est le processus d'entra√Æner un LLM existant sur des donn√©es sp√©cifiques pour le sp√©cialiser.

### Exemple
- Prendre GPT-4 et l'entra√Æner sur des dossiers m√©dicaux pour cr√©er un assistant m√©dical.
- Prendre Mistral et l'entra√Æner sur du code Python pour am√©liorer ses comp√©tences en programmation.

### Diff√©rence avec le RAG
- **Fine-tuning** : Modifie les poids du mod√®le (co√ªteux, technique).
- **RAG** : Ajoute simplement des documents dans la base de donn√©es (rapide, accessible).

---

## Prompt Engineering

### D√©finition
Le **prompt engineering** est l'art et la science d'√©crire des prompts efficaces pour obtenir les meilleures r√©ponses d'un LLM.

### Techniques avanc√©es
- **Chain-of-Thought** : Demander √† l'IA de raisonner √©tape par √©tape.
  - Exemple : "Explique ton raisonnement avant de r√©pondre."
- **Role-playing** : Assigner un r√¥le √† l'IA.
  - Exemple : "Tu es un professeur de physique. Explique la relativit√©."
- **Few-shot learning** : Donner des exemples avant la question.
  - Exemple : "Synonyme de 'heureux' : content. Synonyme de 'triste' : ?"

### Dans √âMERGENCE
Les agents utilisent d√©j√† des techniques de prompt engineering en interne pour optimiser leurs r√©ponses.

---

## Zero-shot / Few-shot Learning

### D√©finition
- **Zero-shot** : L'IA r√©pond sans aucun exemple pr√©alable, juste avec la consigne.
- **Few-shot** : Vous donnez quelques exemples avant la question pour guider l'IA.

### Exemple

**Zero-shot** :
```
Traduis en anglais : "Bonjour"
‚Üí "Hello"
```

**Few-shot** :
```
Traduis en anglais :
- "Bonjour" ‚Üí "Hello"
- "Merci" ‚Üí "Thank you"
- "Au revoir" ‚Üí ?
‚Üí "Goodbye"
```

### Pourquoi c'est utile ?
Few-shot am√©liore la pr√©cision sans avoir √† fine-tuner le mod√®le.

---

## Chunking

### D√©finition
Le **chunking** est le processus de d√©couper un document long en morceaux (chunks) plus petits pour faciliter l'indexation et la recherche.

### Pourquoi c'est n√©cessaire ?
Les LLM ont des limites de contexte. Un document de 100 pages ne peut pas √™tre trait√© en une fois. On le d√©coupe donc en chunks de ~500-1000 mots.

### Dans √âMERGENCE
Quand vous t√©l√©chargez un PDF, le syst√®me :
1. Le d√©coupe en chunks (~500 tokens par d√©faut).
2. Vectorise chaque chunk.
3. Lors d'une question, cherche les chunks pertinents (RAG).

### Param√©trage
Vous pouvez ajuster la taille des chunks dans les Param√®tres :
- **Petits chunks (200 tokens)** : R√©ponses tr√®s pr√©cises, mais peut manquer le contexte global.
- **Grands chunks (800 tokens)** : Plus de contexte, mais moins de pr√©cision.

---

## Similarit√© Cosinus

### D√©finition
La **similarit√© cosinus** est une mesure math√©matique pour comparer deux vecteurs (embeddings) et d√©terminer leur degr√© de similarit√©.

### Comment √ßa fonctionne ?
- **Score 1.0** : Identique
- **Score 0.8-0.99** : Tr√®s similaire
- **Score 0.5-0.79** : Moyennement similaire
- **Score 0.0** : Pas de similarit√©
- **Score n√©gatif** : Oppos√©

### Exemple
Si vous cherchez "voyage en Italie", le syst√®me compare votre question vectoris√©e avec tous les chunks de documents :
- "Guide de voyage √† Rome" ‚Üí 0.92 (tr√®s pertinent)
- "Histoire de l'Italie" ‚Üí 0.78 (pertinent)
- "Recette de pizza" ‚Üí 0.65 (moyennement pertinent)
- "M√©canique automobile" ‚Üí 0.12 (pas pertinent)

### Dans √âMERGENCE
Le RAG utilise la similarit√© cosinus pour classer les documents par pertinence avant de g√©n√©rer une r√©ponse.

---

## Ressources pour aller plus loin

### Articles p√©dagogiques
- [Qu'est-ce qu'un LLM ?](https://fr.wikipedia.org/wiki/Grand_mod%C3%A8le_de_langage) (Wikipedia)
- [Comprendre le RAG](https://www.anthropic.com/research/retrieval-augmented-generation) (Anthropic)
- [Guide du prompt engineering](https://www.promptingguide.ai/) (Prompting Guide)

### Tutoriels techniques
- [Documentation OpenAI](https://platform.openai.com/docs) (GPT, embeddings)
- [Documentation Anthropic](https://docs.anthropic.com) (Claude)
- [LangChain](https://python.langchain.com/) (Framework RAG)

### Cha√Ænes YouTube vulgarisation
- [Underscore_](https://www.youtube.com/@Underscore_) (IA en fran√ßais)
- [Lex Fridman](https://www.youtube.com/@lexfridman) (Interviews experts IA)
- [Two Minute Papers](https://www.youtube.com/@TwoMinutePapers) (Recherche IA vulgaris√©e)

---

## Contribuer au glossaire

Ce glossaire est vivant et s'enrichit r√©guli√®rement. Si vous identifiez :
- Un terme manquant
- Une d√©finition impr√©cise
- Une meilleure analogie

Contactez l'√©quipe √âMERGENCE ou proposez une modification via le d√©p√¥t GitHub.

---

**Version** : 1.0
**Derni√®re mise √† jour** : Octobre 2025
**Auteur** : √âquipe √âMERGENCE
**Licence** : CC BY-SA 4.0 (Creative Commons Attribution-ShareAlike)

---

**üîó Retour au** [Tutoriel √âMERGENCE](EMERGENCE_TUTORIEL_VULGARISE_V2.md)
