# mypy: ignore-errors
# src/backend/features/memory/vector_service.py
# V3.6.0 (V13.2 - Startup-safe RAG)
#          - Lazy-load s√ªr (double-checked lock) + t√©l√©m√©trie ultra-OFF conserv√©e
#          - __init__ ne charge plus ni SBERT ni Chroma
#          - _ensure_inited() d√©clench√© au 1er appel public
#          - Pr√©-check corruption + backup AVANT init Chroma (inchang√©)
#          - API publique identique
#          - üÜï V13.2: Mode READ-ONLY fallback si ChromaDB indisponible au d√©marrage
#            √âcritures bloqu√©es (upsert/update/delete) avec logs structur√©s
#            Nouvelles m√©thodes: get_vector_mode(), get_last_init_error(), is_vector_store_reachable()

import logging
import os
import shutil
import sqlite3
import sys
import types
import threading
import uuid
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional, Sequence, Union, cast


# ---- Force disable telemetry as early as possible (before importing chromadb) ----
def _force_disable_telemetry_env() -> None:
    try:
        os.environ.setdefault("CHROMA_DISABLE_TELEMETRY", "1")
        os.environ.setdefault("ANONYMIZED_TELEMETRY", "0")
        os.environ.setdefault("PERSIST_TELEMETRY", "0")
        os.environ.setdefault("CHROMA_TELEMETRY_ENABLED", "0")
        os.environ.setdefault("DO_NOT_TRACK", "1")
        os.environ.setdefault("POSTHOG_DISABLED", "1")
    except Exception:
        pass


def _monkeypatch_posthog_noop() -> None:
    """Neutralise totalement le module posthog (classe + fonctions module-level)."""

    def _noop(*_: Any, **__: Any) -> None:
        return None

    class _NoopPosthog:
        def __init__(self, *args: Any, **kwargs: Any) -> None:
            return None

        def capture(self, *args: Any, **kwargs: Any) -> None:
            return None

        def identify(self, *args: Any, **kwargs: Any) -> None:
            return None

        def flush(self, *args: Any, **kwargs: Any) -> None:
            return None

        def shutdown(self, *args: Any, **kwargs: Any) -> None:
            return None

    try:
        import posthog  # type: ignore

        posthog_mod = cast(Any, posthog)
        try:
            setattr(posthog_mod, "Posthog", _NoopPosthog)
            setattr(posthog_mod, "capture", _noop)
            setattr(posthog_mod, "identify", _noop)
            setattr(posthog_mod, "flush", _noop)
            setattr(posthog_mod, "shutdown", _noop)
        except Exception:
            pass
    except Exception:
        shim = types.ModuleType("posthog")
        setattr(shim, "Posthog", _NoopPosthog)
        setattr(shim, "capture", _noop)
        setattr(shim, "identify", _noop)
        setattr(shim, "flush", _noop)
        setattr(shim, "shutdown", _noop)
        sys.modules.setdefault("posthog", shim)


_force_disable_telemetry_env()
_monkeypatch_posthog_noop()


def _env_flag(name: str, default: str = "0") -> bool:
    value = os.getenv(name, default)
    if value is None:
        return False
    return value.strip().lower() in {"1", "true", "yes", "on"}


class _StubSentenceTransformer:
    """Minimal stub used when SentenceTransformer cannot be loaded (tests/offline)."""

    def __init__(self, embedding_dim: int = 384) -> None:
        self.embedding_dim = embedding_dim

    def encode(
        self,
        sentences: Union[str, Iterable[str]],
        show_progress_bar: bool = False,
        convert_to_tensor: bool = False,
        **_: Any,
    ) -> Any:
        if isinstance(sentences, str):
            normalized: List[str] = [sentences]
        else:
            normalized = list(sentences or [])

        try:
            import numpy as np

            embeddings = np.zeros(
                (len(normalized), self.embedding_dim), dtype="float32"
            )
        except Exception:
            embeddings = [[0.0] * self.embedding_dim for _ in normalized]

        if convert_to_tensor:
            try:
                import torch

                return torch.as_tensor(embeddings)
            except Exception:
                pass

        return embeddings

    __call__ = encode


class _VectorEmbeddingFunction:
    """Embedding function wrapper compatible with Chroma's interface."""

    def __init__(self, service: "VectorService") -> None:
        self._service = service

    def __call__(self, input: Sequence[str]) -> List[List[float]]:
        if not input:
            return []
        encoded = self._service.model.encode(list(input), show_progress_bar=False)  # type: ignore[union-attr]
        if hasattr(encoded, "tolist"):
            return encoded.tolist()
        return cast(List[List[float]], encoded)


# Imports de libs (on garde les imports module-level, l'instanciation sera lazy)
import chromadb  # noqa: E402
from chromadb.config import Settings  # noqa: E402
from chromadb.types import Collection  # noqa: E402
from sentence_transformers import SentenceTransformer  # type: ignore[import-untyped]  # noqa: E402

try:
    from qdrant_client import QdrantClient  # type: ignore
    from qdrant_client.http import models as qdrant_models  # type: ignore
except Exception:  # pragma: no cover - d√©pendance optionnelle
    QdrantClient = None  # type: ignore
    qdrant_models = None  # type: ignore

logger = logging.getLogger(__name__)


# ---- Memory Config Loader ----
class MemoryConfig:
    """Configuration pour le syst√®me de retrieval pond√©r√© par l'horodatage."""

    def __init__(self, config_path: Optional[str] = None):
        # Valeurs par d√©faut
        self.decay_lambda = 0.02
        self.reinforcement_alpha = 0.1
        self.top_k = 8
        self.score_threshold = 0.2
        self.enable_trace_logging = False
        self.gc_inactive_days = 180

        # Charger depuis fichier si disponible
        if config_path is None:
            # Chemin par d√©faut
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(current_dir, "memory_config.json")

        if os.path.exists(config_path):
            try:
                import json

                with open(config_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # Charger la section "default" si elle existe
                    config_data = data.get("default", data)
                    self.decay_lambda = config_data.get(
                        "decay_lambda", self.decay_lambda
                    )
                    self.reinforcement_alpha = config_data.get(
                        "reinforcement_alpha", self.reinforcement_alpha
                    )
                    self.top_k = config_data.get("top_k", self.top_k)
                    self.score_threshold = config_data.get(
                        "score_threshold", self.score_threshold
                    )
                    self.enable_trace_logging = config_data.get(
                        "enable_trace_logging", self.enable_trace_logging
                    )
                    self.gc_inactive_days = config_data.get(
                        "gc_inactive_days", self.gc_inactive_days
                    )
                logger.info(
                    f"MemoryConfig charg√©e depuis {config_path}: Œª={self.decay_lambda}, Œ±={self.reinforcement_alpha}, "
                    f"top_k={self.top_k}, threshold={self.score_threshold}"
                )
            except Exception as e:
                logger.warning(
                    f"Impossible de charger {config_path}, utilisation des valeurs par d√©faut: {e}"
                )
        else:
            logger.info(
                f"Fichier config {config_path} absent, utilisation des valeurs par d√©faut"
            )

    @classmethod
    def from_env(cls) -> "MemoryConfig":
        """Charge la config depuis les variables d'environnement (priorit√© sur le fichier JSON)."""
        config = cls()
        # Override depuis env si pr√©sent
        if os.getenv("MEMORY_DECAY_LAMBDA"):
            try:
                config.decay_lambda = float(os.getenv("MEMORY_DECAY_LAMBDA"))
            except ValueError:
                pass
        if os.getenv("MEMORY_REINFORCEMENT_ALPHA"):
            try:
                config.reinforcement_alpha = float(
                    os.getenv("MEMORY_REINFORCEMENT_ALPHA")
                )
            except ValueError:
                pass
        if os.getenv("MEMORY_TOP_K"):
            try:
                config.top_k = int(os.getenv("MEMORY_TOP_K"))
            except ValueError:
                pass
        if os.getenv("MEMORY_TRACE_LOGGING"):
            config.enable_trace_logging = os.getenv(
                "MEMORY_TRACE_LOGGING", ""
            ).lower() in {"1", "true", "yes"}
        return config


# ---- P2.2 - RAG freshness & diversity helpers ----
def recency_decay(age_days: float, half_life: float = 90.0) -> float:
    """
    Calcule un facteur de d√©croissance temporelle exponentiel.

    Args:
        age_days: √Çge du document en jours (0 = aujourd'hui)
        half_life: Nombre de jours pour que le poids soit divis√© par 2 (d√©faut: 90j)

    Returns:
        Float entre 0 et 1 (1 = document tr√®s r√©cent, 0 = tr√®s ancien)

    Exemples:
        - age_days=0   ‚Üí 1.0   (aujourd'hui)
        - age_days=90  ‚Üí 0.5   (3 mois)
        - age_days=180 ‚Üí 0.25  (6 mois)
        - age_days=270 ‚Üí 0.125 (9 mois)
    """
    if age_days < 0:
        age_days = 0
    return 0.5 ** (age_days / half_life)


def compute_memory_score(
    cosine_sim: float,
    delta_days: float,
    freq: int,
    lambda_: float = 0.02,
    alpha: float = 0.1,
) -> float:
    """
    Calcule un score pond√©r√© combinant similarit√© s√©mantique, fra√Æcheur temporelle et fr√©quence d'utilisation.

    Formule : score = cosine_sim √ó exp(-Œª √ó Œît) √ó (1 + Œ± √ó freq)

    o√π :
    - cosine_sim : similarit√© cosine entre query et m√©moire (0-1)
    - Œît : nombre de jours depuis last_used_at
    - freq : nombre d'utilisations r√©centes (use_count)
    - Œª (lambda) : taux de d√©croissance temporelle (0.01-0.05 recommand√©)
    - Œ± (alpha) : facteur de renforcement par usage (0.05-0.2 recommand√©)

    Args:
        cosine_sim: Similarit√© cosine (0-1, 1 = identique)
        delta_days: Jours depuis derni√®re utilisation (Œît)
        freq: Nombre d'utilisations (use_count)
        lambda_: Taux de d√©croissance exponentielle (d√©faut: 0.02)
                 Plus Œª est grand, plus l'oubli est rapide
                 Exemples: Œª=0.02 ‚Üí demi-vie ~35 jours, Œª=0.05 ‚Üí demi-vie ~14 jours
        alpha: Facteur de renforcement par fr√©quence (d√©faut: 0.1)
               Exemples: Œ±=0.1, freq=5 ‚Üí boost +50%

    Returns:
        Score pond√©r√© (float > 0)

    Exemples:
        >>> # M√©moire r√©cente tr√®s utilis√©e
        >>> compute_memory_score(0.85, delta_days=2, freq=10, lambda_=0.02, alpha=0.1)
        1.615  # Excellent score (r√©cent + fr√©quent)

        >>> # M√©moire ancienne peu utilis√©e
        >>> compute_memory_score(0.85, delta_days=100, freq=1, lambda_=0.02, alpha=0.1)
        0.115  # Score faible (ancien + rare)

        >>> # M√©moire ancienne mais tr√®s utilis√©e
        >>> compute_memory_score(0.85, delta_days=50, freq=20, lambda_=0.02, alpha=0.1)
        0.864  # Score moyen-bon (renforcement compense l'anciennet√©)
    """
    import math

    # Protection contre valeurs invalides
    cosine_sim = max(0.0, min(1.0, cosine_sim))
    delta_days = max(0.0, delta_days)
    freq = max(0, freq)
    lambda_ = max(0.001, lambda_)
    alpha = max(0.0, alpha)

    # D√©croissance temporelle : exp(-Œª √ó Œît)
    freshness_weight = math.exp(-lambda_ * delta_days)

    # Renforcement par fr√©quence : (1 + Œ± √ó freq)
    reinforcement = 1.0 + alpha * freq

    # Score final combin√©
    score = cosine_sim * freshness_weight * reinforcement

    return score


def mmr(
    query_embedding: List[float],
    candidates: List[Dict[str, Any]],
    k: int = 5,
    lambda_param: float = 0.7,
) -> List[Dict[str, Any]]:
    """
    Maximal Marginal Relevance - s√©lectionne les k r√©sultats les plus pertinents ET diversifi√©s.

    Formule: MMR = Œª * sim(query, doc) - (1-Œª) * max(sim(doc, selected))

    Args:
        query_embedding: Embedding de la requ√™te
        candidates: Liste de dicts avec {id, text, metadata, distance, embedding}
        k: Nombre de r√©sultats √† retourner
        lambda_param: Balance pertinence/diversit√© (1.0 = full pertinence, 0.0 = full diversit√©)

    Returns:
        Liste de k r√©sultats ordonn√©s par score MMR d√©croissant
    """
    import numpy as np

    if not candidates or k <= 0:
        return []

    # Prendre au max k r√©sultats
    k = min(k, len(candidates))

    # Si un seul r√©sultat, pas besoin de MMR
    if k == 1 or len(candidates) == 1:
        return candidates[:1]

    # Convertir query en numpy
    query_vec = np.array(query_embedding)

    # Assurer que tous les candidats ont un embedding
    # (si pas pr√©sent, utiliser l'embedding query comme fallback - pas id√©al mais safe)
    for cand in candidates:
        if "embedding" not in cand or cand["embedding"] is None:
            cand["embedding"] = query_embedding

    # Extraire embeddings des candidats
    candidate_vecs = [np.array(c["embedding"]) for c in candidates]

    # Fonction de similarit√© cosine
    def cosine_sim(a, b):
        dot = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(dot / (norm_a * norm_b))

    # Calcul de la similarit√© query<->candidates
    query_sims = [cosine_sim(query_vec, vec) for vec in candidate_vecs]

    # Indices des candidats s√©lectionn√©s et restants
    selected_indices: List[int] = []
    remaining_indices = list(range(len(candidates)))

    # Premier r√©sultat : le plus similaire √† la query
    best_idx = int(np.argmax(query_sims))
    selected_indices.append(best_idx)
    remaining_indices.remove(best_idx)

    # S√©lection it√©rative des k-1 autres r√©sultats
    while len(selected_indices) < k and remaining_indices:
        mmr_scores = []
        for idx in remaining_indices:
            # Pertinence par rapport √† la query
            relevance = query_sims[idx]

            # Similarit√© max avec les docs d√©j√† s√©lectionn√©s
            max_sim_selected = max(
                cosine_sim(candidate_vecs[idx], candidate_vecs[sel_idx])
                for sel_idx in selected_indices
            )

            # Score MMR
            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim_selected
            mmr_scores.append((idx, mmr_score))

        # S√©lectionner le meilleur score MMR
        best_idx, _ = max(mmr_scores, key=lambda x: x[1])
        selected_indices.append(best_idx)
        remaining_indices.remove(best_idx)

    # Retourner les r√©sultats s√©lectionn√©s dans l'ordre MMR
    return [candidates[idx] for idx in selected_indices]


def compute_specificity_score(text: str) -> float:
    """
    Calcule un score de sp√©cificit√© bas√© sur la densit√© de contenu informatif.

    Crit√®res:
    - Densit√© de tokens rares (long tokens > 6 caract√®res)
    - Densit√© de nombres/dates
    - Densit√© d'entit√©s nomm√©es (mots capitalis√©s)

    Args:
        text: Texte du chunk √† analyser

    Returns:
        Score de sp√©cificit√© entre 0 et 1 (0 = peu sp√©cifique, 1 = tr√®s sp√©cifique)

    Examples:
        >>> compute_specificity_score("The configuration parameter is 0.75")
        0.82  # Haute sp√©cificit√© (nombres + tokens longs)

        >>> compute_specificity_score("this is a simple text")
        0.15  # Basse sp√©cificit√© (mots communs courts)
    """
    if not text or not text.strip():
        return 0.0

    import re

    # Tokenize (split by whitespace and punctuation)
    tokens = re.findall(r"\b\w+\b", text)
    if not tokens:
        return 0.0

    total_tokens = len(tokens)

    # 1. Densit√© tokens rares (IDF approxim√©)
    # Heuristique: tokens longs (> 6 car) + tokens mixtes alphanum√©riques
    rare_tokens = [t for t in tokens if len(t) > 6 or any(c.isdigit() for c in t)]
    rare_density = len(rare_tokens) / total_tokens

    # 2. Densit√© nombres/dates
    # Regex: nombres d√©cimaux, ann√©es, dates
    numbers = re.findall(r"\b\d+\.?\d*\b|\b\d{4}\b|\b\d{1,2}/\d{1,2}/\d{2,4}\b", text)
    number_density = len(numbers) / total_tokens

    # 3. Densit√© entit√©s nomm√©es (heuristique: mots capitalis√©s hors d√©but de phrase)
    # Split en phrases
    sentences = re.split(r"[.!?]+", text)
    capitalized = []
    for sentence in sentences:
        words = re.findall(r"\b[A-Z][a-z]+\b", sentence)
        # Exclure le premier mot (probablement d√©but de phrase)
        if len(words) > 1:
            capitalized.extend(words[1:])
        elif len(words) == 1:
            # Si un seul mot et pas en d√©but de phrase, c'est probablement une entit√©
            if sentence.strip() and not sentence.strip().startswith(words[0]):
                capitalized.append(words[0])

    ner_density = len(capitalized) / total_tokens if capitalized else 0.0

    # Combinaison pond√©r√©e des 3 facteurs
    # Poids: rare_tokens (40%), numbers (30%), NER (30%)
    specificity_score = rare_density * 0.40 + number_density * 0.30 + ner_density * 0.30

    # Normaliser sur [0, 1] avec saturation douce (tanh)
    import math

    normalized_score = math.tanh(
        specificity_score * 2.0
    )  # tanh(x*2) ‚Üí saturation √† ~0.96 pour x=1

    return max(0.0, min(1.0, normalized_score))


def rerank_with_lexical_overlap(
    query: str,
    results: List[Dict[str, Any]],
    topk: int = 8,
    cosine_weight: float = 0.7,
    lexical_weight: float = 0.3,
) -> List[Dict[str, Any]]:
    """
    Re-ranke les r√©sultats en combinant score cosine et overlap lexical (Jaccard).

    Formule: rerank_score = cosine_weight * cosine_sim + lexical_weight * jaccard(query, text)

    Args:
        query: Requ√™te utilisateur
        results: Liste de r√©sultats avec {text, distance, ...}
        topk: Nombre de r√©sultats √† retourner apr√®s re-ranking
        cosine_weight: Poids du score cosine (d√©faut: 0.7)
        lexical_weight: Poids du score Jaccard (d√©faut: 0.3)

    Returns:
        Liste de r√©sultats re-rank√©s (top-k)

    Examples:
        >>> results = [
        ...     {"text": "machine learning model training", "distance": 0.3},
        ...     {"text": "deep learning neural networks", "distance": 0.4},
        ... ]
        >>> reranked = rerank_with_lexical_overlap("train ML model", results, topk=2)
        >>> reranked[0]["text"]
        "machine learning model training"  # Meilleur score lexical
    """
    if not results or topk <= 0:
        return []

    import re

    # Fonction de lemmatisation simple (lowercase + strip)
    def simple_lemmatize(text: str) -> set[str]:
        """Tokenize et normalise le texte (lowercase, alphanum√©rique uniquement)."""
        tokens = re.findall(r"\b\w+\b", text.lower())
        return set(tokens)

    # Lemmatiser la query
    query_lemmas = simple_lemmatize(query)

    # Calculer Jaccard similarity pour chaque r√©sultat
    scored_results = []
    for r in results:
        text = r.get("text", "")
        distance = r.get("distance", 1.0)

        # Score cosine (distance L2 squared ‚Üí cosine)
        # ChromaDB L2 squared: distance = 2 * (1 - cosine_sim)
        cosine_sim = max(0.0, 1.0 - (distance / 2.0))

        # Score Jaccard
        text_lemmas = simple_lemmatize(text)
        if query_lemmas and text_lemmas:
            intersection = len(query_lemmas & text_lemmas)
            union = len(query_lemmas | text_lemmas)
            jaccard_score = intersection / union if union > 0 else 0.0
        else:
            jaccard_score = 0.0

        # Score de rerank combin√©
        rerank_score = cosine_weight * cosine_sim + lexical_weight * jaccard_score

        # Enrichir r√©sultat avec les scores
        result_copy = dict(r)
        result_copy["cosine_sim"] = round(cosine_sim, 4)
        result_copy["jaccard_score"] = round(jaccard_score, 4)
        result_copy["rerank_score"] = round(rerank_score, 4)

        scored_results.append(result_copy)

    # Trier par rerank_score d√©croissant
    scored_results.sort(key=lambda x: x.get("rerank_score", 0.0), reverse=True)

    # Retourner top-k
    return scored_results[:topk]


class QdrantCollectionAdapter:
    """Adapte l'API collection Chroma attendue par le code pour Qdrant."""

    def __init__(self, service: "VectorService", name: str):
        self._service = service
        self.name = name

    # Les signatures restent compatibles avec chroma.Collection.get/delete
    def get(self, where: Optional[Dict[str, Any]] = None, limit: Optional[int] = None):
        return self._service._qdrant_get(self.name, where_filter=where, limit=limit)

    def delete(
        self, where: Optional[Dict[str, Any]] = None, ids: Optional[List[str]] = None
    ) -> None:
        self._service._qdrant_delete_via_collection(
            self.name, where_filter=where, ids=ids
        )


class VectorService:
    """
    VectorService V3.6.0 (V13.2 - Startup-safe RAG)
    - API identique.
    - Lazy-load: mod√®le SBERT + backend vectoriel (Chroma ou Qdrant) instanci√©s au 1er usage.
    - Auto-reset AVANT instanciation Chroma si DB corrompue (√©vite locks Windows).
    - T√©l√©m√©trie Chroma/PostHog durcie (env + shim).
    - Normalisation des filtres where conserv√©e.
    - Backend Qdrant optionnel (via qdrant-client) avec fallback automatique sur Chroma.
    - üÜï V13.2: Mode READ-ONLY fallback si ChromaDB indisponible au d√©marrage.
      √âcritures bloqu√©es avec logs structur√©s. Endpoint /health/ready expose status.
    """

    def __init__(
        self,
        persist_directory: str,
        embed_model_name: str,
        auto_reset_on_schema_error: bool = True,
        backend_preference: str = "auto",
        qdrant_url: Optional[str] = None,
        qdrant_api_key: Optional[str] = None,
    ):
        self.persist_directory = os.path.abspath(persist_directory)
        self.embed_model_name = embed_model_name
        self.auto_reset_on_schema_error = auto_reset_on_schema_error
        self.backend_preference = (backend_preference or "auto").strip().lower()
        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL") or None
        self.qdrant_api_key = qdrant_api_key or os.getenv("QDRANT_API_KEY") or None

        os.makedirs(self.persist_directory, exist_ok=True)

        # Lazy members (instanci√©s √† la demande)
        self.model: Any = None
        self.client: Any = None
        self.qdrant_client: Optional[QdrantClient] = None  # type: ignore[assignment]

        # Backend effectif s√©lectionn√© ("chroma" ou "qdrant")
        self.backend: str = "chroma"
        self._qdrant_known_collections: Dict[str, int] = {}

        # Guard thread-safe (double-checked lock)
        self._init_lock = threading.Lock()
        self._inited = False

        # V13.2 - Startup-safe mode
        self._vector_mode = "readwrite"  # "readwrite" | "readonly"
        self._last_init_error: Optional[str] = None

        # üß™ Tests/offline: autoriser un stub SBERT si configur√©
        self._allow_stub_model = _env_flag("VECTOR_SERVICE_ALLOW_STUB") or _env_flag(
            "ALLOW_SENTENCE_TRANSFORMER_STUB"
        )
        stub_dim_env = os.getenv("VECTOR_SERVICE_STUB_DIM") or os.getenv(
            "SENTENCE_TRANSFORMER_STUB_DIM"
        )
        try:
            self._stub_embedding_dim = int(stub_dim_env) if stub_dim_env else 384
        except (TypeError, ValueError):
            self._stub_embedding_dim = 384
        self._using_stub_model = False

        # üÜï Score cache pour performance
        from backend.features.memory.score_cache import ScoreCache

        cache_size = int(os.getenv("MEMORY_SCORE_CACHE_SIZE", "10000"))
        cache_ttl = int(os.getenv("MEMORY_SCORE_CACHE_TTL", "3600"))
        self.score_cache = ScoreCache(max_size=cache_size, ttl_seconds=cache_ttl)
        logger.info(
            f"[VectorService] Score cache initialis√© (size={cache_size}, ttl={cache_ttl}s)"
        )

        # üÜï M√©triques Prometheus pour weighted retrieval
        from backend.features.memory.weighted_retrieval_metrics import (
            WeightedRetrievalMetrics,
        )

        self.metrics = WeightedRetrievalMetrics()
        logger.info("[VectorService] M√©triques Prometheus initialis√©es")

        # Weighted retrieval config
        self.memory_config = MemoryConfig.from_env()
        logger.info(
            f"VectorService: Weighted retrieval activ√© (Œª={self.memory_config.decay_lambda}, "
            f"Œ±={self.memory_config.reinforcement_alpha}, top_k={self.memory_config.top_k})"
        )

    # ---------- Lazy init ----------
    def _ensure_inited(self) -> None:
        if self._inited and self.model is not None:
            if self.backend == "chroma" and self.client is not None:
                return
            if self.backend == "qdrant" and self.qdrant_client is not None:
                return
        with self._init_lock:
            if self._inited and self.model is not None:
                if self.backend == "chroma" and self.client is not None:
                    return
                if self.backend == "qdrant" and self.qdrant_client is not None:
                    return

            # 0) Pr√©-check : corruption SQLite ‚Üí backup + reset AVANT Chroma
            if self.auto_reset_on_schema_error and self._is_sqlite_corrupted(
                self.persist_directory
            ):
                logger.warning(
                    "Pr√©-check: DB Chroma corrompue d√©tect√©e. Auto-reset prot√©g√© AVANT initialisation‚Ä¶"
                )
                backup_path = self._backup_persist_dir(self.persist_directory)
                logger.warning(f"Store existant d√©plac√© en backup: {backup_path}")

            # 1) Charger le mod√®le d'embedding (commun aux backends)
            if self.model is None:
                try:
                    self.model = SentenceTransformer(self.embed_model_name)
                    logger.info(
                        f"Mod√®le SentenceTransformer '{self.embed_model_name}' charg√© (lazy)."
                    )
                except Exception as e:
                    logger.error(
                        f"√âchec du chargement du mod√®le '{self.embed_model_name}': {e}",
                        exc_info=True,
                    )
                    if self._allow_stub_model:
                        logger.warning(
                            "VectorService: fallback stub activ√© pour '%s' (dim=%s).",
                            self.embed_model_name,
                            self._stub_embedding_dim,
                        )
                        self.model = _StubSentenceTransformer(self._stub_embedding_dim)
                        self._using_stub_model = True
                    else:
                        self._last_init_error = str(e)
                        raise

            # 2) S√©lectionner et initialiser le backend vectoriel
            backend = self._select_backend()
            if backend == "qdrant":
                if not self._init_qdrant_client():
                    logger.warning(
                        "VectorService: fallback sur Chroma (init Qdrant impossible)."
                    )
                    backend = "chroma"

            if backend == "chroma":
                self.client = self._init_client_with_guard(
                    self.persist_directory, self.auto_reset_on_schema_error
                )
            else:
                logger.info("VectorService: backend Qdrant activ√©.")

            self.backend = backend
            self._inited = True
            logger.info(
                "VectorService initialis√© (lazy) : SBERT + backend %s pr√™ts.",
                backend.upper(),
            )

    def _build_collection_embedding_function(self):
        return _VectorEmbeddingFunction(self)

    # ---------- V13.2 - Startup-safe READ-ONLY mode ----------
    def _check_write_allowed(self, operation: str, collection_name: str = "") -> None:
        """
        V√©rifie si les √©critures sont autoris√©es. Si mode readonly, log structur√© et raise.
        """
        if self._vector_mode == "readonly":
            error_msg = (
                f"‚ö†Ô∏è  Op√©ration d'√©criture bloqu√©e (mode READ-ONLY): "
                f"op={operation}, collection={collection_name}, "
                f"reason=ChromaDB unavailable, last_error={self._last_init_error}"
            )
            logger.warning(error_msg)
            raise RuntimeError(
                f"VectorService en mode READ-ONLY (√©critures bloqu√©es). "
                f"Op√©ration: {operation}"
            )

    def get_vector_mode(self) -> str:
        """Retourne le mode actuel: 'readwrite' ou 'readonly'"""
        return self._vector_mode

    def get_last_init_error(self) -> Optional[str]:
        """Retourne la derni√®re erreur d'initialisation (si readonly)"""
        return self._last_init_error

    def is_vector_store_reachable(self) -> bool:
        """
        V√©rifie si le vector store (ChromaDB/Qdrant) est accessible.
        Retourne False si mode readonly ou si client non initialis√©.
        """
        if self._vector_mode == "readonly":
            return False
        if self.backend == "chroma":
            return self.client is not None
        if self.backend == "qdrant":
            return self.qdrant_client is not None
        return False

    # ---------- Pr√©-check SQLite ----------
    def _is_sqlite_corrupted(self, path: str) -> bool:
        db_path = os.path.join(path, "chroma.sqlite3")
        if not os.path.exists(db_path):
            return False
        try:
            con = sqlite3.connect(db_path)
            try:
                cur = con.execute("PRAGMA integrity_check;")
                row = cur.fetchone()
                ok = row and isinstance(row[0], str) and row[0].lower() == "ok"
                return not ok
            finally:
                con.close()
        except sqlite3.DatabaseError:
            return True
        except Exception:
            return False

    # ---------- Initialisation prot√©g√©e du client ----------
    def _init_client_with_guard(self, path: str, allow_auto_reset: bool) -> Any:
        try:
            client = chromadb.PersistentClient(
                path=path, settings=Settings(anonymized_telemetry=False)
            )
            _ = client.list_collections()
            logger.info(f"Client ChromaDB connect√© au r√©pertoire: {path}")
            return client

        except Exception as e:
            msg = str(e).lower()
            schema_signatures = (
                "no such column",
                "schema mismatch",
                "wrong number of columns",
                "has no column named",
                "operationalerror",
                "file is not a database",
                "not a database",
                "database disk image is malformed",
                "could not connect to tenant",
                "default_tenant",
            )
            is_schema_issue = any(sig in msg for sig in schema_signatures)

            if is_schema_issue and allow_auto_reset:
                logger.warning(
                    "Incompatibilit√©/corruption du store Chroma d√©tect√©e durant init. Auto-reset prot√©g√© (post-essai)‚Ä¶"
                )
                backup_path = self._backup_persist_dir(self.persist_directory)
                logger.warning(f"Store existant d√©plac√© en backup: {backup_path}")
                client = chromadb.PersistentClient(
                    path=path, settings=Settings(anonymized_telemetry=False)
                )
                _ = client.list_collections()
                logger.info(
                    f"Nouveau store ChromaDB initialis√© avec succ√®s dans: {path}"
                )
                return client
            else:
                # V13.2 - Fallback READ-ONLY au lieu de crash
                error_msg = f"ChromaDB init failed: {e}"
                self._last_init_error = error_msg
                self._vector_mode = "readonly"
                logger.warning(
                    f"‚ö†Ô∏è  VectorService bascul√© en mode READ-ONLY (ChromaDB indisponible). "
                    f"√âcritures bloqu√©es. Erreur: {e}",
                    exc_info=False,
                )
                return None  # Signal √©chec init sans crash

    def _backup_persist_dir(self, path: str) -> str:
        ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        backup_path = f"{path}_backup_{ts}"
        try:
            if os.path.isdir(path) and os.listdir(path):
                shutil.move(path, backup_path)
            os.makedirs(path, exist_ok=True)
        except Exception as e:
            logger.error(
                f"√âchec du backup '{path}' vers '{backup_path}': {e}", exc_info=True
            )
            raise
        return backup_path

    # ---------- S√©lection backend ----------
    def _select_backend(self) -> str:
        pref = (self.backend_preference or "auto").lower()
        if pref in {"chroma", "chromadb"}:
            return "chroma"
        if pref == "qdrant":
            return "qdrant"
        if pref == "auto":
            if QdrantClient is not None and self.qdrant_url:
                return "qdrant"
        return "chroma"

    def _init_qdrant_client(self) -> bool:
        if QdrantClient is None:
            logger.warning(
                "qdrant-client non install√© - impossible d'initialiser le backend Qdrant."
            )
            self.qdrant_client = None
            return False
        target = self.qdrant_url
        if not target:
            logger.warning("VectorService: URL Qdrant absente (env QDRANT_URL).")
            self.qdrant_client = None
            return False
        try:
            self.qdrant_client = QdrantClient(
                url=target, api_key=self.qdrant_api_key, timeout=5.0
            )  # type: ignore[call-arg]
            self.qdrant_client.get_collections()
            logger.info(f"Client Qdrant connect√©: {target}")
            return True
        except Exception as e:
            logger.error(f"√âchec connexion Qdrant ({target}): {e}", exc_info=True)
            self.qdrant_client = None
            return False

    # ---------- Normalisation where (FIX) ----------
    def _normalize_where(
        self, where: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        if not where:
            return None
        if any(str(k).startswith("$") for k in where.keys()):
            if "$and" in where and isinstance(where["$and"], list):
                lst = where["$and"]
                if len(lst) == 0:
                    return None
                if len(lst) == 1 and isinstance(lst[0], dict):
                    return lst[0]
                return where
            if "$or" in where and isinstance(where["$or"], list):
                lst = where["$or"]
                if len(lst) == 0:
                    return None
                if len(lst) == 1 and isinstance(lst[0], dict):
                    return lst[0]
                return where
            return where
        items = list(where.items())
        if len(items) <= 1:
            return where
        return {"$and": [{k: v} for k, v in items]}

    # ---------- Backend Qdrant helpers ----------
    def _flatten_where_pairs(
        self, where_filter: Optional[Dict[str, Any]]
    ) -> List[tuple[str, Any]]:
        pairs: List[tuple[str, Any]] = []

        def _walk(node: Any) -> None:
            if isinstance(node, dict):
                for key, value in node.items():
                    if key in {"$and", "$or"} and isinstance(value, list):
                        for child in value:
                            _walk(child)
                    elif not str(key).startswith("$"):
                        pairs.append((key, value))

        if where_filter:
            _walk(where_filter)
        return pairs

    def _build_qdrant_filter(self, where_filter: Optional[Dict[str, Any]]):
        if qdrant_models is None or not where_filter:
            return None
        pairs = self._flatten_where_pairs(where_filter)
        if not pairs:
            return None
        conditions = [
            qdrant_models.FieldCondition(
                key=key, match=qdrant_models.MatchValue(value=value)
            )
            for key, value in pairs
        ]
        if not conditions:
            return None
        return qdrant_models.Filter(must=conditions)

    def _ensure_qdrant_collection(self, name: str, vector_size: int) -> None:
        if self.qdrant_client is None or qdrant_models is None:
            raise RuntimeError("Backend Qdrant indisponible")
        if name in self._qdrant_known_collections:
            return
        try:
            info = self.qdrant_client.get_collection(name)
            if info:
                self._qdrant_known_collections[name] = vector_size or getattr(
                    getattr(info, "config", None), "vectors_count", 0
                )
                return
        except Exception:
            pass
        if vector_size <= 0:
            raise ValueError(
                "vector_size requis pour initialiser une collection Qdrant"
            )
        params = qdrant_models.VectorParams(
            size=vector_size, distance=qdrant_models.Distance.COSINE
        )
        try:
            self.qdrant_client.create_collection(
                collection_name=name, vectors_config=params
            )
            logger.info(f"Collection Qdrant '{name}' cr√©√©e (dim={vector_size}).")
        except Exception as e:
            if "exists" not in str(e).lower():
                logger.warning(f"Cr√©ation collection Qdrant '{name}' impossible: {e}")
            else:
                logger.info(f"Collection Qdrant '{name}' d√©j√† existante.")
        self._qdrant_known_collections[name] = vector_size

    def _qdrant_upsert(
        self,
        collection_name: str,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ) -> None:
        if self.qdrant_client is None or qdrant_models is None:
            raise RuntimeError("Backend Qdrant non initialis√©")
        if not embeddings:
            logger.warning("Tentative d'upsert Qdrant sans embeddings.")
            return
        vector_size = len(embeddings[0])
        self._ensure_qdrant_collection(collection_name, vector_size)

        points = []
        for idx, vector in enumerate(embeddings):
            payload = dict(metadatas[idx] or {}) if idx < len(metadatas) else {}
            text_value = documents[idx] if idx < len(documents) else None
            if text_value is not None:
                payload.setdefault("text", text_value)
            payload = {k: v for k, v in payload.items() if v is not None}
            point_id = ids[idx] if idx < len(ids) else uuid.uuid4().hex
            points.append(
                qdrant_models.PointStruct(id=point_id, vector=vector, payload=payload)
            )

        if not points:
            return

        self.qdrant_client.upsert(collection_name=collection_name, points=points)
        logger.info(f"{len(points)} vecteurs upsert√©s dans Qdrant '{collection_name}'.")

    def _qdrant_query(
        self,
        collection_name: str,
        query_vector: List[float],
        n_results: int,
        where_filter: Optional[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        if self.qdrant_client is None or qdrant_models is None:
            return []
        filter_obj = self._build_qdrant_filter(where_filter)
        try:
            results = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=max(1, n_results),
                with_payload=True,
                with_vectors=False,
                filter=filter_obj,
            )
        except Exception as e:
            logger.error(
                f"√âchec recherche Qdrant '{collection_name}': {e}", exc_info=True
            )
            return []

        formatted: List[Dict[str, Any]] = []
        for scored in results or []:
            payload = dict(scored.payload or {})
            text_value = payload.pop("text", None)
            formatted.append(
                {
                    "id": str(scored.id),
                    "text": text_value,
                    "metadata": payload,
                    "distance": scored.score,
                }
            )
        return formatted

    def _qdrant_delete(
        self,
        collection_name: str,
        where_filter: Optional[Dict[str, Any]],
        ids: Optional[List[str]] = None,
    ) -> None:
        if self.qdrant_client is None or qdrant_models is None:
            return
        if ids:
            selector = qdrant_models.PointIdsList(points=[str(i) for i in ids])
        else:
            filter_obj = self._build_qdrant_filter(where_filter)
            if not filter_obj:
                logger.warning(
                    f"Suppression Qdrant '{collection_name}' ignor√©e (aucun filtre)."
                )
                return
            selector = qdrant_models.FilterSelector(filter=filter_obj)
        try:
            self.qdrant_client.delete(
                collection_name=collection_name, points_selector=selector
            )
        except Exception as e:
            logger.error(
                f"√âchec suppression Qdrant '{collection_name}': {e}", exc_info=True
            )

    def _qdrant_get(
        self,
        collection_name: str,
        where_filter: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
    ) -> Dict[str, Any]:
        if self.qdrant_client is None or qdrant_models is None:
            return {"ids": [[]], "documents": [[]], "metadatas": [[]]}
        filter_obj = self._build_qdrant_filter(where_filter)
        fetched = []
        offset = None
        remaining = limit if limit is not None else None
        while True:
            batch_size = 128
            if remaining is not None:
                if remaining <= 0:
                    break
                batch_size = min(batch_size, remaining)
            try:
                page, offset = self.qdrant_client.scroll(
                    collection_name=collection_name,
                    limit=batch_size,
                    filter=filter_obj,
                    with_payload=True,
                    with_vectors=False,
                    offset=offset,
                )
            except Exception as e:
                logger.error(
                    f"√âchec scroll Qdrant '{collection_name}': {e}", exc_info=True
                )
                break
            if not page:
                break
            fetched.extend(page)
            if remaining is not None:
                remaining -= len(page)
                if remaining <= 0:
                    break
            if offset is None:
                break

        ids = [[str(r.id) for r in fetched]]
        documents = [[(r.payload or {}).get("text") for r in fetched]]
        metadatas = [
            [
                {k: v for k, v in (r.payload or {}).items() if k != "text"}
                for r in fetched
            ]
        ]
        return {"ids": ids, "documents": documents, "metadatas": metadatas}

    def _qdrant_delete_via_collection(
        self,
        collection_name: str,
        where_filter: Optional[Dict[str, Any]] = None,
        ids: Optional[List[str]] = None,
    ) -> None:
        self._qdrant_delete(collection_name, where_filter, ids)

    # ---------- API publique ----------
    def get_or_create_collection(
        self, name: str, metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Get or create a ChromaDB collection with optimized HNSW parameters.

        Args:
            name: Collection name
            metadata: Optional collection metadata (HNSW config, etc.)
                     Default: Optimized for LTM queries (M=16, space=cosine)

        Returns:
            Collection object (ChromaDB or QdrantCollectionAdapter)
        """
        self._ensure_inited()
        if self.backend == "qdrant":
            return QdrantCollectionAdapter(self, name)

        # Default optimized metadata for LTM collections (P2 performance)
        if metadata is None:
            metadata = {
                "hnsw:space": "cosine",  # Cosine similarity (standard for embeddings)
                "hnsw:M": 16,  # Connections per node (balance precision/speed)
                # Note: ChromaDB v0.4+ auto-optimizes metadata filters (user_id, type, confidence)
                # No explicit index creation needed
            }

        embedding_fn = self._build_collection_embedding_function()

        try:
            collection = self.client.get_or_create_collection(  # type: ignore[union-attr]
                name=name,
                metadata=metadata,
                embedding_function=embedding_fn,
            )
            # Certaines versions de Chroma ignorent embedding_function si collection existante
            try:
                setattr(collection, "_embedding_function", embedding_fn)
            except Exception:
                pass
            logger.info(
                f"Collection '{name}' charg√©e/cr√©√©e avec HNSW optimis√© "
                f"(M={metadata.get('hnsw:M', 'default')}, space={metadata.get('hnsw:space', 'default')})"
            )
            return collection
        except Exception as e:
            logger.error(
                f"Impossible de get/create la collection '{name}': {e}", exc_info=True
            )
            raise

    def add_items(
        self, collection, items: List[Dict[str, Any]], item_text_key: str = "text"
    ) -> None:
        self._ensure_inited()
        collection_name = getattr(collection, "name", str(collection))
        self._check_write_allowed("vector_upsert", collection_name)
        if not items:
            logger.warning(f"Tentative d'ajout d'items vides √† '{collection.name}'.")
            return
        try:
            ids = [item["id"] for item in items]
            documents_text = [item[item_text_key] for item in items]

            # ChromaDB only accepts str, int, float, bool in metadata - filter out invalid types
            # Filter out None, lists, dicts, and any other non-primitive types
            metadatas = [
                {
                    k: v
                    for k, v in item.get("metadata", {}).items()
                    if isinstance(v, (str, int, float, bool))
                }
                for item in items
            ]

            precomputed_embeddings: List[List[float]] = []
            use_precomputed = True
            for item in items:
                embedding = item.get("embedding")
                if embedding is None:
                    use_precomputed = False
                    break
                precomputed_embeddings.append(list(embedding))

            if use_precomputed:
                embeddings_list = precomputed_embeddings
            else:
                embeddings = self.model.encode(documents_text, show_progress_bar=False)  # type: ignore[union-attr]
                embeddings_list = (
                    embeddings.tolist() if hasattr(embeddings, "tolist") else embeddings
                )

            if self.backend == "qdrant":
                collection_name = getattr(collection, "name", str(collection))
                self._qdrant_upsert(
                    collection_name, ids, embeddings_list, documents_text, metadatas
                )
            else:
                collection.upsert(
                    embeddings=embeddings_list,
                    documents=documents_text,
                    metadatas=metadatas,
                    ids=ids,
                )
                logger.info(
                    f"{len(ids)} items ajout√©s/mis √† jour dans '{collection.name}'."
                )
        except Exception as e:
            logger.error(
                f"√âchec de l'ajout d'items √† '{collection.name}': {e}", exc_info=True
            )
            raise

    def query(
        self,
        collection,
        query_text: str,
        n_results: int = 5,
        where_filter: Optional[Dict[str, Any]] = None,
        apply_recency: bool = True,  # P2.2 - Enable recency decay
        apply_mmr: bool = True,  # P2.2 - Enable MMR diversity
        recency_half_life: float = 90.0,  # P2.2 - Half-life in days
        mmr_lambda: float = 0.7,  # P2.2 - MMR balance (0.7 = 70% relevance, 30% diversity)
        apply_specificity_boost: bool = True,  # P2.1 - Enable specificity scoring
        apply_rerank: bool = True,  # P2.1 - Enable lexical rerank
    ) -> List[Dict[str, Any]]:
        """
        Recherche vectorielle avec support optionnel de recency decay, MMR, specificity boost et rerank.

        Args:
            collection: Collection Chroma/Qdrant
            query_text: Texte de la requ√™te
            n_results: Nombre de r√©sultats souhait√©s
            where_filter: Filtre de m√©tadonn√©es (optionnel)
            apply_recency: Appliquer la d√©croissance temporelle (d√©faut: True)
            apply_mmr: Appliquer MMR pour diversit√© (d√©faut: True)
            recency_half_life: Demi-vie pour recency decay en jours (d√©faut: 90)
            mmr_lambda: Balance MMR (1.0 = full relevance, 0.0 = full diversity)
            apply_specificity_boost: Appliquer boost sp√©cificit√© (densit√© IDF/NER/nombres) (d√©faut: True)
            apply_rerank: Appliquer rerank lexical avec Jaccard (d√©faut: True)

        Returns:
            Liste de r√©sultats avec {id, text, metadata, distance, [age_days, recency_score, specificity_score, rerank_score]}
        """
        self._ensure_inited()
        if not query_text:
            return []
        try:
            embeddings = self.model.encode([query_text], show_progress_bar=False)  # type: ignore[union-attr]
            embeddings_list = (
                embeddings.tolist() if hasattr(embeddings, "tolist") else embeddings
            )
            query_embedding = embeddings_list[0] if embeddings_list else []

            if self.backend == "qdrant":
                collection_name = getattr(collection, "name", str(collection))
                raw_results = self._qdrant_query(
                    collection_name,
                    query_embedding,
                    n_results * 2,
                    where_filter,  # Fetch more for MMR
                )
            else:
                results = collection.query(
                    query_embeddings=embeddings_list,
                    n_results=n_results * 2,  # Fetch more candidates for MMR filtering
                    where=self._normalize_where(where_filter),
                    include=[
                        "documents",
                        "metadatas",
                        "distances",
                        "embeddings",
                    ],  # Need embeddings for MMR
                )

                raw_results: List[Dict[str, Any]] = []
                if results and results.get("ids") and results["ids"][0]:
                    ids = results["ids"][0]
                    docs = results.get("documents", [[]])[0]
                    metas = results.get("metadatas", [[]])[0]
                    dists = results.get("distances", [[]])[0]
                    embeds = (
                        results.get("embeddings", [[]])[0]
                        if "embeddings" in results
                        else [[]] * len(ids)
                    )
                    for i, doc_id in enumerate(ids):
                        # Fix: Avoid ambiguous truth check on numpy array
                        embed_value = embeds[i] if i < len(embeds) else None
                        use_embed = embed_value is not None and (
                            not hasattr(embed_value, "__len__") or len(embed_value) > 0
                        )
                        raw_results.append(
                            {
                                "id": doc_id,
                                "text": docs[i] if i < len(docs) else None,
                                "metadata": metas[i] if i < len(metas) else None,
                                "distance": dists[i] if i < len(dists) else None,
                                "embedding": embed_value
                                if use_embed
                                else query_embedding,
                            }
                        )

            # P2.2 - Apply recency decay if enabled
            if apply_recency and raw_results:
                from datetime import datetime, timezone

                now = datetime.now(timezone.utc)
                for result in raw_results:
                    meta = result.get("metadata") or {}
                    ts_str = meta.get("ts") or meta.get("timestamp")
                    if ts_str:
                        try:
                            # Parse timestamp (ISO format expected)
                            ts = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
                            age_days = (now - ts).total_seconds() / 86400
                            recency_score = recency_decay(
                                age_days, half_life=recency_half_life
                            )
                            result["age_days"] = round(age_days, 1)
                            result["recency_score"] = round(recency_score, 3)
                            # Adjust distance by recency (lower distance = better match)
                            # We boost relevance by dividing distance by recency factor
                            # (capped to avoid division by zero)
                            if result.get("distance") is not None:
                                recency_factor = max(recency_score, 0.1)  # Floor at 0.1
                                result["distance"] = result["distance"] / recency_factor
                        except Exception:
                            pass  # Skip recency for malformed timestamps

            # Sort by adjusted distance (lower = better)
            raw_results.sort(key=lambda x: x.get("distance", float("inf")))

            # P2.1 - Apply specificity boost if enabled
            if apply_specificity_boost and raw_results:
                specificity_weight = float(os.getenv("RAG_SPECIFICITY_WEIGHT", "0.15"))
                collection_name = getattr(collection, "name", "unknown")

                for result in raw_results:
                    text = result.get("text", "")
                    distance = result.get("distance", 1.0)

                    # Calculer score de sp√©cificit√©
                    specificity_score = compute_specificity_score(text)
                    result["specificity_score"] = round(specificity_score, 4)

                    # üìä Enregistrer m√©trique Prometheus
                    try:
                        from backend.features.memory.rag_metrics import (
                            memory_rag_precision_score,
                        )

                        memory_rag_precision_score.labels(
                            collection=collection_name, metric_type="specificity"
                        ).observe(specificity_score)
                    except Exception:
                        pass  # Graceful degradation si Prometheus indisponible

                    # Combiner avec score cosine
                    # Distance ChromaDB ‚Üí Score cosine : cosine = 1 - (distance / 2)
                    cosine_score = max(0.0, 1.0 - (distance / 2.0))

                    # Score final combin√© : weighted average
                    # final_score = (1 - weight) * cosine + weight * specificity
                    combined_score = (
                        1 - specificity_weight
                    ) * cosine_score + specificity_weight * specificity_score

                    # üìä Enregistrer score combin√©
                    try:
                        memory_rag_precision_score.labels(
                            collection=collection_name, metric_type="combined"
                        ).observe(combined_score)
                    except Exception:
                        pass

                    # Convertir score en distance (pour tri) : distance = 2 * (1 - score)
                    result["distance"] = 2.0 * (1.0 - combined_score)
                    result["combined_score"] = round(combined_score, 4)

                # Re-trier par distance ajust√©e
                raw_results.sort(key=lambda x: x.get("distance", float("inf")))

            # P2.1 - Apply lexical rerank if enabled
            if apply_rerank and len(raw_results) > 1:
                rerank_topk = int(os.getenv("RAG_RERANK_TOPK", "8"))
                collection_name = getattr(collection, "name", "unknown")
                # Reranker avec top-k initial (on prend plus que n_results pour avoir du choix pour MMR)
                fetch_for_rerank = max(n_results * 2, rerank_topk * 2)
                candidates_for_rerank = raw_results[:fetch_for_rerank]

                raw_results = rerank_with_lexical_overlap(
                    query=query_text,
                    results=candidates_for_rerank,
                    topk=min(
                        len(candidates_for_rerank), rerank_topk * 2
                    ),  # Garder assez pour MMR
                    cosine_weight=0.7,
                    lexical_weight=0.3,
                )

                # üìä Enregistrer m√©triques jaccard pour r√©sultats reranked
                try:
                    from backend.features.memory.rag_metrics import (
                        memory_rag_precision_score,
                    )

                    for r in raw_results[:5]:  # Top-5 pour √©viter trop de m√©triques
                        jaccard = r.get("jaccard_score", 0.0)
                        memory_rag_precision_score.labels(
                            collection=collection_name, metric_type="jaccard"
                        ).observe(jaccard)
                except Exception:
                    pass

            # P2.2 - Apply MMR if enabled
            if apply_mmr and len(raw_results) > 1:
                # MMR needs embeddings - ensure they're present
                final_results = mmr(
                    query_embedding=query_embedding,
                    candidates=raw_results,
                    k=n_results,
                    lambda_param=mmr_lambda,
                )
            else:
                final_results = raw_results[:n_results]

            # Clean up: remove embeddings from final results (internal use only)
            for res in final_results:
                res.pop("embedding", None)

            return final_results

        except Exception as e:
            safe_q = (query_text or "")[:50]
            logger.error(
                f"√âchec de la recherche '{safe_q}‚Ä¶' dans '{collection.name}': {e}",
                exc_info=True,
            )
            return []

    def update_metadatas(
        self, collection: Collection, ids: List[str], metadatas: List[Dict[str, Any]]
    ) -> None:
        self._ensure_inited()
        collection_name = getattr(collection, "name", str(collection))
        self._check_write_allowed("metadata_update", collection_name)
        if not ids:
            return
        if len(ids) != len(metadatas):
            logger.warning(
                "update_metadatas: taille ids/metadatas incoherente - abandon."
            )
            return
        try:
            collection.update(ids=ids, metadatas=metadatas)
            logger.info(
                f"Metadatas mises a jour pour {len(ids)} items dans '{collection.name}'."
            )
        except Exception as e:
            logger.warning(
                f"Echec update metadatas '{collection.name}': {e}", exc_info=True
            )

    def _is_filter_empty(self, where_filter: Dict[str, Any]) -> bool:
        """V√©rifie r√©cursivement si un filtre est vide ou sans crit√®res valides."""
        if not where_filter:
            return True

        # V√©rifier op√©rateurs logiques ($and, $or, $not)
        for op in ["$and", "$or"]:
            if op in where_filter:
                values = where_filter[op]
                if isinstance(values, list):
                    # Liste vide ‚Üí filtre vide
                    if not values:
                        return True
                    # Si toutes les sous-conditions sont vides ‚Üí filtre vide
                    if all(
                        self._is_filter_empty(v) if isinstance(v, dict) else False
                        for v in values
                    ):
                        return True

        # V√©rifier si toutes les valeurs sont None
        non_operator_keys = [k for k in where_filter.keys() if not k.startswith("$")]
        if non_operator_keys and all(
            where_filter[k] is None for k in non_operator_keys
        ):
            return True

        return False

    def delete_vectors(
        self, collection: Collection, where_filter: Dict[str, Any]
    ) -> None:
        self._ensure_inited()
        collection_name = getattr(collection, "name", str(collection))
        self._check_write_allowed("vector_delete", collection_name)
        if self._is_filter_empty(where_filter):
            logger.error(
                f"[VectorService] Suppression refus√©e sur '{collection.name}': "
                f"filtre vide ou invalide (protection suppression globale)"
            )
            raise ValueError(
                "Cannot delete with empty or invalid filter (global deletion protection)"
            )
        try:
            if self.backend == "qdrant":
                collection_name = getattr(collection, "name", str(collection))
                self._qdrant_delete(collection_name, where_filter)
            else:
                collection.delete(where=self._normalize_where(where_filter))
                logger.info(
                    f"Vecteurs supprim√©s de '{collection.name}' avec filtre {where_filter}."
                )
        except Exception as e:
            logger.error(
                f"√âchec suppression vecteurs dans '{collection.name}': {e}",
                exc_info=True,
            )
            raise

    # ---------- Recherche hybride BM25 + Vectorielle (P1.5) ----------
    def hybrid_query(
        self,
        collection,
        query_text: str,
        n_results: int = 5,
        where_filter: Optional[Dict[str, Any]] = None,
        alpha: float = 0.5,
        score_threshold: float = 0.0,
        bm25_k1: float = 1.5,
        bm25_b: float = 0.75,
    ) -> List[Dict[str, Any]]:
        """
        Recherche hybride combinant BM25 (lexical) et vectorielle (s√©mantique).

        Args:
            collection: Collection Chroma/Qdrant
            query_text: Requ√™te utilisateur
            n_results: Nombre de r√©sultats finaux
            where_filter: Filtres de m√©tadonn√©es (optionnel)
            alpha: Poids du scoring vectoriel (0.0 = full BM25, 1.0 = full vector)
            score_threshold: Seuil minimum de score pour retourner un r√©sultat
            bm25_k1: Param√®tre k1 de BM25 (saturation TF)
            bm25_b: Param√®tre b de BM25 (normalisation longueur)

        Returns:
            Liste de r√©sultats avec scores hybrides d√©taill√©s
        """
        try:
            from backend.features.memory.hybrid_retriever import (
                hybrid_query as _hybrid_query,
            )

            return _hybrid_query(
                vector_service=self,
                collection=collection,
                query_text=query_text,
                n_results=n_results,
                where_filter=where_filter,
                alpha=alpha,
                score_threshold=score_threshold,
                bm25_k1=bm25_k1,
                bm25_b=bm25_b,
            )
        except ImportError:
            logger.warning(
                "HybridRetriever non disponible, fallback sur query() classique"
            )
            return self.query(collection, query_text, n_results, where_filter)
        except Exception as e:
            logger.error(f"Erreur hybrid_query: {e}", exc_info=True)
            # Fallback sur query vectorielle standard
            return self.query(collection, query_text, n_results, where_filter)

    # ---------- Weighted Retrieval avec d√©croissance temporelle (P2.3) ----------
    def query_weighted(
        self,
        collection,
        query_text: str,
        n_results: Optional[int] = None,
        where_filter: Optional[Dict[str, Any]] = None,
        lambda_: Optional[float] = None,
        alpha: Optional[float] = None,
        score_threshold: Optional[float] = None,
        enable_trace: Optional[bool] = None,
        update_metadata: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        Recherche vectorielle avec scoring pond√©r√© combinant similarit√©, fra√Æcheur et fr√©quence d'utilisation.

        Formule : score = cosine_sim √ó exp(-Œª √ó Œît) √ó (1 + Œ± √ó freq)

        o√π :
        - cosine_sim : similarit√© entre query et m√©moire
        - Œît : jours depuis last_used_at
        - freq : use_count (nombre de r√©cup√©rations)
        - Œª : taux de d√©croissance temporelle
        - Œ± : facteur de renforcement par fr√©quence

        Args:
            collection: Collection Chroma/Qdrant
            query_text: Texte de requ√™te
            n_results: Nombre de r√©sultats (d√©faut: memory_config.top_k)
            where_filter: Filtre de m√©tadonn√©es
            lambda_: Taux d√©croissance (d√©faut: memory_config.decay_lambda)
            alpha: Facteur renforcement (d√©faut: memory_config.reinforcement_alpha)
            score_threshold: Seuil minimum de score (d√©faut: memory_config.score_threshold)
            enable_trace: Active logs de trace (d√©faut: memory_config.enable_trace_logging)
            update_metadata: Met √† jour last_used_at et use_count (d√©faut: True)

        Returns:
            Liste de r√©sultats avec {id, text, metadata, distance, weighted_score, [trace_info]}

        Example:
            >>> results = vector_service.query_weighted(
            ...     collection=knowledge_collection,
            ...     query_text="CI/CD pipeline",
            ...     n_results=5
            ... )
            >>> for r in results:
            ...     print(f"{r['text']}: score={r['weighted_score']:.3f}")
        """
        self._ensure_inited()

        # Charger param√®tres depuis config si non sp√©cifi√©s
        lambda_ = lambda_ if lambda_ is not None else self.memory_config.decay_lambda
        alpha = alpha if alpha is not None else self.memory_config.reinforcement_alpha
        n_results = n_results if n_results is not None else self.memory_config.top_k
        score_threshold = (
            score_threshold
            if score_threshold is not None
            else self.memory_config.score_threshold
        )
        enable_trace = (
            enable_trace
            if enable_trace is not None
            else self.memory_config.enable_trace_logging
        )

        if not query_text:
            return []

        import time

        query_start = time.time()

        try:
            # 1. R√©cup√©rer candidats avec query standard (fetch plus pour re-ranking)
            fetch_size = max(n_results * 3, 20)
            raw_results = self.query(
                collection=collection,
                query_text=query_text,
                n_results=fetch_size,
                where_filter=where_filter,
                apply_recency=False,  # D√©sactiver recency decay standard (on utilise notre propre scoring)
                apply_mmr=False,  # D√©sactiver MMR (on trie par weighted_score)
            )

            if not raw_results:
                return []

            # 2. Calculer weighted scores pour chaque r√©sultat
            from datetime import datetime, timezone

            now = datetime.now(timezone.utc)
            weighted_results = []
            collection_name = getattr(collection, "name", "unknown")

            for res in raw_results:
                meta = res.get("metadata", {})
                entry_id = res.get("id", "unknown")

                # Extraire m√©tadonn√©es de retrieval
                last_used_str = meta.get("last_used_at") or ""
                use_count = int(meta.get("use_count", 0))

                # üÜï V√©rifier cache d'abord
                cached_score = self.score_cache.get(query_text, entry_id, last_used_str)
                if cached_score is not None:
                    # Cache hit ‚Üí utiliser score cach√©
                    weighted_score = cached_score
                    cosine_sim = 0.0  # Pas recalcul√© (pas besoin)
                    delta_days = 0.0
                else:
                    # Cache miss ‚Üí calculer score
                    score_start = time.time()

                    # Calculer Œît (jours depuis last_used_at)
                    if last_used_str:
                        try:
                            last_used = datetime.fromisoformat(
                                last_used_str.replace("Z", "+00:00")
                            )
                            delta_days = (now - last_used).total_seconds() / 86400
                        except Exception:
                            # Si parsing √©choue, consid√©rer comme jamais utilis√© (tr√®s ancien)
                            delta_days = 999.0
                    else:
                        # Jamais utilis√© ‚Üí tr√®s ancien
                        delta_days = 999.0

                    # Calculer similarit√© cosine depuis distance
                    # ChromaDB L2 squared distance for normalized vectors: distance = 2 * (1 - cosine_sim)
                    distance = res.get("distance", 2.0)
                    cosine_sim = 1.0 - (distance / 2.0)

                    # Calculer score pond√©r√©
                    weighted_score = compute_memory_score(
                        cosine_sim=cosine_sim,
                        delta_days=delta_days,
                        freq=use_count,
                        lambda_=lambda_,
                        alpha=alpha,
                    )

                    # üÜï Stocker dans cache
                    self.score_cache.set(
                        query_text, entry_id, last_used_str, weighted_score
                    )

                    # üÜï M√©triques scoring
                    score_duration = time.time() - score_start
                    self.metrics.record_score(
                        collection_name, weighted_score, score_duration
                    )
                    self.metrics.record_entry_age(collection_name, delta_days)
                    self.metrics.record_use_count(collection_name, use_count)

                # Appliquer seuil
                if weighted_score < score_threshold:
                    continue

                # Enrichir r√©sultat avec score et trace
                res["weighted_score"] = round(weighted_score, 4)
                res["cosine_sim"] = round(cosine_sim, 4)

                if enable_trace:
                    res["trace_info"] = {
                        "cosine_sim": round(cosine_sim, 4),
                        "delta_days": round(delta_days, 1),
                        "use_count": use_count,
                        "lambda": lambda_,
                        "alpha": alpha,
                        "weighted_score": round(weighted_score, 4),
                    }
                    logger.debug(
                        f"[Memory] Entry {res.get('id', 'unknown')[:8]}: "
                        f"sim={cosine_sim:.3f} | Œît={delta_days:.1f}j | freq={use_count} | score={weighted_score:.3f}"
                    )

                weighted_results.append(res)

            # 3. Trier par weighted_score d√©croissant
            weighted_results.sort(
                key=lambda x: x.get("weighted_score", 0.0), reverse=True
            )

            # 4. Conserver top_k
            final_results = weighted_results[:n_results]

            # 5. Mettre √† jour m√©tadonn√©es de retrieval (last_used_at, use_count)
            if update_metadata and final_results:
                self._update_retrieval_metadata(
                    collection=collection,
                    results=final_results,
                )

            score_info = (
                f", score_min={final_results[-1]['weighted_score']:.3f}"
                if final_results
                else ""
            )
            logger.info(
                f"[VectorService] Weighted query '{query_text[:30]}...': "
                f"{len(final_results)} r√©sultats{score_info}"
            )

            # üÜï M√©triques requ√™te
            query_duration = time.time() - query_start
            self.metrics.record_query(
                collection=collection_name,
                status="success",
                results_count=len(final_results),
                duration_seconds=query_duration,
            )

            return final_results

        except Exception as e:
            safe_q = (query_text or "")[:50]
            logger.error(
                f"√âchec de la recherche pond√©r√©e '{safe_q}‚Ä¶' dans '{collection.name}': {e}",
                exc_info=True,
            )

            # üÜï M√©triques erreur
            collection_name = (
                getattr(collection, "name", "unknown") if collection else "unknown"
            )
            self.metrics.record_query(
                collection=collection_name,
                status="error",
                results_count=0,
                duration_seconds=time.time() - query_start,
            )

            return []

    def _update_retrieval_metadata(
        self,
        collection,
        results: List[Dict[str, Any]],
    ) -> None:
        """
        Met √† jour les m√©tadonn√©es de retrieval (last_used_at, use_count) pour les entr√©es r√©cup√©r√©es.

        Args:
            collection: Collection Chroma/Qdrant
            results: Liste de r√©sultats avec {id, metadata, ...}

        Returns:
            None
        """
        if not results:
            return

        import time

        update_start = time.time()

        try:
            from datetime import datetime, timezone

            now = datetime.now(timezone.utc).isoformat()
            collection_name = getattr(collection, "name", "unknown")

            ids = []
            updated_metadatas = []

            for res in results:
                entry_id = res.get("id")
                if not entry_id:
                    continue

                meta = res.get("metadata", {})
                current_use_count = int(meta.get("use_count", 0))

                # Mise √† jour
                new_meta = dict(meta)
                new_meta["last_used_at"] = now
                new_meta["use_count"] = current_use_count + 1

                # Filter out invalid types for ChromaDB (only str, int, float, bool)
                new_meta = {
                    k: v
                    for k, v in new_meta.items()
                    if isinstance(v, (str, int, float, bool))
                }

                ids.append(entry_id)
                updated_metadatas.append(new_meta)

                # üÜï Invalider cache pour cette entr√©e (m√©tadonn√©es chang√©es)
                self.score_cache.invalidate(entry_id)

            if ids:
                self.update_metadatas(
                    collection=collection,
                    ids=ids,
                    metadatas=updated_metadatas,
                )
                logger.debug(
                    f"[VectorService] Metadatas de retrieval mis √† jour pour {len(ids)} entr√©es"
                )

                # üÜï M√©triques metadata update
                update_duration = time.time() - update_start
                self.metrics.record_metadata_update(collection_name, update_duration)

        except Exception as e:
            logger.warning(
                f"√âchec mise √† jour retrieval metadata dans '{collection.name}': {e}",
                exc_info=True,
            )
