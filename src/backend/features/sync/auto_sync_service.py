"""
Service de synchronisation automatique inter-agents.

Option A - Synchronisation automatique complÃ¨te :
- DÃ©tection automatique des changements (file watchers)
- Consolidation intelligente (triggers basÃ©s sur seuils)
- Scheduler en arriÃ¨re-plan (tÃ¢ches pÃ©riodiques)
- Monitoring du statut de sync (mÃ©triques + logs)
"""

import asyncio
import hashlib
import json
import logging
from collections.abc import Callable
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from prometheus_client import Counter, Gauge, Histogram

logger = logging.getLogger(__name__)


# ============================================================================
# MÃ‰TRIQUES PROMETHEUS
# ============================================================================

sync_changes_detected = Counter(
    "sync_changes_detected_total",
    "Nombre de changements dÃ©tectÃ©s dans les fichiers surveillÃ©s",
    ["file_type", "agent"],
)

sync_consolidations_triggered = Counter(
    "sync_consolidations_triggered_total",
    "Nombre de consolidations dÃ©clenchÃ©es",
    ["trigger_type"],
)

sync_status = Gauge(
    "sync_status",
    "Statut de synchronisation (1=synced, 0=out_of_sync, -1=error)",
    ["file_path"],
)

sync_check_duration = Histogram(
    "sync_check_duration_seconds",
    "DurÃ©e des vÃ©rifications de synchronisation",
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],
)

sync_consolidation_duration = Histogram(
    "sync_consolidation_duration_seconds",
    "DurÃ©e des consolidations",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0],
)


# ============================================================================
# DATA MODELS
# ============================================================================


@dataclass
class FileChecksum:
    """Checksum d'un fichier surveillÃ©."""

    path: str
    checksum: str
    last_modified: datetime
    agent_owner: str | None = None


@dataclass
class SyncEvent:
    """Ã‰vÃ©nement de changement dÃ©tectÃ©."""

    file_path: str
    event_type: str  # "modified", "created", "deleted"
    timestamp: datetime
    old_checksum: str | None = None
    new_checksum: str | None = None
    agent_owner: str | None = None


@dataclass
class ConsolidationTrigger:
    """DÃ©clencheur de consolidation."""

    trigger_type: str  # "threshold", "time_based", "manual"
    conditions_met: dict[str, Any]
    timestamp: datetime


# ============================================================================
# AUTO SYNC SERVICE
# ============================================================================


class AutoSyncService:
    """Service de synchronisation automatique inter-agents."""

    def __init__(
        self,
        repo_root: Path,
        check_interval_seconds: int = 30,
        consolidation_threshold: int = 5,
        consolidation_interval_minutes: int = 60,
    ):
        """
        Initialize AutoSyncService.

        Args:
            repo_root: Racine du dÃ©pÃ´t Git
            check_interval_seconds: Intervalle de vÃ©rification des changements (dÃ©faut: 30s)
            consolidation_threshold: Nombre de changements avant consolidation (dÃ©faut: 5)
            consolidation_interval_minutes: Intervalle min entre consolidations (dÃ©faut: 60min)
        """
        self.repo_root = repo_root
        self.check_interval = check_interval_seconds
        self.consolidation_threshold = consolidation_threshold
        self.consolidation_interval = timedelta(minutes=consolidation_interval_minutes)

        # Fichiers critiques Ã  surveiller (relatifs Ã  repo_root)
        self.watched_files = [
            "AGENT_SYNC.md",
            "docs/passation.md",
            "AGENTS.md",
            "CODEV_PROTOCOL.md",
            "docs/architecture/00-Overview.md",
            "docs/architecture/10-Memoire.md",
            "docs/architecture/30-Contracts.md",
            "ROADMAP.md",
        ]

        # Ã‰tat interne
        self.checksums: dict[str, FileChecksum] = {}
        self.pending_changes: list[SyncEvent] = []
        self.last_consolidation: datetime | None = None
        self.consolidation_callbacks: list[Callable[[ConsolidationTrigger], None]] = []

        # TÃ¢ches asyncio
        self._running = False
        self._check_task: asyncio.Task[None] | None = None
        self._consolidation_task: asyncio.Task[None] | None = None

    # ========================================================================
    # LIFECYCLE
    # ========================================================================

    async def start(self) -> None:
        """DÃ©marre le service de synchronisation automatique."""
        if self._running:
            logger.warning("AutoSyncService already running")
            return

        logger.info(
            "Starting AutoSyncService (check_interval=%ds, threshold=%d, consolidation_interval=%dmin)",
            self.check_interval,
            self.consolidation_threshold,
            self.consolidation_interval.total_seconds() / 60,
        )

        # Initialiser les checksums
        await self._initialize_checksums()

        # DÃ©marrer les tÃ¢ches en arriÃ¨re-plan
        self._running = True
        self._check_task = asyncio.create_task(self._check_loop())
        self._consolidation_task = asyncio.create_task(self._consolidation_loop())

        logger.info("AutoSyncService started successfully")

    async def stop(self) -> None:
        """ArrÃªte le service de synchronisation."""
        if not self._running:
            return

        logger.info("Stopping AutoSyncService...")
        self._running = False

        # Annuler les tÃ¢ches
        if self._check_task:
            self._check_task.cancel()
            try:
                await self._check_task
            except asyncio.CancelledError:
                pass

        if self._consolidation_task:
            self._consolidation_task.cancel()
            try:
                await self._consolidation_task
            except asyncio.CancelledError:
                pass

        logger.info("AutoSyncService stopped")

    # ========================================================================
    # FILE WATCHING
    # ========================================================================

    async def _initialize_checksums(self) -> None:
        """Initialise les checksums de tous les fichiers surveillÃ©s."""
        logger.info(
            "Initializing checksums for %d watched files", len(self.watched_files)
        )

        for rel_path in self.watched_files:
            file_path = self.repo_root / rel_path
            if file_path.exists():
                checksum = await self._compute_checksum(file_path)
                agent_owner = self._detect_agent_owner(rel_path)

                self.checksums[rel_path] = FileChecksum(
                    path=rel_path,
                    checksum=checksum,
                    last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
                    agent_owner=agent_owner,
                )

                sync_status.labels(file_path=rel_path).set(1)  # synced
            else:
                logger.warning("Watched file not found: %s", rel_path)
                sync_status.labels(file_path=rel_path).set(-1)  # error

    async def _compute_checksum(self, file_path: Path) -> str:
        """Calcule le checksum MD5 d'un fichier."""
        md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                md5.update(chunk)
        return md5.hexdigest()

    def _detect_agent_owner(self, rel_path: str) -> str | None:
        """DÃ©tecte l'agent propriÃ©taire d'un fichier (basÃ© sur le dernier commit)."""
        # Simplification : on pourrait parser git blame ou git log
        # Pour l'instant, on retourne None
        return None

    async def _check_loop(self) -> None:
        """Boucle de vÃ©rification pÃ©riodique des changements."""
        while self._running:
            try:
                await asyncio.sleep(self.check_interval)

                with sync_check_duration.time():
                    await self._check_for_changes()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Error in check loop: %s", e, exc_info=True)

    async def _check_for_changes(self) -> None:
        """VÃ©rifie les changements dans les fichiers surveillÃ©s."""
        for rel_path in self.watched_files:
            file_path = self.repo_root / rel_path

            if not file_path.exists():
                # Fichier supprimÃ©
                if rel_path in self.checksums:
                    event = SyncEvent(
                        file_path=rel_path,
                        event_type="deleted",
                        timestamp=datetime.now(),
                        old_checksum=self.checksums[rel_path].checksum,
                        new_checksum=None,
                        agent_owner=self.checksums[rel_path].agent_owner,
                    )
                    self.pending_changes.append(event)
                    del self.checksums[rel_path]

                    sync_status.labels(file_path=rel_path).set(0)  # out_of_sync
                    sync_changes_detected.labels(
                        file_type=self._get_file_type(rel_path), agent="unknown"
                    ).inc()

                    logger.warning("File deleted: %s", rel_path)
                continue

            # Calculer nouveau checksum
            new_checksum = await self._compute_checksum(file_path)

            if rel_path not in self.checksums:
                # Fichier crÃ©Ã©
                event = SyncEvent(
                    file_path=rel_path,
                    event_type="created",
                    timestamp=datetime.now(),
                    old_checksum=None,
                    new_checksum=new_checksum,
                    agent_owner=self._detect_agent_owner(rel_path),
                )
                self.pending_changes.append(event)

                self.checksums[rel_path] = FileChecksum(
                    path=rel_path,
                    checksum=new_checksum,
                    last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
                    agent_owner=event.agent_owner,
                )

                sync_status.labels(file_path=rel_path).set(0)  # out_of_sync
                sync_changes_detected.labels(
                    file_type=self._get_file_type(rel_path), agent="unknown"
                ).inc()

                logger.info("File created: %s", rel_path)

            elif new_checksum != self.checksums[rel_path].checksum:
                # Fichier modifiÃ©
                event = SyncEvent(
                    file_path=rel_path,
                    event_type="modified",
                    timestamp=datetime.now(),
                    old_checksum=self.checksums[rel_path].checksum,
                    new_checksum=new_checksum,
                    agent_owner=self._detect_agent_owner(rel_path),
                )
                self.pending_changes.append(event)

                self.checksums[rel_path] = FileChecksum(
                    path=rel_path,
                    checksum=new_checksum,
                    last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
                    agent_owner=event.agent_owner,
                )

                sync_status.labels(file_path=rel_path).set(0)  # out_of_sync
                sync_changes_detected.labels(
                    file_type=self._get_file_type(rel_path),
                    agent=event.agent_owner or "unknown",
                ).inc()

                old_chk_display = (
                    event.old_checksum[:8] if event.old_checksum else "None"
                )
                logger.info(
                    "File modified: %s (checksum: %s -> %s)",
                    rel_path,
                    old_chk_display,
                    new_checksum[:8],
                )

    def _get_file_type(self, rel_path: str) -> str:
        """Retourne le type de fichier pour les mÃ©triques."""
        if rel_path.endswith(".md"):
            if "architecture" in rel_path:
                return "architecture"
            if "passation" in rel_path:
                return "passation"
            if rel_path == "AGENT_SYNC.md":
                return "sync"
            return "docs"
        return "other"

    # ========================================================================
    # CONSOLIDATION
    # ========================================================================

    async def _consolidation_loop(self) -> None:
        """Boucle de vÃ©rification des triggers de consolidation."""
        while self._running:
            try:
                await asyncio.sleep(60)  # VÃ©rifier toutes les minutes

                await self._check_consolidation_triggers()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Error in consolidation loop: %s", e, exc_info=True)

    async def _check_consolidation_triggers(self) -> None:
        """VÃ©rifie si une consolidation doit Ãªtre dÃ©clenchÃ©e."""
        # Trigger 1 : Seuil de changements atteint
        if len(self.pending_changes) >= self.consolidation_threshold:
            trigger = ConsolidationTrigger(
                trigger_type="threshold",
                conditions_met={
                    "pending_changes": len(self.pending_changes),
                    "threshold": self.consolidation_threshold,
                },
                timestamp=datetime.now(),
            )
            await self._trigger_consolidation(trigger)
            return

        # Trigger 2 : Intervalle de temps Ã©coulÃ©
        if self.last_consolidation:
            time_since_last = datetime.now() - self.last_consolidation
            if (
                time_since_last >= self.consolidation_interval
                and len(self.pending_changes) > 0
            ):
                trigger = ConsolidationTrigger(
                    trigger_type="time_based",
                    conditions_met={
                        "pending_changes": len(self.pending_changes),
                        "time_since_last_minutes": time_since_last.total_seconds() / 60,
                    },
                    timestamp=datetime.now(),
                )
                await self._trigger_consolidation(trigger)

    async def _trigger_consolidation(self, trigger: ConsolidationTrigger) -> None:
        """DÃ©clenche une consolidation."""
        logger.info(
            "Triggering consolidation (type=%s, conditions=%s, pending_changes=%d)",
            trigger.trigger_type,
            trigger.conditions_met,
            len(self.pending_changes),
        )

        with sync_consolidation_duration.time():
            # Appeler les callbacks enregistrÃ©s
            for callback in self.consolidation_callbacks:
                try:
                    callback(trigger)
                except Exception as e:
                    logger.error(
                        "Error in consolidation callback: %s", e, exc_info=True
                    )

            # GÃ©nÃ©rer rapport de consolidation
            report = await self._generate_consolidation_report(trigger)

            # Ã‰crire le rapport dans AGENT_SYNC.md (section automatique)
            await self._write_consolidation_report(report)

            # RÃ©initialiser l'Ã©tat
            self.pending_changes.clear()
            self.last_consolidation = datetime.now()

            # Mettre Ã  jour les statuts (tous les fichiers sont maintenant "synced")
            for rel_path in self.checksums:
                sync_status.labels(file_path=rel_path).set(1)

        sync_consolidations_triggered.labels(trigger_type=trigger.trigger_type).inc()

        logger.info("Consolidation completed successfully")

    async def _generate_consolidation_report(
        self, trigger: ConsolidationTrigger
    ) -> dict[str, Any]:
        """GÃ©nÃ¨re un rapport de consolidation."""
        # Grouper les Ã©vÃ©nements par fichier
        events_by_file: dict[str, list[SyncEvent]] = {}
        for event in self.pending_changes:
            if event.file_path not in events_by_file:
                events_by_file[event.file_path] = []
            events_by_file[event.file_path].append(event)

        return {
            "timestamp": trigger.timestamp.isoformat(),
            "trigger_type": trigger.trigger_type,
            "conditions_met": trigger.conditions_met,
            "total_changes": len(self.pending_changes),
            "files_changed": len(events_by_file),
            "events_by_file": {
                file_path: [
                    {
                        "event_type": e.event_type,
                        "timestamp": e.timestamp.isoformat(),
                        "agent_owner": e.agent_owner,
                    }
                    for e in events
                ]
                for file_path, events in events_by_file.items()
            },
        }

    async def _write_consolidation_report(self, report: dict[str, Any]) -> None:
        """Ã‰crit le rapport de consolidation dans AGENT_SYNC.md."""
        agent_sync_path = self.repo_root / "AGENT_SYNC.md"

        if not agent_sync_path.exists():
            logger.warning("AGENT_SYNC.md not found, skipping report write")
            return

        # Lire le contenu actuel
        content = agent_sync_path.read_text(encoding="utf-8")

        # Chercher la section "## ðŸ¤– Synchronisation automatique"
        section_marker = "## ðŸ¤– Synchronisation automatique"

        if section_marker not in content:
            # CrÃ©er la section Ã  la fin
            content += f"\n\n---\n\n{section_marker}\n\n"

        # InsÃ©rer le rapport aprÃ¨s le marqueur
        report_text = f"""
### Consolidation - {report["timestamp"]}

**Type de dÃ©clenchement** : `{report["trigger_type"]}`
**Conditions** : {json.dumps(report["conditions_met"], indent=2)}
**Changements consolidÃ©s** : {report["total_changes"]} Ã©vÃ©nements sur {report["files_changed"]} fichiers

**Fichiers modifiÃ©s** :
{self._format_events_for_markdown(report["events_by_file"])}

---
"""

        # InsÃ©rer aprÃ¨s le marqueur de section
        parts = content.split(section_marker, 1)
        if len(parts) == 2:
            # InsÃ©rer aprÃ¨s le marqueur, avant le contenu existant
            new_content = parts[0] + section_marker + report_text + parts[1]
        else:
            # Ajouter Ã  la fin
            new_content = content + report_text

        # Ã‰crire le nouveau contenu
        agent_sync_path.write_text(new_content, encoding="utf-8")

        logger.info("Consolidation report written to AGENT_SYNC.md")

    def _format_events_for_markdown(
        self, events_by_file: dict[str, list[dict[str, Any]]]
    ) -> str:
        """Formate les Ã©vÃ©nements pour Markdown."""
        lines = []
        for file_path, events in events_by_file.items():
            lines.append(f"- **{file_path}** : {len(events)} Ã©vÃ©nement(s)")
            for event in events:
                lines.append(
                    f"  - `{event['event_type']}` Ã  {event['timestamp']} (agent: {event['agent_owner'] or 'unknown'})"
                )
        return "\n".join(lines)

    # ========================================================================
    # PUBLIC API
    # ========================================================================

    def register_consolidation_callback(
        self, callback: Callable[[ConsolidationTrigger], None]
    ) -> None:
        """Enregistre un callback appelÃ© lors de chaque consolidation."""
        self.consolidation_callbacks.append(callback)

    async def trigger_manual_consolidation(self) -> dict[str, Any]:
        """DÃ©clenche manuellement une consolidation."""
        # Capturer le nombre de changements AVANT la consolidation
        changes_count = len(self.pending_changes)

        trigger = ConsolidationTrigger(
            trigger_type="manual",
            conditions_met={"pending_changes": changes_count},
            timestamp=datetime.now(),
        )

        await self._trigger_consolidation(trigger)

        return {
            "status": "success",
            "trigger": asdict(trigger),
            "changes_consolidated": changes_count,
        }

    def get_status(self) -> dict[str, Any]:
        """Retourne le statut actuel de la synchronisation."""
        return {
            "running": self._running,
            "pending_changes": len(self.pending_changes),
            "last_consolidation": self.last_consolidation.isoformat()
            if self.last_consolidation
            else None,
            "watched_files": len(self.watched_files),
            "checksums_tracked": len(self.checksums),
            "consolidation_threshold": self.consolidation_threshold,
            "check_interval_seconds": self.check_interval,
        }


# ============================================================================
# SINGLETON INSTANCE
# ============================================================================

_auto_sync_service: AutoSyncService | None = None


def get_auto_sync_service(
    repo_root: Path | None = None,
    **kwargs: Any,
) -> AutoSyncService:
    """Retourne l'instance singleton du service."""
    global _auto_sync_service

    if _auto_sync_service is None:
        if repo_root is None:
            # DÃ©tecter la racine du repo (remonter depuis ce fichier)
            current_file = Path(__file__)
            repo_root = current_file.parent.parent.parent.parent.parent

        _auto_sync_service = AutoSyncService(repo_root=repo_root, **kwargs)

    return _auto_sync_service
