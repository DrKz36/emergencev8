"""
Suite de validation compl√®te pour le syst√®me de m√©moire Phase 3
- Validation des m√©triques Prometheus
- Stress test avec 100+ messages
- Test du clustering automatique
- Validation du recall contextuel Nexus
"""
import asyncio
import json
import time
import sys
import io
from datetime import datetime
from typing import List, Dict, Any
import aiohttp
import numpy as np
from collections import defaultdict

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Configuration
BASE_URL = "http://127.0.0.1:8000"
PROMETHEUS_URL = f"{BASE_URL}/metrics"
MEMORY_URL = f"{BASE_URL}/api/memory"
NEXUS_URL = f"{BASE_URL}/api/nexus/chat"

class MemoryValidationSuite:
    def __init__(self):
        self.results = {
            "prometheus_metrics": {},
            "stress_test": {},
            "clustering": {},
            "recall_contextual": {}
        }
        self.start_time = datetime.now()

    async def fetch_prometheus_metrics(self) -> Dict[str, Any]:
        """R√©cup√®re et parse les m√©triques Prometheus"""
        async with aiohttp.ClientSession() as session:
            async with session.get(PROMETHEUS_URL) as resp:
                if resp.status != 200:
                    return {"error": f"Status {resp.status}"}

                text = await resp.text()
                metrics = {}

                for line in text.split('\n'):
                    if line.startswith('#') or not line.strip():
                        continue

                    # Parse les m√©triques pertinentes au syst√®me de m√©moire
                    if any(keyword in line for keyword in ['memory', 'redis', 'temporal', 'activation']):
                        parts = line.split()
                        if len(parts) >= 2:
                            metric_name = parts[0].split('{')[0]
                            try:
                                metric_value = float(parts[-1])
                                metrics[metric_name] = metric_value
                            except ValueError:
                                continue

                return metrics

    async def validate_prometheus_metrics(self):
        """√âtape 1: Valider les m√©triques Prometheus"""
        print("\n" + "="*80)
        print("üìà VALIDATION DES M√âTRIQUES PROMETHEUS")
        print("="*80)

        metrics = await self.fetch_prometheus_metrics()

        # M√©triques attendues pour le syst√®me de m√©moire
        expected_metrics = [
            'memory_store_total',
            'memory_query_duration_seconds',
            'redis_activation_hits',
            'redis_activation_misses',
            'temporal_context_queries',
            'memory_consolidation_operations'
        ]

        found_metrics = []
        missing_metrics = []

        for metric in expected_metrics:
            matching = [k for k in metrics.keys() if metric in k]
            if matching:
                found_metrics.append(metric)
                print(f"‚úì {metric}: {matching[0]} = {metrics[matching[0]]}")
            else:
                missing_metrics.append(metric)
                print(f"‚úó {metric}: NON TROUV√â")

        # Statistiques
        coverage = len(found_metrics) / len(expected_metrics) * 100
        print(f"\nüìä Couverture des m√©triques: {coverage:.1f}% ({len(found_metrics)}/{len(expected_metrics)})")

        self.results["prometheus_metrics"] = {
            "coverage_percent": coverage,
            "found_metrics": found_metrics,
            "missing_metrics": missing_metrics,
            "total_metrics_collected": len(metrics),
            "all_metrics": metrics
        }

        return coverage >= 50  # Au moins 50% des m√©triques attendues

    async def create_memory_entry(self, session: aiohttp.ClientSession,
                                  content: str, context: Dict = None) -> Dict:
        """Cr√©e une entr√©e m√©moire"""
        payload = {
            "content": content,
            "metadata": context or {},
            "timestamp": datetime.now().isoformat()
        }

        async with session.post(f"{MEMORY_URL}/store", json=payload) as resp:
            if resp.status in [200, 201]:
                return await resp.json()
            else:
                return {"error": f"Status {resp.status}", "content": content}

    async def stress_test_memories(self):
        """√âtape 2: Stress test avec 100+ messages"""
        print("\n" + "="*80)
        print("üß™ STRESS TEST AVEC 100+ MESSAGES")
        print("="*80)

        # G√©n√©rer des messages vari√©s avec patterns r√©p√©titifs pour tester le clustering
        message_templates = [
            "L'utilisateur aime la programmation en {}",
            "Discussion sur {} et son importance",
            "Pr√©f√©rence pour {} dans le d√©veloppement",
            "Question √† propos de {} et ses applications",
            "R√©flexion sur l'avenir de {}",
            "Analyse de {} dans le contexte actuel",
            "Comparaison entre {} et d'autres technologies",
            "Apprentissage de {} pour d√©butants",
            "Tutoriel avanc√© sur {}",
            "D√©bug d'un probl√®me avec {}"
        ]

        topics = [
            "Python", "Redis", "Prometheus", "FastAPI", "Docker",
            "Kubernetes", "Machine Learning", "AI", "Cloud Computing",
            "DevOps", "Microservices", "API Design", "Testing",
            "CI/CD", "Monitoring"
        ]

        messages = []
        for i, topic in enumerate(topics * 8):  # 120 messages
            template = message_templates[i % len(message_templates)]
            messages.append(template.format(topic))

        # M√©triques de d√©part
        metrics_before = await self.fetch_prometheus_metrics()

        # Ex√©cution du stress test
        start = time.time()
        async with aiohttp.ClientSession() as session:
            tasks = [self.create_memory_entry(session, msg, {"index": i})
                    for i, msg in enumerate(messages)]

            results = await asyncio.gather(*tasks, return_exceptions=True)

        duration = time.time() - start

        # M√©triques apr√®s
        await asyncio.sleep(1)  # Attendre que les m√©triques se mettent √† jour
        metrics_after = await self.fetch_prometheus_metrics()

        # Analyser les r√©sultats
        successful = sum(1 for r in results if isinstance(r, dict) and 'error' not in r)
        failed = len(results) - successful
        throughput = len(messages) / duration

        print(f"\nüìù Messages envoy√©s: {len(messages)}")
        print(f"‚úì Succ√®s: {successful}")
        print(f"‚úó √âchecs: {failed}")
        print(f"‚è±Ô∏è  Dur√©e totale: {duration:.2f}s")
        print(f"üöÄ D√©bit: {throughput:.2f} msg/s")

        # Comparaison des m√©triques
        print(f"\nüìä √âvolution des m√©triques:")
        for key in metrics_before:
            if key in metrics_after:
                delta = metrics_after[key] - metrics_before[key]
                if delta != 0:
                    print(f"  {key}: {metrics_before[key]} ‚Üí {metrics_after[key]} (+{delta})")

        self.results["stress_test"] = {
            "messages_sent": len(messages),
            "successful": successful,
            "failed": failed,
            "duration_seconds": duration,
            "throughput_msg_per_sec": throughput,
            "metrics_delta": {k: metrics_after.get(k, 0) - metrics_before.get(k, 0)
                            for k in metrics_before if k in metrics_after}
        }

        return successful >= len(messages) * 0.8  # Au moins 80% de succ√®s

    async def test_concept_clustering(self):
        """√âtape 3: Tester le clustering automatique de concepts similaires"""
        print("\n" + "="*80)
        print("üîç TEST DU CLUSTERING AUTOMATIQUE DE CONCEPTS")
        print("="*80)

        # Cr√©er des clusters de concepts similaires
        concept_groups = {
            "python_dev": [
                "J'adore coder en Python",
                "Python est mon langage pr√©f√©r√©",
                "Je d√©veloppe principalement en Python",
                "La programmation Python est g√©niale",
                "J'utilise Python quotidiennement"
            ],
            "redis_cache": [
                "Redis est excellent pour le cache",
                "J'utilise Redis pour la mise en cache",
                "Le cache Redis am√©liore les performances",
                "Redis comme syst√®me de cache distribu√©",
                "Configuration de Redis pour le caching"
            ],
            "monitoring": [
                "Prometheus collecte mes m√©triques",
                "J'aime surveiller avec Prometheus",
                "Grafana et Prometheus pour le monitoring",
                "M√©triques syst√®me avec Prometheus",
                "Dashboard de monitoring Prometheus"
            ]
        }

        # Ins√©rer les concepts
        async with aiohttp.ClientSession() as session:
            for group_name, messages in concept_groups.items():
                for msg in messages:
                    await self.create_memory_entry(session, msg, {"concept_group": group_name})

        await asyncio.sleep(2)  # Attendre que les embeddings soient g√©n√©r√©s

        # Tester la recherche pour chaque groupe
        async with aiohttp.ClientSession() as session:
            cluster_results = {}

            for group_name in concept_groups:
                # Requ√™te qui devrait rappeler tous les concepts du groupe
                test_query = list(concept_groups[group_name])[0]

                async with session.post(f"{MEMORY_URL}/search", json={
                    "query": test_query,
                    "top_k": 10,
                    "threshold": 0.5
                }) as resp:
                    if resp.status == 200:
                        search_results = await resp.json()

                        # Compter combien de r√©sultats appartiennent au m√™me groupe
                        same_group = 0
                        if 'results' in search_results:
                            for result in search_results['results']:
                                meta = result.get('metadata', {})
                                if meta.get('concept_group') == group_name:
                                    same_group += 1

                        cluster_quality = same_group / len(concept_groups[group_name]) * 100
                        cluster_results[group_name] = {
                            "query": test_query,
                            "same_group_found": same_group,
                            "total_in_group": len(concept_groups[group_name]),
                            "cluster_quality_percent": cluster_quality
                        }

                        print(f"\nüéØ Groupe: {group_name}")
                        print(f"  Requ√™te: {test_query[:50]}...")
                        print(f"  Trouv√© du m√™me groupe: {same_group}/{len(concept_groups[group_name])}")
                        print(f"  Qualit√© du cluster: {cluster_quality:.1f}%")

        avg_quality = np.mean([r['cluster_quality_percent'] for r in cluster_results.values()])
        print(f"\nüìä Qualit√© moyenne du clustering: {avg_quality:.1f}%")

        self.results["clustering"] = {
            "cluster_results": cluster_results,
            "average_quality_percent": avg_quality,
            "total_concepts_tested": sum(len(msgs) for msgs in concept_groups.values())
        }

        return avg_quality >= 60  # Au moins 60% de qualit√©

    async def test_nexus_contextual_recall(self):
        """√âtape 4: Valider le recall contextuel lors d'une conversation Nexus"""
        print("\n" + "="*80)
        print("üí¨ VALIDATION DU RECALL CONTEXTUEL NEXUS")
        print("="*80)

        # Cr√©er un contexte conversationnel
        conversation_context = [
            ("user", "Je travaille sur un projet d'IA avec Python et Redis"),
            ("assistant", "C'est une excellente combinaison ! Python pour le ML et Redis pour le cache."),
            ("user", "Comment optimiser les performances avec Redis ?"),
            ("assistant", "Utilisez le pipelining, les connexions pool et l'expiration des cl√©s."),
            ("user", "J'ai aussi besoin de monitoring"),
            ("assistant", "Je recommande Prometheus et Grafana pour surveiller Redis."),
        ]

        # Stocker le contexte
        async with aiohttp.ClientSession() as session:
            for i, (role, content) in enumerate(conversation_context):
                await self.create_memory_entry(session, content, {
                    "role": role,
                    "turn": i,
                    "conversation_id": "test_nexus_001"
                })

        await asyncio.sleep(2)

        # Questions qui n√©cessitent le recall du contexte
        recall_tests = [
            {
                "query": "Quel langage j'utilise pour mon projet d'IA ?",
                "expected_keywords": ["Python", "projet", "IA"],
                "context_turn": 0
            },
            {
                "query": "Quelles √©taient tes recommandations pour Redis ?",
                "expected_keywords": ["pipeline", "pool", "expiration", "cl√©"],
                "context_turn": 3
            },
            {
                "query": "Quels outils de monitoring as-tu sugg√©r√©s ?",
                "expected_keywords": ["Prometheus", "Grafana", "Redis"],
                "context_turn": 5
            }
        ]

        async with aiohttp.ClientSession() as session:
            recall_scores = []

            for test in recall_tests:
                # Recherche m√©moire contextuelle
                async with session.post(f"{MEMORY_URL}/search", json={
                    "query": test["query"],
                    "top_k": 5,
                    "threshold": 0.3,
                    "filters": {"conversation_id": "test_nexus_001"}
                }) as resp:
                    if resp.status == 200:
                        search_results = await resp.json()

                        # V√©rifier si les mots-cl√©s attendus sont dans les r√©sultats
                        results_text = " ".join([
                            r.get('content', '') for r in search_results.get('results', [])
                        ]).lower()

                        keywords_found = sum(
                            1 for kw in test["expected_keywords"]
                            if kw.lower() in results_text
                        )

                        recall_score = keywords_found / len(test["expected_keywords"]) * 100
                        recall_scores.append(recall_score)

                        print(f"\nüîç Question: {test['query']}")
                        print(f"  Mots-cl√©s trouv√©s: {keywords_found}/{len(test['expected_keywords'])}")
                        print(f"  Score de recall: {recall_score:.1f}%")
                        print(f"  R√©sultats: {len(search_results.get('results', []))} entr√©es")

        avg_recall = np.mean(recall_scores) if recall_scores else 0
        print(f"\nüìä Score moyen de recall contextuel: {avg_recall:.1f}%")

        self.results["recall_contextual"] = {
            "tests_performed": len(recall_tests),
            "recall_scores": recall_scores,
            "average_recall_percent": avg_recall,
            "conversation_turns": len(conversation_context)
        }

        return avg_recall >= 50  # Au moins 50% de recall

    async def run_all_validations(self):
        """Ex√©cuter toute la suite de validation"""
        print("\n" + "="*80)
        print("üöÄ D√âMARRAGE DE LA SUITE DE VALIDATION M√âMOIRE PHASE 3")
        print(f"‚è∞ Heure de d√©but: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)

        # Ex√©cuter chaque validation
        validations = [
            ("Prometheus Metrics", self.validate_prometheus_metrics),
            ("Stress Test 100+", self.stress_test_memories),
            ("Concept Clustering", self.test_concept_clustering),
            ("Nexus Contextual Recall", self.test_nexus_contextual_recall)
        ]

        validation_results = {}

        for name, validation_func in validations:
            try:
                success = await validation_func()
                validation_results[name] = "‚úì PASS" if success else "‚úó FAIL"
            except Exception as e:
                print(f"\n‚ùå Erreur lors de {name}: {str(e)}")
                validation_results[name] = f"‚úó ERROR: {str(e)}"

        # Rapport final
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()

        print("\n" + "="*80)
        print("üìã RAPPORT FINAL DE VALIDATION")
        print("="*80)

        for name, result in validation_results.items():
            print(f"{result} - {name}")

        passed = sum(1 for r in validation_results.values() if "PASS" in r)
        total = len(validation_results)
        success_rate = passed / total * 100

        print(f"\nüéØ Taux de r√©ussite global: {success_rate:.1f}% ({passed}/{total})")
        print(f"‚è±Ô∏è  Dur√©e totale: {duration:.2f}s")
        print(f"‚è∞ Heure de fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # Sauvegarder le rapport
        report = {
            "start_time": self.start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "validation_results": validation_results,
            "success_rate_percent": success_rate,
            "detailed_results": self.results
        }

        report_path = f"reports/memory_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ Rapport sauvegard√©: {report_path}")

        return success_rate >= 75  # Au moins 75% de validations pass√©es


async def main():
    suite = MemoryValidationSuite()
    overall_success = await suite.run_all_validations()

    if overall_success:
        print("\n‚úÖ VALIDATION GLOBALE R√âUSSIE!")
        return 0
    else:
        print("\n‚ö†Ô∏è  VALIDATION GLOBALE √âCHOU√âE - V√©rifier les d√©tails ci-dessus")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
